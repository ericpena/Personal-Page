<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | ERIC PEÑA</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2019 Eric Peña</copyright><lastBuildDate>Mon, 09 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>First Genetic Algorithm</title>
      <link>/post/ga-intro/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/ga-intro/</guid>
      <description>&lt;h1 id=&#34;components-of-a-genetic-algorithm&#34;&gt;Components of a Genetic Algorithm&lt;/h1&gt;
&lt;h1 id=&#34;genetic-algorithm-written-in-python&#34;&gt;Genetic Algorithm Written in Python&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fuzzywuzzy import fuzz
import random
import string

class Agent:

	def __init__(self, length):

		# Initialize a new agent
		self.string = &#39;&#39;.join(random.choice(string.ascii_letters) for _ in range(length))
		self.fitness = -1

	def __str__(self):

		return &#39;String: &#39; + str(self.string) + &#39; Fitness: &#39; + str(self.fitness)

in_str = None
in_str_len = None
population = 20
generations = 5000

# All the code to evolve
def ga():
	
	agents = init_agents(population, in_str_len)

	for generation in range(generations):

		print(&#39;Generation: &#39; + str(generation))

		agents = fitness(agents)
		agents = selection(agents)
		agents = crossover(agents)
		agents = mutation(agents)

		if any(agent.fitness &amp;gt;= 90 for agent in agents):

			print(&#39;Threshold met!&#39;)
			exit(0)

def init_agents(population, length):

	return [Agent(length) for _ in range(population)]

def fitness(agents):

	for agent in agents:

		agent.fitness = fuzz.ratio(agent.string, in_str)

	return agents

def selection(agents):

	agents = sorted(agents, key=lambda agent: agent.fitness, reverse=True)
	print(&#39;\n&#39;.join(map(str, agents)))
	agents = agents[:int(0.2 * len(agents))]

	return agents

def crossover(agents):

	offspring = []

	for _ in range(int((population - len(agents)) / 2)):

		parent1 = random.choice(agents)
		parent2 = random.choice(agents)
		child1 = Agent(in_str_len)
		child2 = Agent(in_str_len)
		split = random.randint(0, in_str_len)
		child1.string = parent1.string[0:split] + parent2.string[split:in_str_len]
		child2.string = parent2.string[0:split] + parent1.string[split:in_str_len]

		offspring.append(child1)
		offspring.append(child2)

	agents.extend(offspring)

	return agents

def mutation(agents):

	for agent in agents:

		for idx, param in enumerate(agent.string):

			if random.uniform(0.0, 1.0) &amp;lt;= 0.1:

				agent.string = agent.string[0:idx] + \
					random.choice(string.ascii_letters) + \
					agent.string[idx + 1:in_str_len]

	return agents

if __name__ == &#39;__main__&#39;:
	
	in_str = &#39;ericpena&#39;
	in_str_len = len(in_str)
	ga()
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Speed of Julia</title>
      <link>/post/julia/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/julia/</guid>
      <description>&lt;figure&gt;
  &lt;img src=&#34;img/logo.png&#34; alt=&#34;Julia Logo&#34; width=&#34;500&#34;/&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;creating-fibn&#34;&gt;Creating fib(n)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;function fib(n)
    if (n == 1 || n == 2)
        return 1
    else
        return fib(n - 1) + fib(n - 2)
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;timing-fibn-140&#34;&gt;Timing fib(n) 1:40&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;function fibTime(k)
    t = []
    for i in 1:k
        push!(t, (@timed fib(i))[2])
    end
    return t
end

# :: Print @timed Fibonacci 1 through 40
println(fibTime(40))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;plotting-timed-results&#34;&gt;Plotting @Timed Results&lt;/h2&gt;
&lt;p&gt;The timing for Julia is surprisingly very fast!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;using Plots
plot(fibTime(40), title=&amp;quot;Timed Recursive Fibonacci Algorithm&amp;quot;, 
	color = :red, fill = (0, .3, :red), legend = false)
xaxis!(&amp;quot;[n given in fib(n)]&amp;quot;)
yaxis!(&amp;quot;Time [seconds]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/timed.png&#34; alt=&#34;Timed&#34; width=&#34;600&#34;/&gt;
  &lt;figcaption&gt;Figure 1 — Timed Recursive Algorithm That Calculates $n^{th}$ Fibonacci Number in Julia&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;achieve-results-in-python&#34;&gt;Achieve Results in Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib import pyplot as plt
import time

def fib(n):
    if (n == 1) or (n == 2):
        return 1
    else:
        return fib(n - 1) + fib(n - 2)

def fibTimed(k):
    t = []
    for i in range(1, k + 1):
        s = time.time()
        fib(i)
        fib_t = time.time() - s
        t.append(fib_t)
    return t

result = fibTimed(40)
print(result)
plt.plot(result)
plt.title(&amp;quot;Python @Timed Recursive fib(n) Algorithm&amp;quot;)
plt.xlabel(&amp;quot;[n given in fin(n)]&amp;quot;)
plt.ylabel(&amp;quot;Time [seconds]&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;The results from Python are significantly slower than compared to Julia. $Fib(40)$ takes nearly $30$ seconds to complete.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/timed_python.png&#34; alt=&#34;Timed&#34; width=&#34;600&#34;/&gt;
  &lt;figcaption&gt;Figure 2 — Timed Recursive Algorithm That Calculates $n^{th}$ Fibonacci Number in Python&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;show-julia-versus-python-comparison&#34;&gt;Show Julia Versus Python Comparison&lt;/h2&gt;
&lt;p&gt;The plot below shows that Julia is significantly more efficient compared to Python for this recursive algorithm.&lt;/p&gt;
&lt;hr&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/comparison.png&#34; alt=&#34;Compare&#34; width=&#34;600&#34;/&gt;
  &lt;figcaption&gt;Figure 3 — Timed Recursive Algorithm Between Julia and Python between $n = 30$ and $n = 40$&lt;/figcaption&gt;
&lt;/figure&gt;</description>
    </item>
    
    <item>
      <title>Complexity — A Guided Tour</title>
      <link>/post/complexity-reflection/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/complexity-reflection/</guid>
      <description>&lt;h1 id=&#34;part-i&#34;&gt;Part I:&lt;/h1&gt;
&lt;p&gt;Providing the multifaceted story of how complex systems has evolved into what it is today is no easy feat. Dr. Melanie Mitchell not only gives a comprehensive historical tour of science and mathematics, she also provides foundational knowledge that any reader can carry to appreciate the essence of what it means for a system to be complex in the technical sense. Early on, she gives a clear definition of complex systems as an interdisciplinary field that studies how large networks of entities with no central controller can organize themselves and exhibit collective behavior given simple rules. However, she also humbly confesses that given how nebulous the field is, there has yet to be one clear definition of complexity that everyone can agree on. I appreciate how candid she is about what is known and what is not known in this field. Mitchell shares that even a panel she organized of great system science thinkers at the Santa Fe Institute could not agree on an answer to the question: &amp;ldquo;how do you define complexity?&amp;quot;.&lt;/p&gt;
&lt;p&gt;She acknowledges the potential skepticism that one might feel because of the nascence of this field. However, she encourages the reader by sharing fascinating real-world examples&amp;mdash;such as ant colonies, brain activity, and internet networks&amp;mdash;that show the ubiquity of complex systems and its necessity in understanding how systems evolve in our lives. She eloquently describes the emergent collective behavior that arises from these systems and the wide scale at which they occur. From thoughts and consciousness in the brain to global economic market movements, these can all be thought of as emergent phenomena attributed to complexity. She mentions that some have even call these systems superorganisms that exhibit a collective intelligence even in non-living, non-conscious systems. She borrows examples from economics stating that the self-interest of smaller economic entities such as companies and individuals create macroscopic effects that tend to not resemble the smaller systems at all. I appreciate that complex systems are not only accessible to all of us everywhere we are, but also require them to survive such as brain neuroactivity and heartbeat rhythms.&lt;/p&gt;
&lt;p&gt;Given my background, the section on dynamics, chaos, and prediction resonated with me most. Mitchell beautifully tells the story of how the natural world was analyzed by great scientists in history and how our understanding of nature evolved. This historical account builds from Aristotle, Copernicus, Galileo, Kepler, and finally Isaac Newton and his revolutionary laws of motion. Newton established a concept of universality extending the natural laws from the terrestrial realm to celestial bodies&amp;mdash;one of the most enlightening realizations in science.&lt;/p&gt;
&lt;p&gt;When I consider physics paradigms in the context of complexity, I think about the dismantling of reductionism&amp;mdash;where the notion that a system is the sum of its parts, as is the case in classical mechanics&amp;mdash;is no longer valid. Mitchell does not talk much about reductionism directly but instead touches on a topic much richer on how concepts in physics directly led to the understanding of complexity and its causes. Much of physics is used primary to make predictions but Mitchell mentions two important scientific discoveries that make prediction very difficult: the uncertainty principle from quantum mechanics and the sensitivity of initial conditions from chaos theory. In 1927, Heisenberg taught us that the momentum and position of a particle cannot be known simultaneously but rather act as a trade-off of information. This is not an experimental limitation but an inherent limitation of information that is written into the law of physics. This must have been devastating during the time of Newton&#39;s &amp;ldquo;clockwork universe&amp;rdquo; when mathematicians such as Pierre Simon Laplace had ideas of being able to, in theory, predict everything at all times given we have the information to do so. The development of chaos theory by Poincar&#39;e and Lorenz had a similar effect stating that even the tiniest variation in initial conditions can lead to drastically different outcomes&amp;mdash;which served as another blow to predictability.&lt;/p&gt;
&lt;p&gt;Mitchell sheds hope and motivates the reader by mentioning that in the same way there are universal laws in physics, there are similar invariants in the study of complex systems as well. Period-doubling as a system evolves to a chaotic state and the Feigenbaum&#39;s constant for the rate at which bifurcations converge are examples of remarkable invariant properties in complex systems. This provides an indication that the essence of physics can likely be applied to the field of complex systems and even possibly toward its version of a unified theory.&lt;/p&gt;
&lt;p&gt;Dr. Melanie Mitchell seamlessly weaves together the concepts of information, entropy, energy, and thermodynamics. These are integral to the field so much so that she even refers to &amp;ldquo;entropy-defying self-organization&amp;rdquo; as the &amp;ldquo;holy grail of complex systems&amp;rdquo;. She resolves the misconception that entropy can at times decrease without consequence even though the law of thermodynamics forbids it. She talks about this in terms of Maxwell&#39;s Demon stating that work is required for the demon to do its job. I had learned about Maxwell&#39;s Demon in the past but had never considered this in terms of information. Szilard thought that work and energy was expended when obtaining the measurements of the particle&#39;s velocities&amp;mdash;in other words, it requires work to obtain the relevant information needed. I find this to be a novel idea and a clever way to connect all of these concepts in one easy to understand example.&lt;/p&gt;
&lt;p&gt;Another pivotal moment for me in reading Part I was in chapter 4&amp;mdash;computation. In the same way that physics endured a disillusionment of endless predictability, mathematics and computability had a relatable event in history through the work of Kurt Gödel and Alan Turing.
David Hilbert&#39;s provocative questions for the mathematics community challenged the stability of mathematics itself. Gödel figured out how to convert statements into mathematical language and learned that there exists self-contradictory mathematical statements&amp;mdash;showing that not everything in mathematics could be proven. Mathematics was viewed as being exacting and able to prove anything at that time. Gödel&#39;s Incompleteness Theorem shattered the apparent grandeur of mathematics and worried many practitioners. Alan Turing also came up with a similar answer using what was called a Turing Machine. The result had extended to machine language as well.&lt;/p&gt;
&lt;p&gt;Dr. Mitchell is not only able to clearly explain a topic as abstruse as complex theory, she does so to a general audience without inundating the reader with technical jargon. Since there is not a definition of complexity that the scientific community can agree on, Mitchell offers some thoughts about possible ways that complexity could be defined. She is again very candid about the limitations of these definitions and explains why they would not hold up in isolation. All throughout Part I, she keeps the theme of being informative, honest, and motivating. She is honest about scientists not yet having a clear understanding of the field but also offers other examples in history where this was also true such as the concept of energy before it was well understood. Even today, research on genes and dark matter have yet to be understood fully but are making tremendous progress. I learned a great deal so far from this book and I look forward to the insight it has to offer in the remaining chapters.&lt;/p&gt;
&lt;h1 id=&#34;parts-ii-and-iii&#34;&gt;Parts II and III&lt;/h1&gt;
&lt;p&gt;Melanie Mitchell states that it is hypothesized that the balance between exploration and exploitation of information may serve as a general property of what are now called complex systems. Having read parts II and III of the book, it is evident that this idea could not have been developed without the use of computation, particularly self-replicating systems. Mitchell takes us deeper into the journey of complex systems and focuses on the connection between computation and living systems. From the fine details of a genetic algorithm to large-scope philosophical ideas, she provides different levels of abstraction allowing the reader to appreciate these topics from all angles. The historical account she gives of evolutionary computation reads as if she is alluding to the past and the future almost simultaneously &amp;mdash; she tells where the inspiration has come from and where it could lead.&lt;/p&gt;
&lt;h2 id=&#34;self-replication&#34;&gt;Self Replication&lt;/h2&gt;
&lt;p&gt;Part II begins with the idea of &lt;em&gt;&lt;strong&gt;what life is&lt;/strong&gt;&lt;/em&gt; and what are its properties. Mitchell explains how the idea of self-replicating artificial life is an old idea but continues to live on, even in our movies and artwork today. I thought this was a great introduction to the idea of evolutionary programming. The discussion leads to the idea of reproduction of computer systems that can, for example, print their own code. This is a novel idea on a fundamental level and a great zeroeth-order example to begin understanding its significance; but the fact that DNA, for example, can actually transcribe its own interpreter may provoke one to admit: some systems actually are &lt;em&gt;living&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;genetic-algorithms&#34;&gt;Genetic Algorithms&lt;/h2&gt;
&lt;p&gt;I was inspired by the chapter on genetic algorithms to create my own program&amp;mdash;a simple program that evolves and learns to print the characters:
&lt;strong&gt;ericpena&lt;/strong&gt;. Although a simple task for a computer to perform, it made apparent the power of this methodology to solve more complicated tasks. John von Neumann is essentially the person who established the idea of self-replicating machines in a tangible way. Although, he thought that replication was not enough as these systems also need to evolve, learn, and compete with one another to survive. It was John Holland that proposed the idea for genetic algorithms that was inspired by how nature evolves to find near-optimal solutions via natural selection. Mitchell gives a great example on how a genetic algorithm can help improve the performance of Robby, the Soda-Can-Collecting Robot. This chapter revealed the vast potential for genetic algorithms and how in many ways they are better than human-developed algorithms for various tasks. One particular discussion that stood out to me regarding Robby is the following: Robby&#39;s success isn&#39;t necessarily dependent on the individual, step-by-step decisions that Robby makes to collect cans but rather the aggregate of these steps that define a hollistic strategy, a rather successful strategy at that. Mitchell explains this by stating that it isn&#39;t always individual genes at work but rather their interactions that produce results.&lt;/p&gt;
&lt;h2 id=&#34;cellular-automata-and-the-nature-of-computability&#34;&gt;Cellular Automata and The Nature of Computability&lt;/h2&gt;
&lt;p&gt;I have been enthralled by cellular automata ever since I first learned about them. In this chapter, Mitchell talks about how computation occurs in nature and what it even means for nature to &amp;ldquo;compute&amp;rdquo;. Similar to the way in which physicists simplify problems to gain understanding&amp;mdash;through frictionless slopes and spherical cows&amp;mdash;scientists have performed a simplification for computation in the form of cellular automata. Although the rules of a cell can be relatively simple, the interconnectedness of a grid of cells can produce complex behavior that can, at times, simulate natural processes such as forest fires, fire flies, and even neurons. The field of cellular automata seemed to have taken a life of its own. John von Neumann, who played a key role in creating cellular automata, proved that it is actually Turing Complete. John Conway&#39;s Game of Life programs popularize the discipline even further especially when proving that his game is capable of simulating a universal computer. With the large number of configurations and possibilities, it may have been difficult to wrap one&#39;s head around the innate behavior of this mechanism. It was Stephan Wolfram, who nothing short of genius, worked diligently and cleverly to classify the ways in which cellular automata can behave and what patterns it can exhibit. He created libraries of patterns for these things, particularly four classes, and became rather popular for his book on them, {\it A New Kind of Science}. I learned about Wolfram during my undergraduate program and used Mathematica for many of my homework problems&amp;mdash;although I had not read about his beliefs that Mitchell mentions in the reading. I found it interesting that Wolfram believes natural processes are intrinsically computable and can be explained in this way. While in physics, I conceded to the idea that nature is infinitely complex and our best hope in grasping it is to make very accurate approximations with efforts such as quantum mechanics or Einsteinian gravity. However, Mitchell explains that Wolfram believes that nothing can be more complex than a universal computer which forces an upper limit on the complexity that the universe can exhibit&amp;mdash;this is very interesting although I am not sure I agree completely given the unpredictability we read about earlier in the book, particularly in quantum mechanical phenomena. I was nevertheless fascinated by these ideas and will revisit them in the future.&lt;/p&gt;
&lt;p&gt;A connection became apparent to me while reflecting on the chapters on genetic algorithms and cellular automata.  On one hand, many processes in nature consists of simple rules but large in number that coalesce into complex behavior. Cellular automata has this idea built into its very structure. In a way, we can say that the mechanism by which nature evolves and produces complexity is comparable to the way in which cellular automata does so. On the other hand, genetic algorithms were inspired by what occurs in nature, namely natural selection and evolution. I found it fascinating that nature utilizes general principles found in cellular automata and that genetic algorithms mimic what is done in nature. This further reinforces the link between computation and living systems.&lt;/p&gt;
&lt;h2 id=&#34;information-processing&#34;&gt;Information Processing&lt;/h2&gt;
&lt;p&gt;Mitchell guides the reader in understanding information processing particularly in living systems. I enjoyed how she uses different levels of programming, high level and machine language, to home in on the concept of abstraction. She mentions that it&#39;s relatively easy to imagine how changes in high level code are translated into low level machine language since the tether between the two is tangible and somewhat predictable. She compares this to cellular automata where the ability to create a productive level of abstraction is not present. This is particular to cellular automata but I imagine that it is a limitation that can be applied to much of complex systems as a whole&amp;mdash;this is, creating a high level framework from which many applications can benefit from especially biological applications.
The book wonderfully explains information processing both in terms of what information is and how it is processed. It accomplishes this with the help of three real-world examples: the immune system, ant colonies, and biological metabolism. One of my favorite parts&amp;mdash;which Mitchell believes is a quite profound idea&amp;mdash;is that on the {\it meaning} of information. We can think in terms of inputs and outputs but what does the information {\it mean} and what part of a system is doing this type of analysis? As Mitchell points out, this is particularly mysterious for systems with no central controller.&lt;/p&gt;
&lt;h1 id=&#34;parts-iv-and-v&#34;&gt;Parts IV and V&lt;/h1&gt;
&lt;p&gt;Part IV of Melanie Mitchell&#39;s book discusses the science of networks&amp;mdash;a topic in which tremendous progress and development has been made relatively recently. She begins the discussion with Harvard University psychologist, Stanley Milgram. He was one of the first to have devised an experiment designed to understand the degrees of separation in a network&amp;mdash;his experiment is the quintessential example of the small world property in a network.&lt;/p&gt;
&lt;p&gt;Professor Mitchell introduces networks and gives examples that demonstrate that networks are ubiquitous in our everyday lives. From social networks to epidemiological studies, these fields can and should be explored in the context of network science. She mentions popular names in the network science world: Duncan Watts, Steven Strogratz, Albert-László Barabási, and Réka Albert who all published papers that formed the foundation of network science. She mentions that physicists made contributions to the field of network science, which is right in the physics wheelhouse given its mathematical nature and real-world application. Mitchell explains that physicists have been trained to simplify complex problems without losing their essential features. I did enjoy and will attempt to commit to memory the eloquent quote from Duncan Watts, &amp;ldquo;No one descends with such fury and in so great a number as a pack of hungry physicists, adrenalized by the scent of a new problem.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;It is helpful to the reader to know generally what a network is. Mitchell explains the basics of in-degree, out-degree, and their distributions in a network. She goes into what clustering means and how we think about clustering in the real-world. She gives popular examples such as an airplane&#39;s use of hubs to optimize flight itineraries. If one were to create a network whose degree distribution is random, all nodes would have somewhat similar degree and the degree distribution would be considered uniform&amp;mdash;unlike what we see in real-world biological and social networks. Naturally occurring networks exhibit hub structures and skewed degree distributions which is quite interesting. What invisible hand is fine-tuning these networks to acquire these specific properties? Mitchell gives us great insight on this. She breaks down the general attributes of networks into two pillars: small-world and scale-free. She explains that when a network has small-world-ness, it exhibits attractive features for a network to have such as robustness and faster communication. A skewed degree distribution makes a network robust to node deletion since hubs are few in number and high in degree. The Milgram experiment is a clear example of how these networks can benefit from efficient information spread whereas disease outbreaks may be exacerbated.&lt;/p&gt;
&lt;p&gt;Network science was especially propelled by the use of computers: their increase in computing power and availability of data. Mitchell explains that although real-world networks resembled small-world structures, it was not exact. They are better explained by a property called scale-free that pervades mostly all natural networks. She gives an explanation of using Google&#39;s page-rank to show that the world wide web itself is a scale-free network. She goes on the talk about this in terms of scaling invariance and self-similarity and says that this is what is meant by scale-free&amp;mdash;terms that are all synonymous. Another concept she equates to the scale-free property of networks is power law degree distribution. An important part of this discussion is Barab&#39;asi&#39;s work on preferential attachment and how this mechanism facilitates the scale-free structure in naturally forming networks.&lt;/p&gt;
&lt;p&gt;It certainly does aide the reader in having stimulating real-world examples of network structures from which to explore these properties. The examples she delves into are the brain of the worm C. elegans, genetic regulatory networks in our bodies, epidemiological studies, and even ecological insights that the reader can learn from. Mitchell not only explains that the brain of a worm is found to have small-world structure, she also encourages the reader to consider a rather provocative question: &amp;ldquo;Why would evolution favor brain networks with the small-world property?&amp;quot;. Two main ideas stem from this question&amp;mdash;the resilience of scale-free network structures and energy efficiency for global information processing. A fully connected network would use entirely too much energy to complete similar tasks. Mitchell briefly mentions synchronization at the end of the brain example. I understand this topic can get much more involved with how nature utilizes synchronization. The synchronization of fire flies is especially interesting&amp;mdash;more of this can be found in an article written by Renato Mirollo and Steven Strogatz, &amp;ldquo;Synchronization of Pulse-Coupled Biological Oscillators&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Mitchell naturally segues into the topic of scaling. The start of this chapter reminded me of Galileo&#39;s Square-Cube Law where he essentially argues that structures cannot simply be scaled up ad infinitum. The core of the argument involves recognizing that volume grows as the cube of the multiplier and the strength of support grows only as the cross-sectional area or square of the multiplier. One of the key messages with regards to scaling is that metabolic rate does not scale linearly with $bodymass$ but rather with $bodymass^{\frac{3}{4}}$. This occurs in a myriad of creatures and shows that they become more efficient as the scaling grows. This power law distribution of metabolic rate versus body-mass is referred to as Kleiber&#39;s law. James Brown, Brian Enquist, and Geoffrey West played pivotal roles in forming the metabolic scaling theory. Geoffrey West wrote a popular book on this subject as well that the general audience can appreciate: &amp;ldquo;Scale: The Universal Laws of Growth, Innovation, Sustainability, and the Pace of Life in Organisms, Cities, Economies, and Companies&amp;rdquo;. He elaborates on how cities and economies tend to exhibit similar power law distributions when they are scaled up as well. Mitchell clearly explains Zipf&#39;s law&amp;mdash;she emphasizes it&#39;s significance and ubiquity and also provides some insight into why this power-law distribution tends to naturally occur. Mitchell shares that this is an important open problem in complexity science but I was particularly interested in Mandelbrot&#39;s argument that the simultaneous optimization of information content and transmission leads directly to Zipf&#39;s law.&lt;/p&gt;
&lt;p&gt;The conclusion of the book serves the reader in several ways. It ties various concepts together mentioned throughout the book and gives a historical account of the key players that contributed to the field of complex systems. It also explains the goal of complexity as a science while sharing the honest opinions of complexity enthusiasts and critics, alike. Mitchell candidly addresses the major challenges of complexity science such as the lack of an all-encompassing mathematical framework from which complexity can be studied. Most importantly, it leaves us with the refreshing hope for future scientists to work diligently to explore the possibility of a grand unified theory of complexity and to discover what Strogatz refers to as the &amp;ldquo;conceptual equivalent of calculus&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speed Walking in Chicago</title>
      <link>/post/speedwalking/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/post/speedwalking/</guid>
      <description>&lt;h1 id=&#34;chicagoan-fast&#34;&gt;Chicagoan Fast&lt;/h1&gt;
&lt;p&gt;My morning commute to work in the heart of downtown Chicago was very regular: Enjoy blazing hot coffee and listen to an interesting podcast all while super-speed-walking all the way to the office. I often think how much time I would save by teleporting to the office instead. Even while often walking faster than most on the busy morning sidewalks, there are folks that zoom past me but still appear to be walking gracefully.&lt;/p&gt;
&lt;p&gt;To start, let’s break speed walking down to two main factors: long legs and step frequency (which differs from walking speed). I like to think I take advantage of both of these to maintain optimal walking speed (without looking too ridiculous). The general question to myself was this:&lt;/p&gt;
&lt;h3 id=&#34;for-someone-whose-legs-are-longer-than-mine-how-much-faster-do-i-have-to-walk-to-keep-up&#34;&gt;For someone whose legs are longer than mine, how much faster do I have to walk to keep up?&lt;/h3&gt;
&lt;p&gt;Another way to ask this is, how are the leg length and step-frequency related? If two people, Person A and Person B, are walking at the same step frequency but Person B has longer legs than Person A by a factor of alpha (alpha &amp;gt; 0), Person B will clearly have higher walking speed. Therefore, you can ask by what factor does Person A’s step frequency has to increase to keep up with Person B while walking down the street.&lt;/p&gt;
&lt;p&gt;Let’s go through it logically together. Let&#39;s draw the some simple person possible:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/diagram0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Given how we&#39;re thinking about this problem, the important values we should be interested in is &lt;strong&gt;Leg Length&lt;/strong&gt; and &lt;strong&gt;Step Frequency&lt;/strong&gt;. The angle created by the legs may be important to so let&#39;s lable it for now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/diagram1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;first-step-literally&#34;&gt;First Step (Literally)&lt;/h1&gt;
&lt;h3 id=&#34;what-happens-when-one-step-is-taken-by-person-a-and-person-b&#34;&gt;What happens when one step is taken by &lt;strong&gt;Person A&lt;/strong&gt; and &lt;strong&gt;Person B&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The distance that each person goes in one step is given by:&lt;/p&gt;
&lt;p&gt;$$d_A = 2A \sin\left(\frac{\alpha_A}{2}\right)$$
$$d_B = 2B \sin\left(\frac{\alpha_B}{2}\right)$$&lt;/p&gt;
&lt;p&gt;Where $A$ and $B$ are the length of the legs of &lt;strong&gt;Person A&lt;/strong&gt; and &lt;strong&gt;Person B&lt;/strong&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Let&#39;s simplify some things before we generalize the problem:&lt;/p&gt;
&lt;p&gt;$$\tilde{A} = 2A$$
$$\tilde{B} = 2B$$
$$\gamma = \sin\left(\frac{\alpha_A}{2}\right) = \sin\left(\frac{\alpha_B}{2}\right)$$
$$d_A = \tilde{A} \gamma$$
$$d_B = \tilde{B} \gamma$$&lt;/p&gt;
&lt;h1 id=&#34;more-steps&#34;&gt;More Steps&lt;/h1&gt;
&lt;p&gt;As shown above, we&#39;ve made the assumption that $\theta_A = \theta_B$. Let&#39;s generalize this to more than one step. Here we will introduce a person&#39;s &lt;strong&gt;Step Frequency&lt;/strong&gt;, $s_A$ and $s_B$, defined as how many steps a person takes per unit time, $t$.&lt;/p&gt;
&lt;p&gt;$$d_A(t) = \tilde{A} \gamma s_A t$$
$$d_B(t) = \tilde{B} \gamma s_B t$$&lt;/p&gt;
&lt;p&gt;The general form can be thought of as the linear relationship:&lt;/p&gt;
&lt;p&gt;$$d = \kappa s t$$&lt;/p&gt;
&lt;p&gt;where $\kappa_A = \tilde{A} \gamma$ or $\kappa_B = \tilde{B} \gamma$.&lt;/p&gt;
&lt;h1 id=&#34;keep-up&#34;&gt;Keep Up!&lt;/h1&gt;
&lt;p&gt;Here is the main problem for us to go through in order to answer the original question. Let&#39;s make &lt;strong&gt;Person B&lt;/strong&gt; have longer legs than &lt;strong&gt;Person A&lt;/strong&gt; by a factor of $x$.&lt;/p&gt;
&lt;p&gt;$$B = x A$$&lt;/p&gt;
&lt;p&gt;Generally, since &lt;strong&gt;Person B&lt;/strong&gt; has longer legs, the Step Frequency of &lt;strong&gt;Person A&lt;/strong&gt;, $s_A$, has to increase to keep up.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/diagram2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now let&#39;s look at the ratio $\frac{d_B}{d_A}$:&lt;/p&gt;
&lt;p&gt;$$\frac{d_B}{d_A} = \frac{2 x A \gamma s_B t}{2 A \gamma s_A t} = \frac{x s_B}{s_A}$$&lt;/p&gt;
&lt;p&gt;To make distance the same between the two people, we&#39;ll make: $\frac{d_B}{d_A} = 1$&lt;/p&gt;
&lt;p&gt;$$\frac{d_B}{d_A} = 1 = \frac{x s_B}{s_A}$$
$$s_A = x s_B$$&lt;/p&gt;
&lt;h1 id=&#34;what-the-heck-did-we-learn&#34;&gt;WHAT THE HECK DID WE LEARN?&lt;/h1&gt;
&lt;p&gt;If &lt;strong&gt;Person B&lt;/strong&gt; has legs longer than &lt;strong&gt;Person A&lt;/strong&gt; by a factor of $x$ and $\theta_A$ = $\theta_B$, then &lt;strong&gt;Person A&lt;/strong&gt; can keep up by increasing their step frequency by $x$. Basically &lt;strong&gt;Person A&lt;/strong&gt; can move their legs faster by this factor to keep up with &lt;strong&gt;Person B&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;this-is-very-simplified-what-happens-if-theta-a-neq-theta-b-are-there-other-factors-to-consider&#34;&gt;This is very simplified. What happens if $\theta_A \neq \theta_B$? Are there other factors to consider?&lt;/h3&gt;
&lt;h1 id=&#34;centerthats-all-gotta-runcenter&#34;&gt;&lt;CENTER&gt;THAT&#39;S ALL, GOTTA RUN&lt;/CENTER&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/speed3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Tensors</title>
      <link>/post/tensors/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/post/tensors/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;img/Vector-1-Form.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Motivation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;General Relativity&lt;/li&gt;
&lt;li&gt;Inertia Tensor&lt;/li&gt;
&lt;li&gt;Stress Tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It took me a whole week figure out what a tensor actually is. You will find many definitions and some are only partially correct. Let&#39;s walk through the different explanations and learn how to think about them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;(Array Definition)&lt;/strong&gt; Tensor = Multi-dimensional array of numbers (scalars (rank 0), vectors (rank 1), matricies (rank 2), etc.). This is true in a sense but there is a truer geometrical meaning behind the concept of a tensor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Coordinate Definition)&lt;/strong&gt; Tensor = an object that is invariant under a change of coordinates and has &lt;em&gt;components&lt;/em&gt; that change in a special, predictable way under a change of coordinates. This is also true but let&#39;s dive even deeper to learn how a tensor allows this behavior to take place.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(Abstract Definition)&lt;/strong&gt; Tensor = a collection of vectors and covectors combined together using the tensor product&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The important thing to keep in mind is that vectors exist independently of their components and their components depend on the coordinate system used to define vectors.&lt;/p&gt;
&lt;h1 id=&#34;changing-coordinate-systems&#34;&gt;Changing Coordinate Systems&lt;/h1&gt;
&lt;p&gt;Let&#39;s go from an old basis to a new basis&lt;/p&gt;
&lt;center&gt;Old Basis: {$\vec{e_1}, \vec{e_2}$}&lt;/center&gt;
&lt;center&gt;New Basis: {$\tilde{\vec{e_1}}, \tilde{\vec{e_2}}$}&lt;/center&gt;
&lt;h3 id=&#34;forward-transformation&#34;&gt;Forward Transformation&lt;/h3&gt;
&lt;p&gt;Suppose:
$$\tilde{\vec{e_1}} = 2\vec{e_1}+1\vec{e_2}$$
$$\tilde{\vec{e_2}} = -\frac{1}{2}\vec{e_1}+\frac{1}{4}\vec{e_2}$$
$$F = \begin{pmatrix}2 &amp;amp; 1\\ -\frac{1}{2} &amp;amp; \frac{1}{4}\end{pmatrix}$$
where $F$ is the Forward Transformation Matrix.&lt;/p&gt;
&lt;h3 id=&#34;backward-transformation&#34;&gt;Backward Transformation&lt;/h3&gt;
&lt;p&gt;This would make the backward transformation:
$$\vec{e_1} = \frac{1}{4}\tilde{\vec{e_1}}+(-1)\tilde{\vec{e_2}}$$
$$\vec{e_2} = \frac{1}{2}\tilde{\vec{e_1}}+2\tilde{\vec{e_2}}$$
$$B = \begin{pmatrix}\frac{1}{4} &amp;amp; -1\\ \frac{1}{2} &amp;amp; 2\end{pmatrix}$$&lt;/p&gt;
&lt;h3 id=&#34;forward-and-backward-transformation&#34;&gt;Forward and Backward Transformation&lt;/h3&gt;
&lt;p&gt;$$F \cdot B = \begin{pmatrix}2 &amp;amp; 1\\ -\frac{1}{2} &amp;amp; \frac{1}{4}\end{pmatrix} \begin{pmatrix}\frac{1}{4} &amp;amp; -1\\ \frac{1}{2} &amp;amp; 2\end{pmatrix} = \begin{pmatrix}1 &amp;amp; 0\\ 0 &amp;amp; 1\end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;In general:
$$F \cdot B = \delta_{ij}$$
$$F = B^{-1}; B = F^{-1}$$
$$\tilde{\vec{e_i}} = \sum_{j=1}^{n} F_{ij} \vec{e_j}$$
$$\vec{e_i} = \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}}$$&lt;/p&gt;
&lt;h1 id=&#34;vectors&#34;&gt;Vectors&lt;/h1&gt;
&lt;p&gt;Vectors are the first example of a tensor. Vectors are invariant but their components are &lt;em&gt;NOT&lt;/em&gt; invariant. It is also important to know that not all vectors are geometrical Euclidean vectors. Some vectors that are harder to visualize.&lt;/p&gt;
&lt;p&gt;The transformation rules for vectors behave in an opposite way compared to the basis vectors {$\vec{e_1}, \vec{e_2}$}&lt;/p&gt;
&lt;p&gt;$$\vec{v} = \sum_{j=1}^{n}v_j \vec{e_j} = \sum_{i=1}^{n} \tilde{v_i} \tilde{\vec{e_i}}$$&lt;/p&gt;
&lt;p&gt;$$\vec{v} = \sum_{j=1}^{n} v_j \vec{e_j} = \sum_{j=1}^{n} v_j \left( \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}} \right) = \sum_{i=1}^{n} \left( \sum_{j=1}^{n} B_{ij} v_j \right) \tilde{\vec{e_i}}$$&lt;/p&gt;
&lt;p&gt;So this shows that:
$$\tilde{v_i} = \sum_{j=1}^{n} B_{ij} v_j$$
Which basically means that if you want to define the vector is the new basis, we have to use the backwards transformation matrix. This will take some getting used to since it is opposite of the unit basis. For this reason, vectors are said to contravary and are even called &lt;strong&gt;contravariant vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Basis&lt;/th&gt;
&lt;th&gt;Vectors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$$\tilde{\vec{e_i}} = \sum_{j=1}^{n} F_{ij} \vec{e_j}$$&lt;/td&gt;
&lt;td&gt;$$\tilde{v_i} = \sum_{j=1}^{n} B_{ij} v_j$$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$\vec{e_i} = \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}}$$&lt;/td&gt;
&lt;td&gt;$$v_i = \sum_{j=1}^{n} F_{ij} \tilde{v_j}$$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
# Notation
In tensor calculus, notation can be tricky so it will be helpful to keep a couple tips about notation in mind:
&lt;ul&gt;
&lt;li&gt;Upper indicies represent contravariant components&lt;/li&gt;
&lt;li&gt;Lower indicies represent covariant components&lt;/li&gt;
&lt;li&gt;Einstein notation uses the upper and lower indicies and also drops the $\sum$ symbol&lt;/li&gt;
&lt;li&gt;When a covector $\alpha_j$ is acting on a vector $v^j$, it can be written as $\alpha_j v^j$ and is assumed to be the sum:
$$\alpha_j v^j = \sum_{j=1}^n \alpha_j v^j = \alpha_1 v^1 + \alpha_2 v^2 + \alpha_3 v^3 + &amp;hellip; + \alpha_n v^n$$&lt;/li&gt;
&lt;li&gt;Using this new notation convention, the formulas in the table above can be written as:&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Basis&lt;/th&gt;
&lt;th&gt;Vectors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$$\tilde{\vec{e_i}} = F_i^j \vec{e_j}$$&lt;/td&gt;
&lt;td&gt;$$\tilde{v^i} = B_j^i v^j$$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$$\vec{e_i} = B_i^j \tilde{\vec{e_j}}$$&lt;/td&gt;
&lt;td&gt;$$v^i = F_j^i \tilde{v^j}$$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;h1 id=&#34;covectors&#34;&gt;Covectors&lt;/h1&gt;
&lt;p&gt;Covectors may be harder to visualize since it differs from the arrow Euclidean vectors that is often used in physics. Here are a couple initial notes about covectors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Covectors can be thought of as row vectors but clarification is needed for this. Row vectors in this sense are not necessarily column vectors flipped on their side. This is only true when using an orthonormal basis. If the basis is not orthonormal, it is more apparent how row vectors are different from column vectors.&lt;/li&gt;
&lt;li&gt;It is better to think of covectors as functions that act on vectors and map them to real numbers:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\alpha: V \rightarrow \mathbb{R}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When we visualize vectors, we think of them as Euclidean vectors with components in a system of coordinates. A covector can be visualized as directed stacks of lines (or surfaces). Below are a few images that can help visualize covectors:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;img/covector1.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;img/covector0.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;img/covector2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The inner product is a mechanism used to combine a covector and vector. The output is a number that represents the number of surfaces of the covector that are pierced by the vector.&lt;/p&gt;
&lt;h3 id=&#34;covector-rules-to-keep-in-mind&#34;&gt;Covector Rules To Keep In Mind:&lt;/h3&gt;
&lt;p&gt;Covector acting on a vector:
$$\alpha(\vec{v}) = \alpha_1 v^1 + \alpha_2 v^2 + \alpha_3 v^3 + &amp;hellip; + \alpha_n v^n = \sum_{j=1}^n \alpha_j v^j$$&lt;/p&gt;
&lt;p&gt;Properties of Linearity:
$$\alpha(\vec{v} + \vec{w}) = \alpha(\vec{v}) + \alpha(\vec{w})$$&lt;/p&gt;
&lt;p&gt;$$\alpha(n \vec{v}) = n \alpha(\vec{v})$$&lt;/p&gt;
&lt;p&gt;$$(\beta + \gamma)(\vec{v}) = \beta(\vec{v}) + \gamma(\vec{v})$$&lt;/p&gt;
&lt;p&gt;Vector Spaces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When scaling and combining vectors, a vector space $V$ is spanned&lt;/li&gt;
&lt;li&gt;Covectors can also be scaled with scalars and combined using addition and multiplication. The vector space spanned by covectors is called the &amp;ldquo;Dual Vector Space&amp;rdquo;, $V^*$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;linear-maps&#34;&gt;Linear Maps&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>Complex Systems News Resources</title>
      <link>/post/news/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/news/</guid>
      <description>&lt;h1 id=&#34;keeping-up-with-complex-systems-science-news&#34;&gt;Keeping Up With Complex Systems Science News&lt;/h1&gt;
&lt;p&gt;It can be difficult to find time to keep up with interesting science, especially when articles are dense and abstruse. With an unfortunate name like complex systems, keeping up with the intersting news can be a daunting task. Here are a few resources that I use to keep up with the fascinating world of complex systems and systems engineering.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;centersanta-fe-institutehttpswwwsantafeedunews-centernewscenter&#34;&gt;&lt;center&gt;&lt;a href=&#34;https://www.santafe.edu/news-center/news&#34;&gt;Santa Fe Institute&lt;/a&gt;&lt;/center&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/santafe_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/santafe_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;centersante-fe-institute-complexity-explorerhttpswwwcomplexityexplorerorgexplorebrowsecenter&#34;&gt;&lt;center&gt;&lt;a href=&#34;https://www.complexityexplorer.org/explore/browse&#34;&gt;Sante Fe Institute Complexity Explorer&lt;/a&gt;&lt;/center&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/explorer_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/explorer_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;centerquanta-magazinequantamagazineorgcenter&#34;&gt;&lt;center&gt;&lt;a href=&#34;quantamagazine.org&#34;&gt;Quanta Magazine&lt;/a&gt;&lt;/center&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/quanta_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/quanta_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
 &lt;hr&gt;
&lt;h1 id=&#34;centernaturehttpswwwnaturecomsubjectscomplexitycenter&#34;&gt;&lt;center&gt;&lt;a href=&#34;https://www.nature.com/subjects/complexity&#34;&gt;Nature&lt;/a&gt;&lt;/center&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/nature_logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Nature journal has a &lt;strong&gt;Complexity&lt;/strong&gt; page that shows the latest new for complex systems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/nature_example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R Programming Data Types</title>
      <link>/post/r-data-types/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/r-data-types/</guid>
      <description>&lt;h1 id=&#34;truth-is-ever-to-be-found-in-simplicity-and-not-in-the-multiplicity-and-confusion-of-thingsbrisaac-newton&#34;&gt;&amp;ldquo;Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things.&amp;ldquo;&lt;br&gt;Isaac Newton&lt;/h1&gt;
&lt;p&gt;I approached R in the same way I would any language. I immediately delve into for-loops, conditional statements, user-defined functions, classes, and so on. I didn&#39;t pay much attention to data types at first - assuming they&#39;re not much different than what I&#39;ve seen already. I found myself using dataframes and matricies often with low confidence and a lingering confusion. I needed to know how these R data structures were related. I finally created these notes for myself to get a grip on the topic. Hopefully you find value in them as well.&lt;/p&gt;
&lt;p&gt;The data structures we will cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#vector&#34;&gt;Vectors&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#matrix&#34;&gt;Matricies&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#array&#34;&gt;Arrays&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#list&#34;&gt;Lists&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#dataframe&#34;&gt;Data Frames&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#factor&#34;&gt;Factors&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;#table&#34;&gt;Tables&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each data type, we will review the basics of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Creation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adding Element&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deleting Elements&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indexing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;and More&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namevectorspan-stylecolor-2980b9vectorsspana&#34;&gt;&lt;a name=&#34;vector&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Vectors&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;All elements in an R vector must have the same mode: &lt;em&gt;integer&lt;/em&gt;, &lt;em&gt;numeric&lt;/em&gt;, &lt;em&gt;character&lt;/em&gt;, &lt;em&gt;logical&lt;/em&gt;, &lt;em&gt;complex&lt;/em&gt;, etc.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- c(88, 12, 23, 74)
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 88 12 23 74
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-elementspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Adding -44 to vector &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- c(x,-44)
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1]  88  12  23  74 -44
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x[5] &amp;lt;- -44
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1]  88  12  23  74 -44
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cremove-elementspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Remove Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Remove 23 from &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- x[-3]
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1]  88  12  74 -44
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&#39;s possible to remove several items at once:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- x[-3:-5]
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 88 12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cindexingspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Indexing&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- rep(1,10)
x[4] &amp;lt;- 3
x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##  [1] 1 1 1 3 1 1 1 1 1 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x[4]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cfilteringspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Filtering&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x[6] &amp;lt;- 5
x[9] &amp;lt;- 2
x[x &amp;gt; 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 3 5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccombining-vectorsspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Combining Vectors&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Find the length of a vector with &lt;code&gt;length(x)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;When adding two vectors, the lengths of the vectors must be the same or one must be a multiple length of the other. When a vector isn&#39;t long enough to add to another vectors, it will keep repeating itself however many times it needs in order for the lengths to match.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- x + x; y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##  [1]  2  2  2  6  2 10  2  2  4  2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;z &amp;lt;- x + c(1,2,3,4,5); z
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##  [1] 2 3 4 7 6 6 3 4 6 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;error &amp;lt;- x + c(1,2,3,4); error
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## Warning in x + c(1, 2, 3, 4): longer object length is not a multiple of
    ## shorter object length

    ##  [1] 2 3 4 7 2 7 4 5 3 3
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namematrixspan-stylecolor-2980b9matriciesspana&#34;&gt;&lt;a name=&#34;matrix&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Matricies&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;A matrix is essentially a vector with two attributes. All the columns in a matrix must have the same mode: &lt;em&gt;integer&lt;/em&gt;, &lt;em&gt;numeric&lt;/em&gt;, &lt;em&gt;character&lt;/em&gt;, &lt;em&gt;logical&lt;/em&gt;, &lt;em&gt;complex&lt;/em&gt;, etc. in the same way it does for a vector. Matricies are special cases of a more general R type of object: &lt;em&gt;arrays&lt;/em&gt; - which we will read about next. Arrays can be multidimensional.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;One way to create a matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- matrix(c(1,2,3,4), nrow = 2, ncol = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or simply:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y &amp;lt;- matrix(c(1,2,3,4), nrow = 2)
y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    3
    ## [2,]    2    4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;byrow&lt;/code&gt; argument (default = FALSE):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- matrix(c(1,2,3,4,5,6), nrow = 2, byrow = T)
m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2] [,3]
    ## [1,]    1    2    3
    ## [2,]    4    5    6
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-and-removing-rows-and-columnsspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding and Removing Rows and Columns&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Rows and columns may be added and deleting from a matrix with operations analogous to the vector operations of adding and deleting. These functions are &lt;code&gt;rbind&lt;/code&gt; and &lt;code&gt;cbind&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Adding a column:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ones_column &amp;lt;- matrix(rep(1,2)); ones_column; m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1]
    ## [1,]    1
    ## [2,]    1

    ##      [,1] [,2] [,3]
    ## [1,]    1    2    3
    ## [2,]    4    5    6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cbind(m, ones_column)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2] [,3] [,4]
    ## [1,]    1    2    3    1
    ## [2,]    4    5    6    1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding a row: (don&#39;t forgot to adjust the row number: &lt;code&gt;nrow = 1&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ones_row &amp;lt;- matrix(rep(1,3), nrow = 1); ones_row; m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2] [,3]
    ## [1,]    1    1    1

    ##      [,1] [,2] [,3]
    ## [1,]    1    2    3
    ## [2,]    4    5    6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rbind(ones_row, m)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2] [,3]
    ## [1,]    1    1    1
    ## [2,]    1    2    3
    ## [3,]    4    5    6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rows may be added by creating matricies and copying:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;new_matrix &amp;lt;- matrix(nrow = 3, ncol = 3)

addded_row &amp;lt;- matrix(c(7,8,9), nrow = 1)

new_matrix[1:2,1:3] &amp;lt;- m
new_matrix[3,1:3] &amp;lt;- addded_row
new_matrix
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2] [,3]
    ## [1,]    1    2    3
    ## [2,]    4    5    6
    ## [3,]    7    8    9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use &lt;code&gt;rbind&lt;/code&gt; and &lt;code&gt;cbind&lt;/code&gt; to reassign values. This is a form of deleting data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- matrix(1:6, nrow = 3); m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    4
    ## [2,]    2    5
    ## [3,]    3    6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m &amp;lt;- m[c(1,3),]; m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    4
    ## [2,]    3    6
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cindexingspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Indexing&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To retrieve information from a matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m[,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 4 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m[2,]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 3 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m[2,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Values may be changed in a matrix as well:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;m[2,2] &amp;lt;- 66; m
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    4
    ## [2,]    3   66
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cfilteringspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Filtering&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- matrix(c(1,2,3,2,3,4), nrow = 3, byrow = F); x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    2
    ## [2,]    2    3
    ## [3,]    3    4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x[x[,2] &amp;gt;= 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 2 3 3 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j &amp;lt;- x[,2] &amp;gt;= 3
x[j,]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    2    3
    ## [2,]    3    4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cmatrix-mathspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Matrix Math&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    1    3
    ## [2,]    2    4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mathematical Matrix Multiplication&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y %*% y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    7   15
    ## [2,]   10   22
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mathematical Muliplication of Matrix by Scalar&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;3*y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    3    9
    ## [2,]    6   12
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mathematical Matrix Addition&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y + y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##      [,1] [,2]
    ## [1,]    2    6
    ## [2,]    4    8
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namearrayspan-stylecolor-2980b9arraysspana&#34;&gt;&lt;a name=&#34;array&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Arrays&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The mechanics of an array is very similar to that of a matrix in R. Unlike a matrix, an array can represent data in higher than two dimensions. We may build a three-dimensional array by conbining two matricies, we can build four-dimensional arrays by combining two or more three-dimensional arrays, and so on.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namelistspan-stylecolor-2980b9listsspana&#34;&gt;&lt;a name=&#34;list&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Lists&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;List are unique in that not all elements have to be of the same mode. List structures can combine different types. An R list is similar to a Python dictionary or C struct. List form the foundation for data frames, object oriented programming (R classes), and more.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;If we wanted to create an employee database, we could start with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j &amp;lt;- list(name = &amp;quot;Eric&amp;quot;, salary = 45000, union = T)
j
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $name
    ## [1] &amp;quot;Eric&amp;quot;
    ## 
    ## $salary
    ## [1] 45000
    ## 
    ## $union
    ## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The component names are called &lt;em&gt;tags&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-elementspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;New components can be added after a list is created:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;z &amp;lt;- list(a = &amp;quot;abc&amp;quot;, b = 12)
z
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $a
    ## [1] &amp;quot;abc&amp;quot;
    ## 
    ## $b
    ## [1] 12
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;z$c &amp;lt;- &amp;quot;sailing&amp;quot; # add a c component
z
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $a
    ## [1] &amp;quot;abc&amp;quot;
    ## 
    ## $b
    ## [1] 12
    ## 
    ## $c
    ## [1] &amp;quot;sailing&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding component can also be done via a vector index:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;z[[4]] &amp;lt;- 28
z[5:7] &amp;lt;- c(F,T,T)
z
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $a
    ## [1] &amp;quot;abc&amp;quot;
    ## 
    ## $b
    ## [1] 12
    ## 
    ## $c
    ## [1] &amp;quot;sailing&amp;quot;
    ## 
    ## [[4]]
    ## [1] 28
    ## 
    ## [[5]]
    ## [1] FALSE
    ## 
    ## [[6]]
    ## [1] TRUE
    ## 
    ## [[7]]
    ## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also concatenate lists:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cat &amp;lt;- c(list(&amp;quot;Joe&amp;quot;, 55000, T), list(5)); cat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [[1]]
    ## [1] &amp;quot;Joe&amp;quot;
    ## 
    ## [[2]]
    ## [1] 55000
    ## 
    ## [[3]]
    ## [1] TRUE
    ## 
    ## [[4]]
    ## [1] 5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cremove-elementspan-1&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Remove Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;You can delete a list component by setting it equal to &lt;code&gt;NULL&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;z$b &amp;lt;- NULL
z
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $a
    ## [1] &amp;quot;abc&amp;quot;
    ## 
    ## $c
    ## [1] &amp;quot;sailing&amp;quot;
    ## 
    ## [[3]]
    ## [1] 28
    ## 
    ## [[4]]
    ## [1] FALSE
    ## 
    ## [[5]]
    ## [1] TRUE
    ## 
    ## [[6]]
    ## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cindexingspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Indexing&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;You can access a list component in several different ways:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j$salary
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 45000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j[[&amp;quot;salary&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 45000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 45000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;What&#39;s the deal with the single and double brackets?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If single brackets are used, the result is another list - a sublist of the original.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j1 &amp;lt;- j[1:2]; j1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## $name
    ## [1] &amp;quot;Eric&amp;quot;
    ## 
    ## $salary
    ## [1] 45000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If double brackets are used, it is for referring to a single component and is return in the type of the component.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;j[[2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 45000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following returns an error since it&#39;s trying to return several components using a function that is meant to return one:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# j[[1:2]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cfilteringspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Filtering&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Accessing list components:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(j)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] &amp;quot;name&amp;quot;   &amp;quot;salary&amp;quot; &amp;quot;union&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get the specific values instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ulj &amp;lt;- unlist(j); ulj
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##    name  salary   union 
    ##  &amp;quot;Eric&amp;quot; &amp;quot;45000&amp;quot;  &amp;quot;TRUE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each values above has a name. This name may be removed with the following function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(ulj) &amp;lt;- NULL
ulj
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] &amp;quot;Eric&amp;quot;  &amp;quot;45000&amp;quot; &amp;quot;TRUE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;using-lapply-and-sapply-functions&#34;&gt;Using &lt;code&gt;lapply()&lt;/code&gt; and &lt;code&gt;sapply()&lt;/code&gt; functions&lt;/h5&gt;
&lt;p&gt;This applies a specific function on each of the compoenents of a list and returns another list:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;lapply(list(1:3,25:29), median)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [[1]]
    ## [1] 2
    ## 
    ## [[2]]
    ## [1] 27
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sapply()&lt;/code&gt; returns a vector-valued answer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sapply(list(1:3,25:29), median)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1]  2 27
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3crecursive-listsspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Recursive Lists&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;You can have lists within lists:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;b &amp;lt;- list(u = 5, v = 12)
c &amp;lt;- list(w = 13)
a &amp;lt;- list(b, c)
a
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [[1]]
    ## [[1]]$u
    ## [1] 5
    ## 
    ## [[1]]$v
    ## [1] 12
    ## 
    ## 
    ## [[2]]
    ## [[2]]$w
    ## [1] 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TIP: The concatenate function c() has an optional argument &lt;code&gt;recursive&lt;/code&gt;, which controls whether &lt;em&gt;flattening&lt;/em&gt; occurs when recursive lists are combined.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namedataframespan-stylecolor-2980b9data-framesspana&#34;&gt;&lt;a name=&#34;dataframe&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Data Frames&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-4&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Data frames are similar to a two dimensional matrix in that it contains rows and columns structure. However, data frame are heterogeneous; columns can be different modes. Technically, a data frame is a list whose components are equal-lengthed vectors as the columns of the data frame. Data frame are commonly used when doing data manipulation and other data analysis techniques in R.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Creating a data frame from scratch:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;scientists &amp;lt;- c(&amp;quot;Einstein&amp;quot;, &amp;quot;Newton&amp;quot;)
born &amp;lt;- c(1879, 1642)

d &amp;lt;- data.frame(scientists, born, stringsAsFactors = FALSE)
d
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##   scientists born
    ## 1   Einstein 1879
    ## 2     Newton 1642
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the named argument &lt;code&gt;stringsAsFactors&lt;/code&gt; is not specified, then by default, &lt;code&gt;stringsAsFactors&lt;/code&gt; will be TRUE.&lt;/p&gt;
&lt;p&gt;Data frames can also be created from external files (.csv, .mtp, .xls, .spss, .txt) using: &lt;br&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;mydata = read.csv(&amp;quot;mydata.csv&amp;quot;, header = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;mydata = read.mtp(&amp;quot;mydata.mtp&amp;quot;)  # read from .mtp file
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;mydata = read.xls(&amp;quot;mydata.xls&amp;quot;)  # read from first sheet
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;mydata = read.spss(&amp;quot;myfile&amp;quot;, to.data.frame=TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;mydata = read.table(&amp;quot;mydata.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and many more options.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-elementspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;rbind()&lt;/code&gt; and &lt;code&gt;cbind()&lt;/code&gt; matrix functions also work in data frames to add new rows or columns of the same length.&lt;/p&gt;
&lt;p&gt;Adding a new row:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##   kids ages
    ## 1 jack   12
    ## 2 Jill   10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;rbind(d1, list(&amp;quot;laura&amp;quot;, 19))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##    kids ages
    ## 1  jack   12
    ## 2  Jill   10
    ## 3 laura   19
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding a column&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3cremove-elementspan-2&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Remove Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Data deletion in a data frame is similar to that of a vector.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##    kids ages
    ## 1  jack   12
    ## 2  Jill   10
    ## 3 laura   19
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d2 &amp;lt;- d2[-2,]
d2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##    kids ages
    ## 1  jack   12
    ## 3 laura   19
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cindexingspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Indexing&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d[[1]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] &amp;quot;Einstein&amp;quot; &amp;quot;Newton&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d$scientists
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] &amp;quot;Einstein&amp;quot; &amp;quot;Newton&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may also access elements in a matrix-like way we well:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;d[,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] &amp;quot;Einstein&amp;quot; &amp;quot;Newton&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can be helpful to know the structure of the data frame and is easy to achieve:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(d)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## &#39;data.frame&#39;:    2 obs. of  2 variables:
    ##  $ scientists: chr  &amp;quot;Einstein&amp;quot; &amp;quot;Newton&amp;quot;
    ##  $ born      : num  1879 1642
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cfilteringspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Filtering&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Let&#39;s take a look at how to filter data in a data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cars &amp;lt;- cars[c(&amp;quot;mpg&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;,&amp;quot;cyl&amp;quot;)]
head(cars)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##                    mpg  hp    wt cyl
    ## Mazda RX4         21.0 110 2.620   6
    ## Mazda RX4 Wag     21.0 110 2.875   6
    ## Datsun 710        22.8  93 2.320   4
    ## Hornet 4 Drive    21.4 110 3.215   6
    ## Hornet Sportabout 18.7 175 3.440   8
    ## Valiant           18.1 105 3.460   6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cars[cars$cyl == 8,]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##                      mpg  hp    wt cyl
    ## Hornet Sportabout   18.7 175 3.440   8
    ## Duster 360          14.3 245 3.570   8
    ## Merc 450SE          16.4 180 4.070   8
    ## Merc 450SL          17.3 180 3.730   8
    ## Merc 450SLC         15.2 180 3.780   8
    ## Cadillac Fleetwood  10.4 205 5.250   8
    ## Lincoln Continental 10.4 215 5.424   8
    ## Chrysler Imperial   14.7 230 5.345   8
    ## Dodge Challenger    15.5 150 3.520   8
    ## AMC Javelin         15.2 150 3.435   8
    ## Camaro Z28          13.3 245 3.840   8
    ## Pontiac Firebird    19.2 175 3.845   8
    ## Ford Pantera L      15.8 264 3.170   8
    ## Maserati Bora       15.0 335 3.570   8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cars[,c(&amp;quot;mpg&amp;quot;, &amp;quot;hp&amp;quot;)][cars$wt &amp;lt;= 4,]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##                    mpg  hp
    ## Mazda RX4         21.0 110
    ## Mazda RX4 Wag     21.0 110
    ## Datsun 710        22.8  93
    ## Hornet 4 Drive    21.4 110
    ## Hornet Sportabout 18.7 175
    ## Valiant           18.1 105
    ## Duster 360        14.3 245
    ## Merc 240D         24.4  62
    ## Merc 230          22.8  95
    ## Merc 280          19.2 123
    ## Merc 280C         17.8 123
    ## Merc 450SL        17.3 180
    ## Merc 450SLC       15.2 180
    ## Fiat 128          32.4  66
    ## Honda Civic       30.4  52
    ## Toyota Corolla    33.9  65
    ## Toyota Corona     21.5  97
    ## Dodge Challenger  15.5 150
    ## AMC Javelin       15.2 150
    ## Camaro Z28        13.3 245
    ## Pontiac Firebird  19.2 175
    ## Fiat X1-9         27.3  66
    ## Porsche 914-2     26.0  91
    ## Lotus Europa      30.4 113
    ## Ford Pantera L    15.8 264
    ## Ferrari Dino      19.7 175
    ## Maserati Bora     15.0 335
    ## Volvo 142E        21.4 109
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-namefactorspan-stylecolor-2980b9factorsspana&#34;&gt;&lt;a name=&#34;factor&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Factors&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-5&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The motivation for factors comes from the concept of categorical data in statistics. An R &lt;code&gt;factor&lt;/code&gt; may be viewed as a vector with more information added. The extra information consists of a record of the distinct values on that vector, called levels.&lt;/p&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan-4&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- c(5, 12, 13, 12)
xf &amp;lt;- factor(x)
xf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 5  12 13 12
    ## Levels: 5 12 13
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The distinct values in xf: 5, 12, and 13 are the levels&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(xf)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ##  Factor w/ 3 levels &amp;quot;5&amp;quot;,&amp;quot;12&amp;quot;,&amp;quot;13&amp;quot;: 1 2 3 2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;unclass(xf)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 1 2 3 2
    ## attr(,&amp;quot;levels&amp;quot;)
    ## [1] &amp;quot;5&amp;quot;  &amp;quot;12&amp;quot; &amp;quot;13&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;length(xf)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-elementspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding Element&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Future new levels can be anticipated as well:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- c(5, 12, 13, 12)
xff &amp;lt;- factor(x, levels = c(5, 12, 13, 88))
xff
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 5  12 13 12
    ## Levels: 5 12 13 88
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xff[2] &amp;lt;- 88
xff
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## [1] 5  88 13 12
    ## Levels: 5 12 13 88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although you cannot add a value that doesn&#39;t have a level associated with it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;xff[2] &amp;lt;- 28
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;    ## invalid factor level, NA generated

### &amp;lt;span style=&amp;quot;color:#E74C3C&amp;quot;&amp;gt;Remove Element&amp;lt;/span&amp;gt;

### &amp;lt;span style=&amp;quot;color:#E74C3C&amp;quot;&amp;gt;Indexing&amp;lt;/span&amp;gt;

### &amp;lt;span style=&amp;quot;color:#E74C3C&amp;quot;&amp;gt;Filtering&amp;lt;/span&amp;gt;

### &amp;lt;span style=&amp;quot;color:#E74C3C&amp;quot;&amp;gt;Math&amp;lt;/span&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;a-nametablespan-stylecolor-2980b9tablesspana&#34;&gt;&lt;a name=&#34;table&#34;&gt;&lt;span style=&#34;color: #2980B9&#34;&gt;Tables&lt;/span&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;span-stylecolore74c3cintroductionspan-6&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Introduction&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3ccreationspan-5&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Creation&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3cadding-elementspan-4&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Adding Element&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3cremove-elementspan-3&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Remove Element&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3cindexingspan-4&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Indexing&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3cfilteringspan-4&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Filtering&lt;/span&gt;&lt;/h3&gt;
&lt;h3 id=&#34;span-stylecolore74c3cmathspan&#34;&gt;&lt;span style=&#34;color:#E74C3C&#34;&gt;Math&lt;/span&gt;&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Mathematics of Network Theory</title>
      <link>/post/network-theory/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/network-theory/</guid>
      <description>&lt;p&gt;Graphs may be represented in the form of a matrix. Main types of graphs that may be represented are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Graph&lt;/li&gt;
&lt;li&gt;Multigraph&lt;/li&gt;
&lt;li&gt;Directed Graph&lt;/li&gt;
&lt;li&gt;Weighted Graph&lt;/li&gt;
&lt;li&gt;Bipartite Graph&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;directed-graph&#34;&gt;Directed Graph&lt;/h3&gt;
&lt;p&gt;Directed graphs are graphs that contain edges with direction. Vertices may have inward and outward edges.&lt;/p&gt;
&lt;p&gt;Unlike adjacency matricies for simped graphs, adjacency matricies for directed graphs are non-symmetric. Elements of an adjacency matrix for a directed graph may be denoted as:
$$A_{ij}$$
which represents an edge from vertex $j$ to $i$.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/DirectedGraph1.png&#34; alt=&#34;Directed Graph&#34; width=&#34;500&#34;/&gt;
  &lt;figcaption&gt;Figure 1 — Directed graph with four verticies&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The corresponding adjacency matrix for the graph above is:
$$A = \begin{pmatrix}0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\ 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \end{pmatrix}$$&lt;/p&gt;
&lt;h3 id=&#34;cocitation&#34;&gt;Cocitation&lt;/h3&gt;
&lt;p&gt;The cocitation of two vertices $i$ and $j$ in a directed network is the number of vertices that have outgoing edges pointing to both $i$ and $j$. We can see that:&lt;/p&gt;
&lt;div&gt;$$A_{ik}A_{jk} = 1$$&lt;/div&gt;
&lt;p&gt;if $i$ and $j$ are both cited by $k$. If we sum over all these elements we get the following relation:&lt;/p&gt;
&lt;div&gt;$$C_{ij} = \sum\limits_{k=1}^n A_{ik}A_{jk} = \sum\limits_{k=1}^n A_{ik}A_{kj}^T = AA^T $$&lt;/div&gt;
&lt;p&gt;This is a cocitation network for which there is an edge between $i$ and $j$ if $C_{ij} &amp;gt; 0$, for $i \neq j$.&lt;/p&gt;
&lt;p&gt;The diagonal elements of the cocitation matrix are given by:&lt;/p&gt;
&lt;div&gt;$$C_{ii} = \sum\limits_{k=1}^n A_{ik}^2 = \sum\limits_{k=1}^n A_{ik}$$&lt;/div&gt;
&lt;p&gt;In constructing the cocitation network we ignore these diagonal elements, meaning that the network&#39;s adjacency matrix is equal to the cocitation matrix but with all the diagonal elements set to zero.&lt;/p&gt;
&lt;h3 id=&#34;bibliographic-coupling&#34;&gt;Bibliographic Coupling&lt;/h3&gt;
&lt;p&gt;Cocitation and Bibliographic coupling are similar mathematically but give different results. They&#39;re both affected by the number of in and out edges. Bibliographic Coupling of two vertices are the number of other vertices to which both $i$ and $j$ point to. Bibliographic Coupling is general more stable since the number of citations can vary with time. Bibliographic Coupling is known at time of publishing and doesn&#39;t change at all. This may or may not be a good thing depending on the situation. Mathematically, it can be described by the following:&lt;/p&gt;
&lt;div&gt;$$B_{ij} = \sum\limits_{k=1}^n A_{ki}A_{kj} = \sum\limits_{k=1}^n A_{ik}^TA_{kj} = A^TA $$&lt;/div&gt;
&lt;p&gt;The diagonal elements of $\textbf{B}$ are:&lt;/p&gt;
&lt;div&gt;$$B_{ii} = \sum\limits_{k=1}^n A_{ki}^2 = \sum\limits_{k=1}^n A_{ki}$$&lt;/div&gt;
&lt;p&gt;$B_{ii}$ is equal to the number of other vertices that vertex $i$ points to - the number of papers $i$ cites.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/DirectedGraph2.png&#34; alt=&#34;Directed Graph&#34; width=&#34;400&#34;/&gt;
  &lt;figcaption&gt;Figure 2 — Shows cocitation and bibliographic coupling network comparison&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;hypergraphs&#34;&gt;Hypergraphs&lt;/h3&gt;
&lt;p&gt;Networks with link that join more than two vertices are called hypergraphs. These types of graphs are useful when representing family relations for example. Edges that relate more than two vertices are called hyperedges. In sociology, these networks may be called &lt;em&gt;affiliation networks&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bipartite-networks&#34;&gt;Bipartite Networks&lt;/h3&gt;
&lt;p&gt;Hypergraphs may be difficult to deal with and represent mathematically but a tool that can help are bipartite graphs - a way of conveniently representing the hypergraph structure. In sociology, this may be called: &lt;em&gt;two-mode networks&lt;/em&gt;. Edges only exist between two vertices of unlike-types.&lt;/p&gt;
&lt;p&gt;The adjacency matrix for a bipartite graph is a rectangular matrix called an &lt;em&gt;incidence matrix&lt;/em&gt; which is a $g$ by $n$ matrix where $g$ is the number of groups and $n$ are the number of members in the groups.&lt;/p&gt;
&lt;div&gt;
$$B_{ij} = \begin{cases} 
      1, &amp; \textit{if vertex j belongs to group i} \\
      0, &amp; \textit{otherwise}
\end{cases}$$
&lt;/div&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/BipartiteGraph1.png&#34; alt=&#34;Directed Graph&#34; width=&#34;400&#34;/&gt;
  &lt;figcaption&gt;Figure 3 — Bipartite Graph&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The adjancency matrix for the bipartite graph above can be written as a $4$ by $5$ matrix:&lt;/p&gt;
&lt;p&gt;$$B = \begin{pmatrix}1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0\\0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;This is a much easier way of representing the hypergraph of actors to movies for example. For much info, read section 6.6 (p.125) of Networks - An Introduction (Newman).&lt;/p&gt;
&lt;p&gt;The bipartite graph can be broken down even further by making two one-mode projections. One projection can be made with the &lt;em&gt;groups&lt;/em&gt; side and another can be made with &lt;em&gt;members&lt;/em&gt; side. These projections have the benefit of being simpler to study but are less powerful because information is lost through these projections.&lt;/p&gt;
&lt;p&gt;The two one-mode projections in words are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of groups for which members $i$ and $j$ are both a part of. This is an $n$ x $n$ matrix: $$P = B^TB$$&lt;/li&gt;
&lt;li&gt;The number of common members of groups $i$ and $j$. This is a $g$ x $g$ matrix: $$P&#39;=BB^T$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;quick-thought&#34;&gt;Quick Thought&lt;/h4&gt;
&lt;p&gt;Naturally you want to relate this to cociation and bibliographic coupling networks but it may be confusing to do so. The main difference between cocitation and bibliographic coupling is the direction of the arrows. This bipartite network consists of two different types of nodes and un-directed edges. Therefore, you may have some cyclic thinking if you try to relate them too much. Although The &lt;em&gt;first&lt;/em&gt; projection (the one on the members) is similar to the cocitation network in that the diagonals should be ignored and forced to be zero.&lt;/p&gt;
&lt;h4 id=&#34;information-loss&#34;&gt;Information Loss&lt;/h4&gt;
&lt;p&gt;Although these projections make life a little easier, it does come at a cost: loss of information. Some of the things we loose are the number of groups in the network and the exact membership of each group. If we make the projection weighted graphs, we can at least get information as to how many commons groups a pair of vertices share for example.&lt;/p&gt;
&lt;h3 id=&#34;trees&#34;&gt;Trees&lt;/h3&gt;
&lt;p&gt;A &lt;em&gt;tree&lt;/em&gt; is a connected, undirected network that contains no closed loops. Connected means that every vertex in the network is reachable from every other via some path through the network. A network can also consists of two or more parts. If the individual parts of the network are trees, the then network as a whole is considered a forest. There are leaves on a tree - vertices with one edge on them but topologically, there isn&#39;t really a root.&lt;/p&gt;
&lt;p&gt;The most important property of a tree is that, since there are no closed loops, there is only one path between any pair of vertices. In a forest, there is at most one path but there may be none.&lt;/p&gt;
&lt;p&gt;Another very useful property of trees is that a tree of $n$ vertices always has $n-1$ edges. The reverse is also true: any connected network with $n$ vertices and $n-1$ edges is a tree. If such a network were not a tree then there must be a loop in the network somewhere, implying that we could remove an edge without disconnecting any part of the network.&lt;/p&gt;
&lt;h3 id=&#34;planar-network&#34;&gt;Planar Network&lt;/h3&gt;
&lt;p&gt;Simply put, a planar network is a network that can be drawn on a plane without having any edges cross. All trees are planar but most of the time, network are not planar (e.g., citation networks, metabolic networks, internet, etc.). Some networks are forced to be planar because of physics space constraints such as rivers or road networks.&lt;/p&gt;
&lt;p&gt;These types of networks play an important role in the &lt;em&gt;four-color theorem&lt;/em&gt; which state that the number of colors required to color a graph in this way is called the chromatic number of the graph and many mathematical results are known about chromatic numbers.&lt;/p&gt;
&lt;p&gt;An important to point out is that there is a method of determining if a network is planar. It&#39;s fairly easy to tell by observation if the network is small but when the network is very large, a general method is required.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kuratowski&#39;s Theorem&lt;/em&gt;: Every non-planar network contains a least one subgraph that is an expansion of $K_5$ and $UG$. (Read more about this on p. 132 of Networks - an Introduction (Newman)).&lt;/p&gt;
&lt;h3 id=&#34;degree&#34;&gt;Degree&lt;/h3&gt;
&lt;h4 id=&#34;mean-degree&#34;&gt;Mean Degree&lt;/h4&gt;
&lt;p&gt;We will denote the degree of vertex $i$ by $k_i$. For an undirected graph of n vertices the degree can be written in terms of the adjacency matrix as:&lt;/p&gt;
&lt;div&gt;$$k_i = \sum\limits_{j=1}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;Every edge in an undirected graph has two ends and if there are m edges in total then there are $2m$ ends of edges. But the number of ends of edges is also equal to the sum of the degrees of all the vertices, so&lt;/p&gt;
&lt;div&gt;$$2m = \sum\limits_{i=1}^n k_i$$&lt;/div&gt;
&lt;p&gt;Another way of writing this that is more intuitive is:&lt;/p&gt;
&lt;div&gt;$$m = \frac{1}{2}\sum\limits_{i=1}^n k_i = \frac{1}{2}\sum\limits_{ij}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;The mean degree $c$ of an undirected graph is:&lt;/p&gt;
&lt;div&gt;$$c = \frac{1}{n} \sum\limits_{i=1}^n k_i$$&lt;/div&gt;
&lt;p&gt;And combining this with the earlier equation:&lt;/p&gt;
&lt;p&gt;$$c = \frac{2m}{n}$$&lt;/p&gt;
&lt;h4 id=&#34;density&#34;&gt;Density&lt;/h4&gt;
&lt;p&gt;The maximum possible number of edges in a simple graph is $\binom{n}{2} = \frac{1}{2}n(n-1)$. The connectance or density $\rho$ of a graph is the fraction of these edges that are actually present:&lt;/p&gt;
&lt;p&gt;$$\rho = \frac{m}{\binom{n}{2}}=\frac{2m}{n(n-1)}=\frac{c}{n-1}$$&lt;/p&gt;
&lt;p&gt;When the network is sufficiently large, $\rho$ may be approximated with just $\frac{c}{n}$.&lt;/p&gt;
&lt;p&gt;A network where $\rho$ tends to a constant as $n \rightarrow \infty$ is said to be &lt;em&gt;dense&lt;/em&gt;. A network in which $\rho \rightarrow 0$ as $n \rightarrow \infty$ is said to be &lt;em&gt;sparse&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;directed-network-degree&#34;&gt;Directed Network Degree&lt;/h4&gt;
&lt;p&gt;Vertex degrees in a directed network are more complicated. They are broken up into &lt;em&gt;in-degree&lt;/em&gt; and &lt;em&gt;out-degree&lt;/em&gt;. If $A_{ij}$ is the adjacency matrix of a directed network, the *in* and *out* degree can be written as:&lt;/p&gt;
&lt;div&gt;$$k_i^{in} = \sum\limits_{j=1}^n A_{ij},\ \ \  k_j^{out} = \sum\limits_{i=1}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;We also know the number of edges are:&lt;/p&gt;
&lt;div&gt;$$m = \sum	\limits_{i=1}^n k_i^{in} = \sum\limits_{j=1}^n k_j^{out} = \sum	\limits_{ij} A_{ij}$$&lt;/div&gt;
&lt;p&gt;As far as the mean degree of directed networks:&lt;/p&gt;
&lt;div&gt;$$c_{in} = \frac{1}{n} \sum\limits_{i=1}^n k_i^{in} = \frac{1}{n} \sum\limits_{j=1}^n k_j^{out} = c_{out}$$&lt;/div&gt;
&lt;p&gt;Combining these two relations, the mean degree can concisely be written as:&lt;/p&gt;
&lt;p&gt;$$c = \frac{m}{n}$$&lt;/p&gt;
&lt;h3 id=&#34;paths&#34;&gt;Paths&lt;/h3&gt;
&lt;p&gt;A path along a network is a route across the network moving from vertex to vertex along the edges. In a directed network, the path can on go in the direction of the edge but can go either way for an undirected network. A path may reach a vertex or go along an edge it has seen before. A path that does not intersect itself is considered a &lt;em&gt;self-avoiding path&lt;/em&gt;. Geodesic paths and Hamiltonian paths are two special cases of self-avoiding paths.&lt;/p&gt;
&lt;p&gt;The number of paths of length $r$ may be important to study and can be calculated for directed and undirected networks. We will use the fact that for directed and undirected networks, $A_{ij}$ is 1 if there is an edge from vertex $j$ to vertex $i$, and 0 otherwise. We can start by asking how many paths of length 2 are there in a network. Imagine we want to study all paths of length 2 from $j$ to $i$ via $k$. The product $A_{ik}A_{kj}$ is 1 where there is a path of length 2 from $j$ to $i$ via $k$, and 0 otherwise.&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{(2)} = \sum\limits_{k=1}^n A_{ik}A_{kj}=\left[A^2\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;We can study the path of length 3 as well. The product $A_{ik}A_{kl}A_{lj}$ is 1 where there exists a path of length 3, and 0 otherwise.&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{(3)} = \sum\limits_{k,l=1}^n A_{ik}A_{kl}A_{lj}=\left[A^3\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;Generalizing to any length $r$ gives:&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{r}=\left[A^r\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;There is a proof of induction on page 137 of Network - An Introduction (Newman).&lt;/p&gt;
&lt;p&gt;Another important thing to consider are loops in a network. The number of loops may be calculated as well.&lt;/p&gt;
&lt;div&gt;$$L_r = \sum\limits_{i=1}^n\left[A^r\right]_{ii}=Tr A^r$$&lt;/div&gt;
&lt;p&gt;There &amp;lsquo;Tr&amp;rsquo; is the trace of a matrix. The number of loops can be written in terms of the eigenvalues of the adjacency matrix as well. The adjacency matrix can be written as $A=UKU^T$ where $U$ is the orthogonal matrix of eigenvectors and $K$ is the orthogonal matrix of eigenvalues:&lt;/p&gt;
&lt;p&gt;$$A^r = (UKU^T)^r = UK^rU^T$$&lt;/p&gt;
&lt;div&gt;$$L_r = Tr(UK^rU^T)=Tr(U^TUK^r)=Tr(k^r)=\sum\limits_i k_i^r$$&lt;/div&gt;
&lt;p&gt;Where $k_i$ is the $i^{th}$ eigenvalue of the adjacency matrix. This applies to directed and undirected graphs. There is one important thing to note when learning about counting the number of loops on length r. For each consideration below, the calculation for determining the number of loops uses the following criteria for counting distinct loops.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although there are loop paths that have the same vertices and same order, if there are different starting points, then they are considered separate loops.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$1\rightarrow 2\rightarrow 3 \rightarrow 1 \ \ \ and \ \ \ 2\rightarrow 3\rightarrow 2 \rightarrow 1$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If loops are in the opposite direction, they are counted as distinct loops.
$$1 \rightarrow 2 \rightarrow 3 \rightarrow 1 \ \ \ and \ \ \ 1 \rightarrow 3 \rightarrow 2 \rightarrow 1$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geodesic-paths&#34;&gt;Geodesic Paths&lt;/h3&gt;
&lt;p&gt;A geodesic path is shortest network distance between vertices in question. This is also called &lt;em&gt;geodesic distance&lt;/em&gt; or &lt;em&gt;shortest distance&lt;/em&gt;. Mathematically, a geodesic distance is the smallest value of r such that $\left[ A^r \right]_{ij} &amp;gt; 0$ between vertices $i$ and $j$.&lt;/p&gt;
&lt;p&gt;It may be the case that no shortest distance exists (for example: for separate components of the network where the distance may be said to be infinity). Another interesting fact - If a path intesects itself, it has a loop and therefore cannot be a geodesic path since it can be shortened by removing this loop.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;diameter&lt;/em&gt; of a graph is the length of the longest geodesic path between any pair of vertices in the network for which a path actually exists.&lt;/p&gt;
&lt;h3 id=&#34;eulerian-and-hamiltonian-paths&#34;&gt;Eulerian and Hamiltonian Paths&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Eulerian Path&lt;/strong&gt;: a path that traverses each edge in the network exactly once&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hamiltonian Path&lt;/strong&gt;: a path that visits each vertex exactly once&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are any vertex degree greater than 2, then the Eulerian path is not self-avoiding since it has to visit vertices more than once in order to traverse tall their edges.&lt;/p&gt;
&lt;h4 id=&#34;kronigsberg-bridges&#34;&gt;Kronigsberg Bridges&lt;/h4&gt;
&lt;p&gt;This problem becomes finding an Eulerian path on this network of bridges and the name is in honor of Euler who presented this problem. Euler observed that since any Eulerian path must both enter and leave every vertex it passes (except for the first and last), there can at most be two vertices with odd degree. All four of the vertices in the Kronigsberg Problem has odd degree. More precisely, there can only be 2 or 0 vertices of odd degree for an Eulerian condition to be possible. With this logic, Euler proved the Kronigsberg problem has no solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lagrangian Mechanics</title>
      <link>/post/gen-coord/</link>
      <pubDate>Tue, 19 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/post/gen-coord/</guid>
      <description>&lt;h1 id=&#34;generalized-coordinates-and-conservation-laws-from-the-lagrangian-formulation-of-theoretical-mechanics&#34;&gt;Generalized Coordinates and Conservation Laws From the Lagrangian Formulation of Theoretical Mechanics&lt;/h1&gt;
&lt;p&gt;These are selected notes from a group of topics I find of particular interest. Although I assume some previous knowledge of classical mechanics from you, I still provide a brief overview of what the Lagrangian method is, where it comes from, how it relates to the more familiar Newtonian formulation, and how these beautiful laws of nature imply conserved quantities in everyday systems. The core of these notes include how we can simplify the Lagrangian method by observing conserved quantities by means of cyclic or &amp;ldquo;ignorable&amp;rdquo; generalized coordinates. The Lagrangian is invariant under variations of these types of coordinates. I will explain how cyclic coordinates and Lagrangian invariance imply conservation laws. There are several results that relate Lagrangian invariance and conserved quantities and they are referred to as Noether&#39;s theorem. I am happy to share what I have learned with you!&lt;/p&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;p&gt;Let&#39;s start with simple laws of motion. They describe how mechanical
systems evolve in time given certain conditions and constraints. Consider
yourself standing on the St. Louis Arch (Gateway Arch, whose width &lt;em&gt;and&lt;/em&gt; height are 630 ft.) for some God awful
reason. You hold out an apple with your hand and drop it so that it is falling
towards the Earth. We can predict, for example, where the apple will be at some later time by using an equation of motion. Say we took air resistance into account or that it then falls into a huge pool of oil or dropping a water balloon instead that oscillates in free space or dropping a single water droplet while it&#39;s raining.
Does the rate at which the water droplet accumulate &amp;ldquo;water-mass&amp;rdquo; increase? And if this drop then fell into the pool of oil&amp;hellip; and if the pool sits on a spring&amp;hellip; or two springs&amp;hellip; or an infinite number of springs and etc. We can study a slew of systems using equations of motion (and some canny logical intuition). Newtonian and
Lagrangian mechanics are just a couple ways a obtaining these types of descriptions of systems we experience everyday.&lt;/p&gt;
&lt;p&gt;The most obvious difference between these methods is the Newtonian method utilize the external &lt;em&gt;forces&lt;/em&gt; of a system and how they apply in
some defined coordinate plane while the Lagrangian focuses on the &lt;em&gt;energy&lt;/em&gt; the system
is experiencing. Now we can begin talking about the Lagrangian and how related these methods actually are.&lt;/p&gt;
&lt;p&gt;The Lagrangian can be written as:&lt;/p&gt;
&lt;p&gt;$$ \mathcal{L} = T - U$$&lt;/p&gt;
&lt;p&gt;The kinetic energy $T$ and the potential energy $U$ are used to make the Lagrangian and
the Lagrangian is used to find the equations of motion with something called the &lt;em&gt;Lagrange&lt;/em&gt; equations. It can be written as:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial q_i}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{q}_i}$$&lt;/p&gt;
&lt;p&gt;where $q_i$ are the generalized coordinates that we are using. Since this is still the overview, I will briefly describe where the &lt;em&gt;Lagrange&lt;/em&gt; equation comes from.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Lagrange&lt;/em&gt; equation is a subset of a far broader area of mathematics called the &lt;em&gt;Calculus of Variations&lt;/em&gt; or &lt;em&gt;Variational Calculus&lt;/em&gt;. It involves finding the maximum and minimum of a quantity that can be expressed as an integral. The general form of what is called the &lt;em&gt;variational problem&lt;/em&gt; is:&lt;/p&gt;
&lt;p&gt;$$S=\int_{x_1}^{x_2} f[y(x), y&amp;rsquo;(x), x]dx$$&lt;/p&gt;
&lt;p&gt;The important part is finding the equation $y(x)$ such that the $S$ integral is stationary
(which means infinitesimal variations of the path $y(x)$ doesn&#39;t change the value of the
integral). After a tedious derivation, we are left with what is called the
&lt;em&gt;Euler-Lagrange&lt;/em&gt; equation. Remember that this result is purely mathematical and hasn&#39;t
been applied to our physics world quite yet.&lt;/p&gt;
&lt;p&gt;We can talk about these variational methods for pages but let&#39;s get to the meat of it
all. The reason why variational calculus and the Euler-Lagrange equation are so
important is because when the function $f[y(x), y&amp;rsquo;(x), x]$ within the integral is the Lagrangian (defined at the top of the page), the integral is called the &lt;em&gt;action&lt;/em&gt; integral and the function that makes the &lt;em&gt;action&lt;/em&gt; integral &lt;em&gt;stationary&lt;/em&gt;
is the equation of motion of the particle in question.&lt;/p&gt;
&lt;p&gt;This result is stated in what is called &lt;em&gt;Hamilton&#39;s Principle&lt;/em&gt;.
It tells us that if we know the energy (and therefore the Lagrangian) of a
system, then we can know about its motion and how it evolves through time.
&lt;em&gt;Hamilton&#39;s Principle&lt;/em&gt; is as follows:&lt;/p&gt;
&lt;h2 id=&#34;hamiltons-principle&#34;&gt;&lt;strong&gt;Hamilton&#39;s Principle&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The actual path which a particle follows between two points 1 and 2 in a given time interval, $t_1$ to $t_2$, is such that the action integral&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;$$S=\int_{t_1}^{t_2} \mathcal{L} dt$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;is stationary when taken along the actual path.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now we may go back to the &lt;em&gt;Lagrange&lt;/em&gt; equation and see that it must provide us with
the path that an actual particle will travel that experiences the forces and energy described by the Lagrangian.
$$\frac{\partial \mathcal{L}}{\partial q_i}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{q}_i}$$&lt;/p&gt;
&lt;p&gt;If what has been written so far seems vague or a bit generalized, it is because this was
intended to be more of a reminder than an introduction. Let&#39;s finish this overview with
the relationship between the Newtonian and Lagrangian formulations.
The equations that are obtained by these methods must be equal; physics does not change
depending on the language we use to describe it; they really are one and the same. Just because
Newton&#39;s laws are usually explicitly written in terms of force doesn&#39;t mean we
can&#39;t express it in terms of another quantity like momentum for example:
$$F = ma = m\dot{v} = \dot{p}$$
Let&#39;s not forget how related force and energy really are:&lt;/p&gt;
&lt;p&gt;$$\Delta T = T_2 - T_1 = \int_{1}^{2}\textbf{F} \cdot d\textbf{r}$$&lt;/p&gt;
&lt;p&gt;$$U(\textbf{r}) = -\int_{r_0}^r F(\textbf{r}&#39;) \cdot d\textbf{r}&amp;lsquo;$$
To relate Lagrangian and Newtonian methods more directly:&lt;/p&gt;
&lt;h2 id=&#34;euler-lagrange-equation&#34;&gt;Euler-Lagrange Equation&lt;/h2&gt;
&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial q_i}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{q}_i}$$
$$\frac{\partial \mathcal{L}}{\partial q_i} = F_i\ \ \ and\ \ \ \frac{\partial \mathcal{L}}{\partial \dot{q}_i} \equiv p_i$$
These are referred to as generalized forces and generalized momenta. We may start our discussion of generalized ignorable coordinates and conservation laws now!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick Fun Fact!&lt;/strong&gt; There also exists a Hamiltonian formulation of Classical
Mechanics that takes advantage of its own coordinate system called &lt;em&gt;Canonical Coordinates&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;generalized-coordinates&#34;&gt;Generalized Coordinates&lt;/h2&gt;
&lt;p&gt;Generalized momenta and generalized forces are not the same thing as the familiar
force and momentum we are used to. We can still relate momentum and force in the
usual manner:
$$F_i = \frac{d}{dt}p_i,$$
except these are understood to be defined using generalized coordinates. The fact that
&lt;em&gt;generalized force&lt;/em&gt; $=$ &lt;em&gt;rate of change of generalized momentum&lt;/em&gt; should not be surprising.
A direct way to think of generalized coordinates is to describe the complete
motion of a system in the fewest number of coordinates. Consider a pendulum bob for one
moment (yes, another pendulum).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/pendulum.png&#34; alt=&#34;Figure 1 - Pendulum&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have a choice of using Cartesian coordinates to describe the motion of this
bob or any other coordinate system we can dream up. The downside of using
Cartesian coordinates $(x,y,z)$, is that $x$, $y$, and $z$ must constantly compensate one another to assure the length L remains constant. We can imagine all the combinations of values that satisfy $(\sqrt{x^2+y^2})^2+l^2(1-z)^2=constant$. Long story short, spherical (or polar) coordinates $(r, \theta, \phi)$ are the most appropriate for this problem for
obvious symmetrical advantages and can be considered the generalized coordinates that define our system.&lt;/p&gt;
&lt;p&gt;Now that we have mentioned what generalized coordinates are, we can gain more insight
on what generalized momenta is. Ordinary momentum by definition is $p = mv$ where $v$ is
one time derivative away from a position variable: $p = m (\dot{x}+\dot{y}+\dot{z})$.
So we can easily see that generalized momenta is simple the mass of an object times the
time derivative of its generalized position vectors! Simple! Generalized force can be found the same way except with two time derivatives. $F_t$ in the Figure 1 is the
generalized force of the pendulum system. This pendulum can escape from the x-y grid universe and be thought of as moving along one generalized coordinate, $\theta(t)$. (The length of the pendulum, L, if fixed but if it weren&#39;t, $r(t)$ could be another generalized coordinate used to describe the system).&lt;/p&gt;
&lt;h2 id=&#34;cyclic-coordinates&#34;&gt;Cyclic Coordinates&lt;/h2&gt;
&lt;p&gt;When the Lagrangian of a system is independent of a generalized coordinate $q_i$, that
coordinate is sometimes called a &lt;em&gt;cyclic&lt;/em&gt; or &lt;em&gt;ignorable&lt;/em&gt; coordinate.
Saying the Lagrangian is independent of a particular variable is exactly what it
sounds like, neither the kinetic nor the potential energy depend on this quantity.
This leads directly to the fact that there exists a conserved quantity!&lt;/p&gt;
&lt;p&gt;We can generally express a Lagrangian that is independent of some generalized
coordinate $q_2$ as:
$$\mathcal{L}=\mathcal{L}(q_1, q_3, q_4, \cdots, \dot{q}_1, \dot{q}_2, \dot{q}_3, \dot{q}_4, \cdots, t)$$&lt;/p&gt;
&lt;p&gt;What this means exactly, is that the generalized momentum that corresponds to $q_2$ is
conserved. This can be written as:
$$\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{q}_2} = \frac{\partial \mathcal{L}}{\partial q_2} = 0$$
$$\frac{\partial \mathcal{L}}{\partial \dot{q}_2} = \kappa$$ where $\kappa$ is constant. We can say that this system exhibits conservation of angular momentum!&lt;/p&gt;
&lt;p&gt;This makes solving equations of motion using Lagrangian even easier than before!&lt;/p&gt;
&lt;p&gt;Let&#39;s take advantage of this property by examining the $\mathcal{L}$ of a standard classical mechanics
problem (one of which never gets old). It is stated as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt; &lt;em&gt;A mass $m$ is free to slide on a frictionless table and is connected, via a string that passes through a hole in the table, to a mass $M$ that hangs below. Assume that $M$
moves in a vertical line only, and assume that the string always remains taut. Figure 2 shows a moment in time of this situation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/tableBall.png&#34; alt=&#34;Figure 2 - Example Problem&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Lagrangian is as follows:
$$\mathcal{L}=T-U$$
$$\mathcal{L}=\frac{1}{2}M \dot{r}^2+\frac{1}{2}m(\dot{r}^2+r^2\dot{\theta}^2)+Mg(l-r)$$
The important thing to notice about this expression is that there is no $\theta$ variable
in it anywhere. The Lagrangian is said to be invariant under variations of the generalized
coordinate $\theta$. The important conclusion to take away from this is that the momentum that corresponds to $\theta$ is conserved or in
this case, the angular momentum $mr^2\dot{\theta}$. Therefore without much investigating we can quickly see that $\frac{d}{dt}(mr^2\dot{\theta})=0$. This comes directly from the &lt;em&gt;Euler-Langrange&lt;/em&gt; equation obtained by varying $\theta$:&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial \theta}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{\theta}}$$
$$0 = \frac{d}{dt}(mr^2\dot{\theta})$$&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;E-L&lt;/em&gt; equation that comes from varying $r$ is:
$$\frac{\partial \mathcal{L}}{\partial r}=\frac{d}{dt}\frac{\partial \mathcal{L}}{\partial \dot{r}}$$
$$(M+m)\ddot{r}=mr\dot{\theta}^2-Mg$$&lt;/p&gt;
&lt;h2 id=&#34;conservation-of-energy&#34;&gt;Conservation of Energy&lt;/h2&gt;
&lt;p&gt;Energy is also another quantity that is often conversed in these types of mechanical problems. We will introduce an important claim that touches on this conservation law. First let&#39;s give a definition of &lt;em&gt;energy&lt;/em&gt; in terms of the &lt;em&gt;Lagrangian&lt;/em&gt;:
$$E \equiv (\sum_{i=1}^{N} \frac{\partial \mathcal{L}}{\partial \dot{q}_i} \dot{q}_i) - \mathcal{L}$$
This may seem random to define the energy in such a way but don&#39;t worry too much about it for now. Without going into too much detail, this result is far from random. In fact, there is a rigorous mathematical reason, called the theory of Legendre transforms, that explains why energy can be written in this form. You see this often in Hamiltonian mechanics since the Hamiltonian is simply the total energy of the system: $T+U$.&lt;/p&gt;
&lt;p&gt;Now we may introduce the claim mentioned earlier:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If $\mathcal{L}$ has no explicit time dependence (that is, if $\frac{\partial \mathcal{L}}{\partial t} = 0$), then E is conserved (that is, $\frac{dE}{dt} = 0$), assuming that the motion obeys the E-L equations.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Without proving it, the following relation summarizes this claim:
$$\frac{dE}{dt}=-\frac{\partial \mathcal{L}}{\partial t}.$$&lt;/p&gt;
&lt;h2 id=&#34;applications&#34;&gt;Applications&lt;/h2&gt;
&lt;p&gt;I programmed this nonlinear double pendulum with Mathematica (Wolfram Language)
&lt;img src=&#34;img/doublependulum_5.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
