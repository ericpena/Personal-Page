<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 11:53:49 -0700</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://ericpena.github.io/machine_learning/basics/index.xml</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/index.xml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_images/index.xml</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_images/index.xml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/index.xml</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/index.xml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/index.xml</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/index.xml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://ericpena.github.io/machine_learning/vectors_matrices_and_arrays/index.xml</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/vectors_matrices_and_arrays/index.xml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accuracy</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/accuracy.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/accuracy.html</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Accuracy # Cross-validate model using accuracy cross_val_score(logit, X, y, scoring=&amp;#34;accuracy&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Adaboost Classifier</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/adaboost_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/adaboost_classifier.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import AdaBoostClassifier from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Adaboost Classifier The most important parameters are base_estimator, n_estimators, and learning_rate.
 base_estimator is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree &amp;ndash; this parameter&amp;rsquo;s default argument.</description>
    </item>
    
    <item>
      <title>Adding Interaction Terms</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/adding_interaction_terms.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/adding_interaction_terms.html</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston from sklearn.preprocessing import PolynomialFeatures import warnings # Suppress Warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Load Boston Housing Dataset # Load the data with only two features boston = load_boston() X = boston.data[:,0:2] y = boston.target Add Interaction Term Interaction effects can be account for by including a new feature comprising the product of corresponding values from the interacting features:</description>
    </item>
    
    <item>
      <title>Agglomerative Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/agglomerative_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/agglomerative_clustering.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import AgglomerativeClustering Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct Agglomerative Clustering In scikit-learn, AgglomerativeClustering uses the linkage parameter to determine the merging strategy to minimize the 1) variance of merged clusters (ward), 2) average of distance between observations from pairs of clusters (average), or 3) maximum distance between observations from pairs of clusters (complete).</description>
    </item>
    
    <item>
      <title>ANOVA F-value For Feature Selection</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/anova_f-value_for_feature_selection.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/anova_f-value_for_feature_selection.html</guid>
      <description>If the features are categorical, calculate a chi-square ($\chi^{2}$) statistic between each feature and the target vector. However, if the features are quantitative, compute the ANOVA F-value between each feature and the target vector.
The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different.
Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.</description>
    </item>
    
    <item>
      <title>Bernoulli Naive Bayes Classifier</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier.html</guid>
      <description>The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).
Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import BernoulliNB Create Binary Feature And Target Data # Create three binary features X = np.random.randint(2, size=(100, 3)) # Create a binary target vector y = np.random.randint(2, size=(100, 1)).ravel() View Feature Data # View first ten observations X[0:10] array([[1, 1, 1], [0, 1, 0], [1, 1, 1], [0, 0, 0], [1, 0, 1], [1, 1, 1], [0, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0]])  Train Bernoulli Naive Bayes Classifier # Create Bernoulli Naive Bayes object with prior probabilities of each class clf = BernoulliNB(class_prior=[0.</description>
    </item>
    
    <item>
      <title>Break Up Dates And Times Into Multiple Features</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/break_up_dates_and_times_into_multiple_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/break_up_dates_and_times_into_multiple_features.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd.DataFrame() # Create five dates df[&amp;#39;date&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=150, freq=&amp;#39;W&amp;#39;) Break Up Dates And Times Into Individual Features # Create features for year, month, day, hour, and minute df[&amp;#39;year&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.year df[&amp;#39;month&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.month df[&amp;#39;day&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.day df[&amp;#39;hour&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.hour df[&amp;#39;minute&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.minute # Show three rows df.head(3) .dataframe thead th { text-align: left; } .</description>
    </item>
    
    <item>
      <title>Calculate Difference Between Dates And Times</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/calculate_difference_between_dates_and_times.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/calculate_difference_between_dates_and_times.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd.DataFrame() # Create two datetime features df[&amp;#39;Arrived&amp;#39;] = [pd.Timestamp(&amp;#39;01-01-2017&amp;#39;), pd.Timestamp(&amp;#39;01-04-2017&amp;#39;)] df[&amp;#39;Left&amp;#39;] = [pd.Timestamp(&amp;#39;01-01-2017&amp;#39;), pd.Timestamp(&amp;#39;01-06-2017&amp;#39;)] Calculate Difference (Method 1) # Calculate duration between features df[&amp;#39;Left&amp;#39;] - df[&amp;#39;Arrived&amp;#39;] 0 0 days 1 2 days dtype: timedelta64[ns]  Calculate Difference (Method 2) # Calculate duration between features pd.Series(delta.days for delta in (df[&amp;#39;Left&amp;#39;] - df[&amp;#39;Arrived&amp;#39;])) 0 0 1 2 dtype: int64  </description>
    </item>
    
    <item>
      <title>Calibrate Predicted Probabilities</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/calibrate_predicted_probabilities.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/calibrate_predicted_probabilities.html</guid>
      <description>Class probabilities are a common and useful part of machine learning models. In scikit-learn, most learning algortihms allow us to see the predicted probabilities of class membership using predict_proba. This can be extremely useful if, for instance, we want to only predict a certain class if the model predicts the probability that they are that class is over 90%. However, some models, including naive Bayes classifiers output probabilities that are not based on the real world.</description>
    </item>
    
    <item>
      <title>Calibrate Predicted Probabilities In SVC</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/calibrate_predicted_probabilities_in_svc.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/calibrate_predicted_probabilities_in_svc.html</guid>
      <description>SVC&amp;rsquo;s use of a hyperplane to create decision regions do not naturally output a probability estimate that an observation is a member of a certain class. However, we can in fact output calibrated class probabilities with a few caveats. In an SVC, Platt scaling can be used, wherein first the SVC is trained, then a separate cross-validated logistic regression is trained to map the SVC outputs into probabilities:
$$P(y=1 \mid x)={\frac {1}{1+e^{(A*f(x)+B)}}}$$</description>
    </item>
    
    <item>
      <title>Chi-Squared For Feature Selection</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/chi-squared_for_feature_selection.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/chi-squared_for_feature_selection.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 Load Data # Load iris data iris = load_iris() # Create features and target X = iris.data y = iris.target # Convert to categorical data by converting data to integers X = X.astype(int) Compare Chi-Squared Statistics # Select two features with highest chi-squared statistics chi2_selector = SelectKBest(chi2, k=2) X_kbest = chi2_selector.fit_transform(X, y) View Results # Show results print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Convert pandas Columns Time Zone</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/convert_pandas_column_timezone.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/convert_pandas_column_timezone.html</guid>
      <description>Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones[0:10] [&#39;Africa/Abidjan&#39;, &#39;Africa/Accra&#39;, &#39;Africa/Addis_Ababa&#39;, &#39;Africa/Algiers&#39;, &#39;Africa/Asmara&#39;, &#39;Africa/Asmera&#39;, &#39;Africa/Bamako&#39;, &#39;Africa/Bangui&#39;, &#39;Africa/Banjul&#39;, &#39;Africa/Bissau&#39;]  Create pandas Series Of Dates # Create ten dates dates = pd.Series(pd.date_range(&amp;#39;2/2/2002&amp;#39;, periods=10, freq=&amp;#39;M&amp;#39;)) Add Time Zone Of pandas Series # Set time zone dates_with_abidjan_time_zone = dates.dt.tz_localize(&amp;#39;Africa/Abidjan&amp;#39;) # View pandas series dates_with_abidjan_time_zone 0 2002-02-28 00:00:00+00:00 1 2002-03-31 00:00:00+00:00 2 2002-04-30 00:00:00+00:00 3 2002-05-31 00:00:00+00:00 4 2002-06-30 00:00:00+00:00 5 2002-07-31 00:00:00+00:00 6 2002-08-31 00:00:00+00:00 7 2002-09-30 00:00:00+00:00 8 2002-10-31 00:00:00+00:00 9 2002-11-30 00:00:00+00:00 dtype: datetime64[ns, Africa/Abidjan]  Convert Time Zone Of pandas Series # Convert time zone dates_with_london_time_zone = dates_with_abidjan_time_zone.</description>
    </item>
    
    <item>
      <title>Convert Strings To Dates</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/convert_strings_to_dates.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/convert_strings_to_dates.html</guid>
      <description>Preliminaries # Load libraries import numpy as np import pandas as pd Create Strings # Create strings date_strings = np.array([&amp;#39;03-04-2005 11:35 PM&amp;#39;, &amp;#39;23-05-2010 12:01 AM&amp;#39;, &amp;#39;04-09-2009 09:09 PM&amp;#39;]) Convert Strings To Timestamps If errors=&amp;quot;coerce&amp;quot; then any problem will not raise an error (the default behavior) but instead will set the value causing the error to NaT (i.e. a missing value).
# Convert to datetimes [pd.to_datetime(date, format=&amp;#34;%d-%m-%Y %I:%M %p&amp;#34;, errors=&amp;#34;coerce&amp;#34;) for date in date_strings] [Timestamp(&#39;2005-04-03 23:35:00&#39;), Timestamp(&#39;2010-05-23 00:01:00&#39;), Timestamp(&#39;2009-09-04 21:09:00&#39;)]  </description>
    </item>
    
    <item>
      <title>Create Baseline Classification Model</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/create_baseline_classification_model.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/create_baseline_classification_model.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.dummy import DummyClassifier from sklearn.model_selection import train_test_split Load Iris Flower Dataset # Load data iris = load_iris() # Create target vector and feature matrix X, y = iris.data, iris.target Split Data Into Training And Test Set # Split into training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Create Dummy Regression Always Predicts The Mean Value Of Target # Create dummy classifer dummy = DummyClassifier(strategy=&amp;#39;uniform&amp;#39;, random_state=1) # &amp;#34;Train&amp;#34; model dummy.</description>
    </item>
    
    <item>
      <title>Create Baseline Regression Model</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/create_baseline_regression_model.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/create_baseline_regression_model.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_boston from sklearn.dummy import DummyRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() # Create features X, y = boston.data, boston.target Split Data Into Training And Test Set # Make test and training split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Create Dummy Regression Always Predicts The Mean Value Of Target # Create a dummy regressor dummy_mean = DummyRegressor(strategy=&amp;#39;mean&amp;#39;) # &amp;#34;Train&amp;#34; dummy regressor dummy_mean.</description>
    </item>
    
    <item>
      <title>Create Interaction Features</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/create_interaction_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/create_interaction_features.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import PolynomialFeatures import numpy as np Create Feature Matrix # Create feature matrix X = np.array([[2, 3], [2, 3], [2, 3]]) Add Interaction Features # Create PolynomialFeatures object with interaction_only set to True interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) # Transform feature matrix interaction.fit_transform(X) array([[ 2., 3., 6.], [ 2., 3., 6.], [ 2., 3., 6.]])  </description>
    </item>
    
    <item>
      <title>Cross Validation Pipeline</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/cross_validation_pipeline.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/cross_validation_pipeline.html</guid>
      <description>The code below does a lot in only a few lines. To help explain things, here are the steps that code is doing:
 Split the raw data into three folds. Select one for testing and two for training. Preprocess the data by scaling the training features. Train a support vector classifier on the training data. Apply the classifier to the test data. Record the accuracy score. Repeat steps 1-5 two more times, once for each fold.</description>
    </item>
    
    <item>
      <title>Cross Validation With Parameter Tuning Using Grid Search</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search.html</guid>
      <description>In machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model&amp;rsquo;s parameters that maximize the accuracy of the model.
In this tutorial we work through an example which combines cross validation and parameter tuning using scikit-learn.</description>
    </item>
    
    <item>
      <title>Cross-Validation</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/cross-validaton.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/cross-validaton.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn import metrics from sklearn.model_selection import KFold, cross_val_score from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler Load Digits Dataset # Load the digits dataset digits = datasets.load_digits() # Create the features matrix X = digits.data # Create the target vector y = digits.target Create Pipeline # Create standardizer standardizer = StandardScaler() # Create logistic regression logit = LogisticRegression() # Create a pipeline that standardizes, then runs logistic regression pipeline = make_pipeline(standardizer, logit) Create k-Fold Cross-Validation # Create k-Fold cross-validation kf = KFold(n_splits=10, shuffle=True, random_state=1) Conduct k-Fold Cross-Validation # Do k-fold cross-validation cv_results = cross_val_score(pipeline, # Pipeline X, # Feature matrix y, # Target vector cv=kf, # Cross-validation technique scoring=&amp;#34;accuracy&amp;#34;, # Loss function n_jobs=-1) # Use all CPU scores Calculate Mean Performance Score # Calculate mean cv_results.</description>
    </item>
    
    <item>
      <title>Custom Performance Metric</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/custom_performance_metric.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/custom_performance_metric.html</guid>
      <description>Preliminaries # Load libraries from sklearn.metrics import make_scorer, r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.datasets import make_regression Create Feature # Generate features matrix and target vector X, y = make_regression(n_samples = 100, n_features = 3, random_state = 1) # Create training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1) Train model # Create ridge regression object classifier = Ridge() # Train ridge regression model model = classifier.</description>
    </item>
    
    <item>
      <title>DBSCAN Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/dbscan_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/dbscan_clustering.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import DBSCAN Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct DBSCAN Clustering DBSCAN has three main parameters to set:
 eps: The maximum distance from an observation for another observation to be considered its neighbor. min_samples: The minimum number of observation less than eps distance from an observation for to be considered a core observation.</description>
    </item>
    
    <item>
      <title>Decision Tree Classifier</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/decision_tree_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/decision_tree_classifier.html</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Decision Tree Using Gini Impurity # Create decision tree classifer object using gini clf = DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;, random_state=0) Train Model # Train model model = clf.fit(X, y) Create Observation To Predict # Make new observation observation = [[ 5, 4, 3, 2]] Predict Observation # Predict observation&amp;#39;s class  model.</description>
    </item>
    
    <item>
      <title>Decision Tree Regression</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/decision_tree_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/decision_tree_regression.html</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeRegressor from sklearn import datasets Load Boston Housing Dataset # Load data with only two features boston = datasets.load_boston() X = boston.data[:,0:2] y = boston.target Create Decision Tree Decision tree regression works similar to decision tree classification, however instead of reducing Gini impurity or entropy, potential splits are measured on how much they reduce the mean squared error (MSE):
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction On Sparse Feature Matrix</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_on_sparse_feature_matrix.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_on_sparse_feature_matrix.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets.load_digits() # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) # Make sparse matrix X_sparse = csr_matrix(X) Create Truncated Singular Value Decomposition # Create a TSVD tsvd = TruncatedSVD(n_components=10) Run Truncated Singular Value Decomposition # Conduct TSVD on sparse matrix X_sparse_tsvd = tsvd.</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction With Kernel PCA</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_with_kernel_pca.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_with_kernel_pca.html</guid>
      <description>Preliminaries # Load libraries from sklearn.decomposition import PCA, KernelPCA from sklearn.datasets import make_circles Create Linearly Inseparable Data # Create linearly inseparable data X, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1) Conduct Kernel PCA # Apply kernal PCA with radius basis function (RBF) kernel kpca = KernelPCA(kernel=&amp;#34;rbf&amp;#34;, gamma=15, n_components=1) X_kpca = kpca.fit_transform(X) View Results print(&amp;#39;Original number of features:&amp;#39;, X.shape[1]) print(&amp;#39;Reduced number of features:&amp;#39;, X_kpca.shape[1]) Original number of features: 2 Reduced number of features: 1  </description>
    </item>
    
    <item>
      <title>Dimensionality Reduction With PCA</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_with_pca.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/dimensionality_reduction_with_pca.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn import datasets Load Data # Load the data digits = datasets.load_digits() Standardize Feature Values # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) Conduct Principal Component Analysis # Create a PCA that will retain 99% of the variance pca = PCA(n_components=0.99, whiten=True) # Conduct PCA X_pca = pca.fit_transform(X) View Results # Show results print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Drop Highly Correlated Features</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/drop_highly_correlated_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/drop_highly_correlated_features.html</guid>
      <description>Preliminaries # Load libraries import pandas as pd import numpy as np Load Data # Create feature matrix with two highly correlated features X = np.array([[1, 1, 1], [2, 2, 0], [3, 3, 1], [4, 4, 0], [5, 5, 1], [6, 6, 0], [7, 7, 1], [8, 7, 0], [9, 7, 1]]) # Convert feature matrix into DataFrame df = pd.DataFrame(X) # View the data frame df .dataframe tbody tr th { vertical-align: top; } .</description>
    </item>
    
    <item>
      <title>Effect Of Alpha On Lasso Regression</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression.html</guid>
      <description>Often we want conduct a process called regularization, wherein we penalize the number of features in a model in order to only keep the most important features. This can be particularly important when you have a dataset with 100,000+ features.
Lasso regression is a common modeling technique to do regularization. The math behind it is pretty interesting, but practically, what you need to know is that Lasso regression comes with a parameter, alpha, and the higher the alpha, the most feature coefficients are zero.</description>
    </item>
    
    <item>
      <title>Encode Days Of The Week</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/encode_days_of_the_week.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/encode_days_of_the_week.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date And Time Data # Create dates dates = pd.Series(pd.date_range(&amp;#39;2/2/2002&amp;#39;, periods=3, freq=&amp;#39;M&amp;#39;)) # View data dates 0 2002-02-28 1 2002-03-31 2 2002-04-30 dtype: datetime64[ns]  Show Days Of The Week # Show days of the week dates.dt.weekday_name 0 Thursday 1 Sunday 2 Tuesday dtype: object  </description>
    </item>
    
    <item>
      <title>Evaluating Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/evaluating_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/evaluating_clustering.html</guid>
      <description>Preliminaries import numpy as np from sklearn.metrics import silhouette_score from sklearn import datasets from sklearn.cluster import KMeans from sklearn.datasets import make_blobs Create Feature Data # Generate feature matrix X, _ = make_blobs(n_samples = 1000, n_features = 10, centers = 2, cluster_std = 0.5, shuffle = True, random_state = 1) Cluster Observations # Cluster data using k-means to predict classes model = KMeans(n_clusters=2, random_state=1).fit(X) # Get predicted classes y_hat = model.labels_ Calculate Silhouette Coefficient Formally, the $i$th observation&amp;rsquo;s silhouette coefficient is:</description>
    </item>
    
    <item>
      <title>F1 Score</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/f1_score.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/f1_score.html</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using F1 # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;f1&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Fast C Hyperparameter Tuning</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/fast_c_hyperparameter_tuning.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/fast_c_hyperparameter_tuning.html</guid>
      <description>Sometimes the characteristics of a learning algorithm allows us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods.
scikit-learn&amp;rsquo;s LogisticRegressionCV method includes a parameter Cs. If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied a integer, Cs a list of that many candidate values will is drawn from a logarithmic scale between 0.0001 and and 10000 (a range of reasonable values for C).</description>
    </item>
    
    <item>
      <title>Feature Extraction With PCA</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/feature_extraction_with_pca.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/feature_extraction_with_pca.html</guid>
      <description>Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions. Practically, PCA converts a matrix of n features into a new dataset of (hopefully) less than n features. That is, it reduces the number of features by constructing a new, smaller number variables which capture a signficant portion of the information found in the original features.</description>
    </item>
    
    <item>
      <title>Feature Importance</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/feature_importance.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/feature_importance.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets import numpy as np import matplotlib.pyplot as plt Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train A Decision Tree Model # Create decision tree classifer object clf = RandomForestClassifier(random_state=0, n_jobs=-1) # Train model model = clf.fit(X, y) View Feature Importance # Calculate feature importances importances = model.feature_importances_ Visualize Feature Importance # Sort feature importances in descending order indices = np.</description>
    </item>
    
    <item>
      <title>Feature Selection Using Random Forest</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/feature_selection_using_random_forest.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/feature_selection_using_random_forest.html</guid>
      <description>Often in data science we have hundreds or even millions of features and we want a way to create a model that only includes the most important features. This has three benefits. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called &amp;ldquo;feature selection.</description>
    </item>
    
    <item>
      <title>Find Best Preprocessing Steps During Model Selection</title>
      <link>https://ericpena.github.io/machine_learning/model_selection/find_best_preprocessing_steps_during_model_selection.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_selection/find_best_preprocessing_steps_during_model_selection.html</guid>
      <description>We have to be careful to properly handle preprocessing when conducting model selection. First, GridSearchCV uses cross-validation to determine which model has the highest performance. However, in cross-validation we are in effect pretending that the fold held out as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g. scaling or standardization). For this reason, we cannot preprocess the data then run GridSearchCV.
Second, some preprocessing methods have their own parameter which often have to be supplied by the user.</description>
    </item>
    
    <item>
      <title>Find Nearest Neighbors</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/find_nearest_neighbors.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/find_nearest_neighbors.html</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import NearestNeighbors from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Iris Data It is important to standardize our data before we calculate any distances.
# Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Find Each Observation&amp;rsquo;s Two Nearest Neighbors # Find three nearest neighbors based on euclidean distance (including itself) nn_euclidean = NearestNeighbors(n_neighbors=3, metric=&amp;#39;euclidean&amp;#39;).</description>
    </item>
    
    <item>
      <title>Find Support Vectors</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/find_support_vectors.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/find_support_vectors.html</guid>
      <description>Preliminaries # Load libraries from sklearn.svm import SVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Dataset #Load data with only two classes iris = datasets.load_iris() X = iris.data[:100,:] y = iris.target[:100] Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Train Support Vector Classifier # Create support vector classifier object svc = SVC(kernel=&amp;#39;linear&amp;#39;, random_state=0) # Train classifier model = svc.fit(X_std, y) View Support Vectors # View support vectors model.</description>
    </item>
    
    <item>
      <title>Gaussian Naive Bayes Classifier</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/gaussian_naive_bayes_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/gaussian_naive_bayes_classifier.html</guid>
      <description>Because of the assumption of the normal distribution, Gaussian Naive Bayes is best used in cases when all our features are continuous.
Preliminaries # Load libraries from sklearn import datasets from sklearn.naive_bayes import GaussianNB Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train Gaussian Naive Bayes Classifier # Create Gaussian Naive Bayes object with prior probabilities of each class clf = GaussianNB(priors=[0.25, 0.</description>
    </item>
    
    <item>
      <title>Generate Text Reports On Performance</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/generate_text_reports_on_performance.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/generate_text_reports_on_performance.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report Load Iris Flower Data # Load data iris = datasets.load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target # Create list of target class names class_names = iris.target_names Create Training And Test Sets # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) Train A Logistic Regression Model # Create logistic regression classifier = LogisticRegression() # Train model and make predictions y_hat = classifier.</description>
    </item>
    
    <item>
      <title>Group Observations Using K-Means Clustering</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/group_observations_using_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/group_observations_using_clustering.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_blobs from sklearn.cluster import KMeans import pandas as pd Create Data # Make simulated feature matrix X, _ = make_blobs(n_samples = 50, n_features = 2, centers = 3, random_state = 1) # Create DataFrame df = pd.DataFrame(X, columns=[&amp;#39;feature_1&amp;#39;,&amp;#39;feature_2&amp;#39;]) Train Clusterer # Make k-means clusterer clusterer = KMeans(3, random_state=1) # Fit clusterer clusterer.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=1, tol=0.0001, verbose=0)  Create Feature Based On Predicted Cluster # Predict values df[&amp;#39;group&amp;#39;] = clusterer.</description>
    </item>
    
    <item>
      <title>Handle Imbalanced Classes In Random Forest</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/handle_imbalanced_classes_in_random_forests.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/handle_imbalanced_classes_in_random_forests.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Adjust Iris Dataset To Make Classes Imbalanced # Make class highly imbalanced by removing first 40 observations X = X[40:,:] y = y[40:] # Create target vector indicating if class 0, otherwise 1 y = np.where((y == 0), 0, 1) Train Random Forest While Balancing Classes When using RandomForestClassifier a useful setting is class_weight=balanced wherein classes are automatically weighted inversely proportional to how frequently they appear in the data.</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes In Logistic Regression</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression.html</guid>
      <description>Like many other learning algorithms in scikit-learn, LogisticRegression comes with a built-in method of handling imbalanced classes. If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Specifically, the balanced argument will automatically weigh classes inversely proportional to their frequency:
$$w_j = \frac{n}{kn_{j}}$$</description>
    </item>
    
    <item>
      <title>Handling Missing Values In Time Series</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/handling_missing_values_in_time_series.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/handling_missing_values_in_time_series.html</guid>
      <description>Preliminaries # Load libraries import pandas as pd import numpy as np Create Date Data With Gap In Values # Create date time_index = pd.date_range(&amp;#39;01/01/2010&amp;#39;, periods=5, freq=&amp;#39;M&amp;#39;) # Create data frame, set index df = pd.DataFrame(index=time_index) # Create feature with a gap of missing values df[&amp;#39;Sales&amp;#39;] = [1.0,2.0,np.nan,np.nan,5.0] Interpolate Missing Values # Interpolate missing values df.interpolate() .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  Forward-fill Missing Values # Forward-fill df.</description>
    </item>
    
    <item>
      <title>Handling Time Zones</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/handling_time_zones.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/handling_time_zones.html</guid>
      <description>Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones[0:10] [&#39;Africa/Abidjan&#39;, &#39;Africa/Accra&#39;, &#39;Africa/Addis_Ababa&#39;, &#39;Africa/Algiers&#39;, &#39;Africa/Asmara&#39;, &#39;Africa/Asmera&#39;, &#39;Africa/Bamako&#39;, &#39;Africa/Bangui&#39;, &#39;Africa/Banjul&#39;, &#39;Africa/Bissau&#39;]  Create Timestamp With Time Zone # Create datetime pd.Timestamp(&amp;#39;2017-05-01 06:00:00&amp;#39;, tz=&amp;#39;Europe/London&amp;#39;) Timestamp(&#39;2017-05-01 06:00:00+0100&#39;, tz=&#39;Europe/London&#39;)  Create Timestamp Without Time Zone # Create datetime date = pd.Timestamp(&amp;#39;2017-05-01 06:00:00&amp;#39;) Add Time Zone # Set time zone date_in_london = date.tz_localize(&amp;#39;Europe/London&amp;#39;) Convert Time Zone # Change time zone date_in_london.</description>
    </item>
    
    <item>
      <title>Hyperparameter Tuning Using Grid Search</title>
      <link>https://ericpena.github.io/machine_learning/model_selection/hyperparameter_tuning_using_grid_search.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_selection/hyperparameter_tuning_using_grid_search.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import linear_model, datasets from sklearn.model_selection import GridSearchCV Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Logistic Regression # Create logistic regression logistic = linear_model.LogisticRegression() Create Hyperparameter Search Space # Create regularization penalty space penalty = [&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;] # Create regularization hyperparameter space C = np.logspace(0, 4, 10) # Create hyperparameter options hyperparameters = dict(C=C, penalty=penalty) Create Grid Search # Create grid search using 5-fold cross validation clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0) Conduct Grid Search # Fit grid search best_model = clf.</description>
    </item>
    
    <item>
      <title>Hyperparameter Tuning Using Random Search</title>
      <link>https://ericpena.github.io/machine_learning/model_selection/hyperparameter_tuning_using_random_search.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_selection/hyperparameter_tuning_using_random_search.html</guid>
      <description>Preliminaries # Load libraries from scipy.stats import uniform from sklearn import linear_model, datasets from sklearn.model_selection import RandomizedSearchCV Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Logistic Regression # Create logistic regression logistic = linear_model.LogisticRegression() Create Hyperparameter Search Space # Create regularization penalty space penalty = [&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;] # Create regularization hyperparameter distribution using uniform distribution C = uniform(loc=0, scale=4) # Create hyperparameter options hyperparameters = dict(C=C, penalty=penalty) Create Random Search # Create randomized search 5-fold cross validation and 100 iterations clf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1) Conduct Random Search # Fit randomized search best_model = clf.</description>
    </item>
    
    <item>
      <title>Identifying Best Value Of k</title>
      <link>https://ericpena.github.io/machine_learning/nearest_neighbors/identifying_best_value_of_k.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/nearest_neighbors/identifying_best_value_of_k.html</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import KNeighborsClassifier from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.model_selection import GridSearchCV Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Data # Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Fit A k-Nearest Neighbor Classifier # Fit a KNN classifier with 5 neighbors knn = KNeighborsClassifier(n_neighbors=5, metric=&amp;#39;euclidean&amp;#39;, n_jobs=-1).</description>
    </item>
    
    <item>
      <title>Imbalanced Classes In SVM</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/imbalanced_classes_in_svm.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/imbalanced_classes_in_svm.html</guid>
      <description>In support vector machines, $C$ is a hyperparameter determining the penalty for misclassifying an observation. One method for handling imbalanced classes in support vector machines is to weight $C$ by classes, so that
$$C_k = C * w_j$$
where $C$ is the penalty for misclassification, $w_j$ is a weight inversely proportional to class $j$&#39;s frequency, and $C_j$ is the $C$ value for class $j$. The general idea is to increase the penalty for misclassifying minority classes to prevent them from being &amp;ldquo;overwhelmed&amp;rdquo; by the majority class.</description>
    </item>
    
    <item>
      <title>k-Means Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/k-means_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/k-means_clustering.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct k-Means Clustering # Create k-mean object clt = KMeans(n_clusters=3, random_state=0, n_jobs=-1) # Train model model = clt.fit(X_std) Show Each Observation&amp;rsquo;s Cluster Membership # View predict class model.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)  Create New Observation # Create new observation new_observation = [[0.</description>
    </item>
    
    <item>
      <title>K-Nearest Neighbors Classification</title>
      <link>https://ericpena.github.io/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer.html</guid>
      <description>K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.
KNN has three basic parts:
 $y_i$: The class of an observation (what we are trying to predict in the test data). $X_i$: The predictors/IVs/attributes of an observation. $K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its &amp;ldquo;neighborhood&amp;rdquo;. For example, K=2 means that each observation&amp;rsquo;s has a neighorhood comprising of the two other observations closest to it.</description>
    </item>
    
    <item>
      <title>Lag A Time Feature</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/lag_a_time_feature.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/lag_a_time_feature.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date Data # Create data frame df = pd.DataFrame() # Create data df[&amp;#39;dates&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=5, freq=&amp;#39;D&amp;#39;) df[&amp;#39;stock_price&amp;#39;] = [1.1,2.2,3.3,4.4,5.5] Lag Time Data By One Row # Lagged values by one row df[&amp;#39;previous_days_stock_price&amp;#39;] = df[&amp;#39;stock_price&amp;#39;].shift(1) # Show data frame df .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  </description>
    </item>
    
    <item>
      <title>Lasso Regression</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/lasso_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/lasso_regression.html</guid>
      <description>Preliminaries # Load library from sklearn.linear_model import Lasso from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Fit Ridge Regression The hyperparameter, $\alpha$, lets us control how much we penalize the coefficients, with higher values of $\alpha$ creating simpler modelers. The ideal value of $\alpha$ should be tuned like any other hyperparameter.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/linear_regression_scikitlearn.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/linear_regression_scikitlearn.html</guid>
      <description>Sources: scikit-learn, DrawMyData.
The purpose of this tutorial is to give a brief introduction into the logic of statistical model building used in machine learning. If you want to read more about the theory behind this tutorial, check out An Introduction To Statistical Learning.
Let us get started.
Preliminary import pandas as pd from sklearn import linear_model import random import numpy as np %matplotlib inline Load Data With those libraries added, let us load the dataset (the dataset is avaliable in his site&amp;rsquo;s GitHub repo).</description>
    </item>
    
    <item>
      <title>Linear Regression Using Scikit-Learn</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/linear_regression_using_scikit-learn.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/linear_regression_using_scikit-learn.html</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston import warnings # Suppress Warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Fit A Linear Regression # Create linear regression regr = LinearRegression() # Fit the linear regression model = regr.fit(X, y) View Intercept Term # View the intercept model.intercept_ 36.491103280361038  View Coefficients # View the feature coefficients model.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression.html</guid>
      <description>Despite having &amp;ldquo;regression&amp;rdquo; in its name, a logistic regression is actually a widely used binary classifier (i.e. the target vector can only take two values). In a logistic regression, a linear model (e.g. $\beta_{0}+\beta_{1}x$) is included in a logistic (also called sigmoid) function, ${\frac{1}{1+e^{-z}}}$, such that:
$$P(y_i=1 \mid X)={\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x)}}}$$
where $P(y_i=1 \mid X)$ is the probability of the $i$th observation&amp;rsquo;s target value, $y_i$, being class 1, $X$ is the training data, $\beta_0$ and $\beta_1$ are the parameters to be learned, and $e$ is Euler&amp;rsquo;s number.</description>
    </item>
    
    <item>
      <title>Logistic Regression On Very Large Data</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression_on_very_large_data.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression_on_very_large_data.html</guid>
      <description>scikit-learn&amp;rsquo;s LogisticRegression offers a number of techniques for training a logistic regression, called solvers. Most of the time scikit-learn will select the best solver automatically for us or warn us that you cannot do some thing with that solver. However, there is one particular case we should be aware of.
While an exact explanation is beyond the bounds of this book, stochastic average gradient descent allows us to train a model much faster than other solvers when our data is very large.</description>
    </item>
    
    <item>
      <title>Logistic Regression With L1 Regularization</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression_with_l1_regularization.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/logistic_regression_with_l1_regularization.html</guid>
      <description>L1 regularization (also called least absolute deviations) is a powerful tool in data science. There are many tutorials out there explaining L1 regularization and I will not try to do that here. Instead, this tutorial is show the effect of the regularization parameter C on the coefficients and model accuracy.
Preliminaries import numpy as np from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Create The Data The dataset used in this tutorial is the famous iris dataset.</description>
    </item>
    
    <item>
      <title>Meanshift Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/meanshift_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/meanshift_clustering.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import MeanShift Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct Meanshift Clustering MeanShift has two important parameters we should be aware of. First, bandwidth sets radius of the area (i.e. kernel) an observation uses to determine the direction to shift.</description>
    </item>
    
    <item>
      <title>Mini-Batch k-Means Clustering</title>
      <link>https://ericpena.github.io/machine_learning/clustering/minibatch_k-means_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/clustering/minibatch_k-means_clustering.html</guid>
      <description>Mini-batch k-means works similarly to the k-means algorithm discussed in the last recipe. Without going into too much detail, the difference is that in mini-batch k-means the most computationally costly step is conducted on only a random sample of observations as opposed to all observations. This approach can significantly reduce the time required for the algorithm to find convergence (i.e. fit the data) with only a small cost in quality.</description>
    </item>
    
    <item>
      <title>Model Selection Using Grid Search</title>
      <link>https://ericpena.github.io/machine_learning/model_selection/model_selection_using_grid_search.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_selection/model_selection_using_grid_search.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline # Set random seed np.random.seed(0) Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Pipeline With Model Selection Search Space Notice that we include both multiple possible learning algorithms and multiple possible hyperparameter values to search over.</description>
    </item>
    
    <item>
      <title>Multinomial Logistic Regression</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/multinomial_logistic_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/multinomial_logistic_regression.html</guid>
      <description>In multinomial logistic regression (MLR) the logistic function we saw in Recipe 15.1 is replaced with a softmax function:
$$P(y_i=k \mid X)={\frac {e^{\beta_{k}x_{i}}}{{\sum_{j=1}^{K}}e^{\beta_{j}x_{i}}}}$$
where $P(y_i=k \mid X)$ is the probability the $i$th observation&amp;rsquo;s target value, $y_i$, is class $k$, and $K$ is the total number of classes. One practical advantage of the MLR is that its predicted probabilities using the predict_proba method are more reliable (i.e. better calibrated).
Preliminaries # Load libraries from sklearn.</description>
    </item>
    
    <item>
      <title>Multinomial Naive Bayes Classifier</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/multinomial_naive_bayes_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/multinomial_naive_bayes_classifier.html</guid>
      <description>Multinomial naive Bayes works similar to Gaussian naive Bayes, however the features are assumed to be multinomially distributed. In practice, this means that this classifier is commonly used when we have discrete data (e.g. movie ratings ranging 1 and 5).
Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Brazil is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Bag Of Words # Create bag of words count = CountVectorizer() bag_of_words = count.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier From Scratch</title>
      <link>https://ericpena.github.io/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch.html</guid>
      <description>Naive bayes is simple classifier known for doing well when only a small number of observations is available. In this tutorial we will create a gaussian naive bayes classifier from scratch and use it to predict the class of a previously unseen data point. This tutorial is based on an example on Wikipedia&amp;rsquo;s naive bayes classifier page, I have implemented it in Python and tweaked some notation to improve explanation.</description>
    </item>
    
    <item>
      <title>Nested Cross Validation</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/nested_cross_validation.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/nested_cross_validation.html</guid>
      <description>Often we want to tune the parameters of a model (for example, C in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation:
 Set the parameter you want to tune to some value. Split your data into K &amp;lsquo;folds&amp;rsquo; (sections). Train your model using K-1 folds using the parameter value.</description>
    </item>
    
    <item>
      <title>One Vs. Rest Logistic Regression</title>
      <link>https://ericpena.github.io/machine_learning/logistic_regression/one-vs-rest_logistic_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/logistic_regression/one-vs-rest_logistic_regression.html</guid>
      <description>On their own, logistic regressions are only binary classifiers, meaning they cannot handle target vectors with more than two classes. However, there are clever extensions to logistic regression to do just that. In one-vs-rest logistic regression (OVR) a separate model is trained for each class predicted whether an observation is that class or not (thus making it a binary classification problem). It assumes that each classification problem (e.g. class 0 or not) is independent.</description>
    </item>
    
    <item>
      <title>Pipelines With Parameter Optimization</title>
      <link>https://ericpena.github.io/machine_learning/model_selection/pipelines_with_parameter_optimization.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_selection/pipelines_with_parameter_optimization.html</guid>
      <description>Preliminaries # Import required packages import numpy as np from sklearn import linear_model, decomposition, datasets from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.preprocessing import StandardScaler Load Data # Load the breast cancer data dataset = datasets.load_breast_cancer() # Create X from the dataset&amp;#39;s features X = dataset.data # Create y from the dataset&amp;#39;s output y = dataset.target Create Pipelines # Create an scaler object sc = StandardScaler() # Create a pca object pca = decomposition.</description>
    </item>
    
    <item>
      <title>Plot The Learning Curve</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_learning_curve.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_learning_curve.html</guid>
      <description>Preliminaries # Load libraries import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve Load Digits Dataset # Load data digits = load_digits() # Create feature matrix and target vector X, y = digits.data, digits.target Plot Learning Curve # Create CV training and test scores for various training set sizes train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), X, y, # Number of folds in cross-validation cv=10, # Evaluation metric scoring=&amp;#39;accuracy&amp;#39;, # Use all computer cores n_jobs=-1, # 50 different sizes of the training set train_sizes=np.</description>
    </item>
    
    <item>
      <title>Plot The Receiving Operating Characteristic Curve</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_receiving_operating_characteristic_curve.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_receiving_operating_characteristic_curve.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, roc_auc_score from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt Generate Features And Target # Create feature matrix and target vector X, y = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=3, random_state=3) Split Data Intro Training And Test Sets # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1) Training Binary Classifier # Create classifier clf = LogisticRegression() # Train model clf.</description>
    </item>
    
    <item>
      <title>Plot The Support Vector Classifiers Hyperplane</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/plot_support_vector_classifier_hyperplane.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/plot_support_vector_classifier_hyperplane.html</guid>
      <description>Preliminaries # Load libraries from sklearn.svm import LinearSVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np from matplotlib import pyplot as plt Load Iris Flower Data # Load data with only two classes and two features iris = datasets.load_iris() X = iris.data[:100,:2] y = iris.target[:100] Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Train Support Vector Classifier # Create support vector classifier svc = LinearSVC(C=1.</description>
    </item>
    
    <item>
      <title>Plot The Validation Curve</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_validation_curve.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/plot_the_validation_curve.html</guid>
      <description>Preliminaries # Load libraries import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import validation_curve Load Digits Dataset # Load data digits = load_digits() # Create feature matrix and target vector X, y = digits.data, digits.target Plot Validation Curve # Create range of values for parameter param_range = np.arange(1, 250, 2) # Calculate accuracy on training and test set using range of parameter values train_scores, test_scores = validation_curve(RandomForestClassifier(), X, y, param_name=&amp;#34;n_estimators&amp;#34;, param_range=param_range, cv=3, scoring=&amp;#34;accuracy&amp;#34;, n_jobs=-1) # Calculate mean and standard deviation for training set scores train_mean = np.</description>
    </item>
    
    <item>
      <title>Precision</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/precision.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/precision.html</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Precision # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;precision&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Radius-Based Nearest Neighbor Classifier</title>
      <link>https://ericpena.github.io/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier.html</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import RadiusNeighborsClassifier from sklearn.preprocessing import StandardScaler from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Features # Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Fit A Radius-Based Nearest Neighbor Classifier In scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius.</description>
    </item>
    
    <item>
      <title>Random Forest Classifier</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_classifier.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets Load Iris Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Random Forest Classifier # Create random forest classifer object that uses entropy clf = RandomForestClassifier(criterion=&amp;#39;entropy&amp;#39;, random_state=0, n_jobs=-1) Train Random Forest Classifier # Train model model = clf.fit(X, y) Predict Previously Unseen Observation # Make new observation observation = [[ 5, 4, 3, 2]] # Predict observation&amp;#39;s class  model.</description>
    </item>
    
    <item>
      <title>Random Forest Classifier Example</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_classifier_example.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_classifier_example.html</guid>
      <description>This tutorial is based on Yhat&amp;rsquo;s 2013 tutorial on Random Forests in Python. If you want a good summary of the theory and uses of random forests, I suggest you check out their guide. In the tutorial below, I annotate, correct, and expand on a short code example of random forests they present at the end of the article. Specifically, I 1) update the code so it runs in the latest version of pandas and Python, 2) write detailed comments explaining what is happening in each step, and 3) expand the code in a number of ways.</description>
    </item>
    
    <item>
      <title>Random Forest Regression</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_regressor.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/random_forest_regressor.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestRegressor from sklearn import datasets Load Boston Housing Data # Load data with only two features boston = datasets.load_boston() X = boston.data[:,0:2] y = boston.target Create Random Forest Regressor # Create decision tree classifer object regr = RandomForestRegressor(random_state=0, n_jobs=-1) Train Random Forest Regressor # Train model model = regr.fit(X, y) </description>
    </item>
    
    <item>
      <title>Recall</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/recall.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/recall.html</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Recall # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;recall&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Recursive Feature Elimination</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/recursive_feature_elimination.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/recursive_feature_elimination.html</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_regression from sklearn.feature_selection import RFECV from sklearn import datasets, linear_model import warnings # Suppress an annoying but harmless warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Create Data # Generate features matrix, target vector, and the true coefficients X, y = make_regression(n_samples = 10000, n_features = 100, n_informative = 2, random_state = 1) Create Linear Model # Create a linear regression ols = linear_model.LinearRegression() Conduct Recursive Feature Elimination # Create recursive feature eliminator that scores features by mean squared errors rfecv = RFECV(estimator=ols, step=1, scoring=&amp;#39;neg_mean_squared_error&amp;#39;) # Fit recursive feature eliminator  rfecv.</description>
    </item>
    
    <item>
      <title>Ridge Regression</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/ridge_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/ridge_regression.html</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import Ridge from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Fit Ridge Regression The hyperparameter, $\alpha$, lets us control how much we penalize the coefficients, with higher values of $\alpha$ creating simpler modelers. The ideal value of $\alpha$ should be tuned like any other hyperparameter.</description>
    </item>
    
    <item>
      <title>Rolling Time Window</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/rolling_time_windows.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/rolling_time_windows.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date Data # Create datetimes time_index = pd.date_range(&amp;#39;01/01/2010&amp;#39;, periods=5, freq=&amp;#39;M&amp;#39;) # Create data frame, set index df = pd.DataFrame(index=time_index) # Create feature df[&amp;#39;Stock_Price&amp;#39;] = [1,2,3,4,5] Create A Rolling Time Window Of Two Rows # Calculate rolling mean df.rolling(window=2).mean() .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  # Identify max value in rolling time window df.</description>
    </item>
    
    <item>
      <title>Select Date And Time Ranges</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/select_date_and_time_ranges.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_dates_and_times/select_date_and_time_ranges.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create pandas Series Time Data # Create data frame df = pd.DataFrame() # Create datetimes df[&amp;#39;date&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=100000, freq=&amp;#39;H&amp;#39;) Select Time Range (Method 1) Use this method if your data frame is not indexed by time.
# Select observations between two datetimes df[(df[&amp;#39;date&amp;#39;] &amp;gt; &amp;#39;2002-1-1 01:00:00&amp;#39;) &amp;amp; (df[&amp;#39;date&amp;#39;] &amp;lt;= &amp;#39;2002-1-1 04:00:00&amp;#39;)] .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  Select Time Range (Method 2) Use this method if your data frame is indexed by time.</description>
    </item>
    
    <item>
      <title>Select Important Features In Random Forest</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/select_important_features_in_random_forest.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/select_important_features_in_random_forest.html</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets from sklearn.feature_selection import SelectFromModel Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Random Forest Classifier # Create random forest classifier clf = RandomForestClassifier(random_state=0, n_jobs=-1) Select Features With Importance Greater Than Threshold The higher the number, the more important the feature (all importance scores sum to one). By plotting these values we can add interpretability to our random forest models.</description>
    </item>
    
    <item>
      <title>Selecting The Best Alpha Value In Ridge Regression</title>
      <link>https://ericpena.github.io/machine_learning/linear_regression/selecting_best_alpha_value_in_ridge_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/linear_regression/selecting_best_alpha_value_in_ridge_regression.html</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import RidgeCV from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features Note: Because in linear regression the value of the coefficients is partially determined by the scale of the feature, and in regularized models all coefficients are summed together, we must make sure to standardize the feature prior to training.</description>
    </item>
    
    <item>
      <title>Selecting The Best Number Of Components For LDA</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/select_best_number_of_components_in_lda.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/select_best_number_of_components_in_lda.html</guid>
      <description>In scikit-learn, LDA is implemented using LinearDiscriminantAnalysis includes a parameter, n_components indicating the number of features we want returned. To figure out what argument value to use with n_components (e.g. how many parameters to keep), we can take advantage of the fact that explained_variance_ratio_ tells us the variance explained by each outputted feature and is a sorted array.
Specifically, we can run LinearDiscriminantAnalysis with n_components set to None to return ratio of variance explained by every component feature, then calculate how many components are required to get above some threshold of variance explained (often 0.</description>
    </item>
    
    <item>
      <title>Selecting The Best Number Of Components For TSVD</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets.load_digits() # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) # Make sparse matrix X_sparse = csr_matrix(X) Run Truncated Singular Value Decomposition # Create and run an TSVD with one less than number of features tsvd = TruncatedSVD(n_components=X_sparse.</description>
    </item>
    
    <item>
      <title>Split Data Into Training And Test Sets</title>
      <link>https://ericpena.github.io/machine_learning/model_evaluation/split_data_into_training_and_test_sets.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/model_evaluation/split_data_into_training_and_test_sets.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split Load Digits Dataset # Load the digits dataset digits = datasets.load_digits() # Create the features matrix X = digits.data # Create the target vector y = digits.target Split Into Training And Test Sets # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1) Fit Standardizer To Training Set # Create standardizer standardizer = StandardScaler() # Fit standardizer to training set standardizer.</description>
    </item>
    
    <item>
      <title>Support Vector Classifier</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/support_vector_classifier.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/support_vector_classifier.html</guid>
      <description>There is a balance between SVC maximizing the margin of the hyperplane and minimizing the misclassification. In SVC, the later is controlled with the hyperparameter $C$, the penalty imposed on errors. C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias but low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias but high variance).</description>
    </item>
    
    <item>
      <title>SVC Parameters When Using RBF Kernel</title>
      <link>https://ericpena.github.io/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel.html</guid>
      <description>In this tutorial we will visually explore the effects of the two parameters from the support vector classifier (SVC) when using the radial basis function kernel (RBF). This tutorial draws heavily on the code used in Sebastian Raschka&amp;rsquo;s book Python Machine Learning.
Preliminaries # Import packages to visualize the classifer from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt import warnings # Import packages to do the classifying import numpy as np from sklearn.</description>
    </item>
    
    <item>
      <title>Titanic Competition With Random Forest</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/titanic_competition_with_random_forest.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/titanic_competition_with_random_forest.html</guid>
      <description>Preliminaries import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV, cross_val_score import csv as csv Get The Data You can get the data on Kaggle&amp;rsquo;s site.
# Load the data train = pd.read_csv(&amp;#39;data/train.csv&amp;#39;) test = pd.read_csv(&amp;#39;data/test.csv&amp;#39;) Data Cleaning # Create a list of the features we will eventually want for our model features = [&amp;#39;Age&amp;#39;, &amp;#39;SibSp&amp;#39;,&amp;#39;Parch&amp;#39;,&amp;#39;Fare&amp;#39;,&amp;#39;male&amp;#39;,&amp;#39;embarked_Q&amp;#39;,&amp;#39;embarked_S&amp;#39;,&amp;#39;Pclass_2&amp;#39;, &amp;#39;Pclass_3&amp;#39;] Sex Here we convert the gender labels (male, female) into a dummy variable (1, 0).</description>
    </item>
    
    <item>
      <title>Using Linear Discriminant Analysis For Dimensionality Reduction</title>
      <link>https://ericpena.github.io/machine_learning/feature_engineering/lda_for_dimensionality_reduction.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_engineering/lda_for_dimensionality_reduction.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis Load Iris Data # Load the Iris flower dataset: iris = datasets.load_iris() X = iris.data y = iris.target Create A Linear # Create an LDA that will reduce the data down to 1 feature lda = LinearDiscriminantAnalysis(n_components=1) # run an LDA and use it to transform the features X_lda = lda.fit(X, y).transform(X) View Results # Print the number of features print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Variance Thresholding Binary Features</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/variance_thresholding_binary_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/variance_thresholding_binary_features.html</guid>
      <description>Preliminaries from sklearn.feature_selection import VarianceThreshold Load Data # Create feature matrix with:  # Feature 0: 80% class 0 # Feature 1: 80% class 1 # Feature 2: 60% class 0, 40% class 1 X = [[0, 1, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0]] Conduct Variance Thresholding In binary features (i.e. Bernoulli random variables), variance is calculated as:
$$\operatorname {Var} (x)= p(1-p)$$
where $p$ is the proportion of observations of class 1.</description>
    </item>
    
    <item>
      <title>Variance Thresholding For Feature Selection</title>
      <link>https://ericpena.github.io/machine_learning/feature_selection/variance_thresholding_for_feature_selection.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/feature_selection/variance_thresholding_for_feature_selection.html</guid>
      <description>Preliminaries from sklearn import datasets from sklearn.feature_selection import VarianceThreshold Load Data # Load iris data iris = datasets.load_iris() # Create features and target X = iris.data y = iris.target Conduct Variance Thresholding # Create VarianceThreshold object with a variance with a threshold of 0.5 thresholder = VarianceThreshold(threshold=.5) # Conduct variance thresholding X_high_variance = thresholder.fit_transform(X) View high variance features # View first five rows with features with variances above threshold X_high_variance[0:5] array([[ 5.</description>
    </item>
    
    <item>
      <title>Visualize A Decision Tree</title>
      <link>https://ericpena.github.io/machine_learning/trees_and_forests/visualize_a_decision_tree.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/trees_and_forests/visualize_a_decision_tree.html</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets from IPython.display import Image from sklearn import tree import pydotplus Load Iris Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train Decision Tree # Create decision tree classifer object clf = DecisionTreeClassifier(random_state=0) # Train model model = clf.fit(X, y) Visualize Decision Tree # Create DOT data dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names) # Draw graph graph = pydotplus.</description>
    </item>
    
  </channel>
</rss>