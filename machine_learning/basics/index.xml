<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/basics/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Jul 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/basics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>PCA Single Value Decomposition</title>
      <link>https://ericpena.github.io/machine_learning/basics/pca-svd.html</link>
      <pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/pca-svd.html</guid>
      <description>In another article, Principal Component Analysis was performed using Eigenvalue Decomposition. In this article we take a different approach: Single Value Decomposition. The general idea is that for any matrix we may perform Single Value Decomposition - this is guaranteed. It is a powerful tool that is used in many fields. On the other hand, the Eigenvalue Decomposition does not always exist. Eigenvalue Decomposition can only be done on square, full-rank, positive semi-definite matricies.</description>
    </item>
    
    <item>
      <title>PCA Eigenvalue Decomposition</title>
      <link>https://ericpena.github.io/machine_learning/basics/pca-evd.html</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/pca-evd.html</guid>
      <description>In this article, I go into how we can perform Principal Component Analysis (PCA) using the method of Eigenvalue Decomposition (EVD). Generally, PCA is done by peforming a change of basis on the data, typically by utilizing eigenvectors that find the principal directions of the data. Another important thing to know is that the Eigenvalue Decomposition does not always exist. Eigenvalue Decomposition can only be done on square, full-rank, positive semi-definite matricies.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://ericpena.github.io/machine_learning/basics/linear-regression.html</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/linear-regression.html</guid>
      <description>Mathematical Foundations The linear model is often the first model we learn fitting data. Given a vector of inputs $X^T = (X_1, X_2, \ldots, X_p)$, we can predict the output $Y$ with the following model:
$$\hat Y = \hat \beta_0 + \sum_{j=1}^p X_j \hat \beta_j$$
Many times, it&amp;rsquo;s convenient to include the vector $\textbf{1}$ in $\textbf{X}$ and include the $\hat \beta_0$ in the vector $\hat \beta$ so we may represent this linear model in vector form as an inner product:</description>
    </item>
    
  </channel>
</rss>