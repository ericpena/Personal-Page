<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/basics/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 11:53:49 -0700</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/basics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Loading Features From Dictionaries</title>
      <link>https://ericpena.github.io/machine_learning/basics/loading_features_from_dictionaries.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/loading_features_from_dictionaries.html</guid>
      <description>Preliminaries from sklearn.feature_extraction import DictVectorizer Create A Dictionary staff = [{&amp;#39;name&amp;#39;: &amp;#39;Steve Miller&amp;#39;, &amp;#39;age&amp;#39;: 33.}, {&amp;#39;name&amp;#39;: &amp;#39;Lyndon Jones&amp;#39;, &amp;#39;age&amp;#39;: 12.}, {&amp;#39;name&amp;#39;: &amp;#39;Baxter Morth&amp;#39;, &amp;#39;age&amp;#39;: 18.}] Convert Dictionary To Feature Matrix # Create an object for our dictionary vectorizer vec = DictVectorizer() # Fit then transform the staff dictionary with vec, then output an array vec.fit_transform(staff).toarray() array([[ 33., 0., 0., 1.], [ 12., 0., 1., 0.], [ 18., 1., 0., 0.]])  View Feature Names # Get Feature Names vec.</description>
    </item>
    
    <item>
      <title>Loading Features From Dictionaries</title>
      <link>https://ericpena.github.io/machine_learning/basics/untitled.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/untitled.html</guid>
      <description>from sklearn import datasets import numpy as np iris = datasets.load_iris() X = iris.data[:, [2, 3]] X array([[1.4, 0.2], [1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [1.4, 0.2], [1.7, 0.4], [1.4, 0.3], [1.5, 0.2], [1.4, 0.2], [1.5, 0.1], [1.5, 0.2], [1.6, 0.2], [1.4, 0.1], [1.1, 0.1], [1.2, 0.2], [1.5, 0.4], [1.3, 0.4], [1.4, 0.3], [1.7, 0.3], [1.5, 0.3], [1.7, 0.2], [1.5, 0.4], [1. , 0.2], [1.7, 0.5], [1.9, 0.2], [1.6, 0.2], [1.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Boston Housing Dataset</title>
      <link>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_boston_housing_dataset.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_boston_housing_dataset.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Boston Housing Dataset The Boston housing dataset is a famous dataset from the 1970s. It contains 506 observations on housing prices around Boston. It is often used in regression examples and contains 15 features.
# Load digits dataset boston = datasets.load_boston() # Create feature matrix X = boston.data # Create target vector y = boston.target # View the first observation&amp;#39;s feature values X[0] array([ 6.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Digits Dataset</title>
      <link>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_digits-dataset.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_digits-dataset.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Digits Dataset Digits is a dataset of handwritten digits. Each feature is the intensity of one pixel of an 8 x 8 image.
# Load digits dataset digits = datasets.load_digits() # Create feature matrix X = digits.data # Create target vector y = digits.target # View the first observation&amp;#39;s feature values X[0] array([ 0., 0., 5., 13., 9.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Iris Dataset</title>
      <link>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_iris_dataset.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/loading_scikit-learns_iris_dataset.html</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Iris Dataset The Iris flower dataset is one of the most famous databases for classification. It contains three classes (i.e. three species of flowers) with 50 observations per class.
# Load digits dataset iris = datasets.load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target # View the first observation&amp;#39;s feature values X[0] array([ 5.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Classification</title>
      <link>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_classification.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_classification.html</guid>
      <description>Preliminaries from sklearn.datasets import make_classification import pandas as pd Create Simulated Data # Create a simulated feature matrix and output vector with 100 samples, features, output = make_classification(n_samples = 100, # ten features n_features = 10, # five features that actually predict the output&amp;#39;s classes n_informative = 5, # five features that are random and unrelated to the output&amp;#39;s classes n_redundant = 5, # three output classes n_classes = 3, # with 20% of observations in the first class, 30% in the second class, # and 50% in the third class.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Clustering</title>
      <link>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_clustering.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_clustering.html</guid>
      <description>Preliminaries from sklearn.datasets import make_blobs import matplotlib.pyplot as plt Make Data # Make the features (X) and output (y) with 200 samples, X, y = make_blobs(n_samples = 200, # two feature variables, n_features = 2, # three clusters, centers = 3, # with .5 cluster standard deviation, cluster_std = 0.5, # shuffled, shuffle = True) View Data # Create a scatterplot of the first and second features plt.scatter(X[:,0], X[:,1]) # Show the scatterplot plt.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Regression</title>
      <link>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_regression.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/make_simulated_data_for_regression.html</guid>
      <description>Preliminaries import pandas as pd from sklearn.datasets import make_regression Create Simulated Data # Generate fetures, outputs, and true coefficient of 100 samples, features, output, coef = make_regression(n_samples = 100, # three features n_features = 3, # where only two features are useful, n_informative = 2, # a single target value per observation n_targets = 1, # 0.0 standard deviation of the guassian noise noise = 0.0, # show the true coefficient used to generated the data coef = True) View Simulated Data # View the features of the first five rows pd.</description>
    </item>
    
    <item>
      <title>Perceptron In Scikit</title>
      <link>https://ericpena.github.io/machine_learning/basics/perceptron_in_scikit-learn.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/perceptron_in_scikit-learn.html</guid>
      <description>A perceptron learner was one of the earliest machine learning techniques and still from the foundation of many modern neural networks. In this tutorial we use a perceptron learner to classify the famous iris dataset. This tutorial was inspired by Python Machine Learning by Sebastian Raschka.
Preliminaries # Load required libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Perceptron from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np Load The Iris Data # Load the iris dataset iris = datasets.</description>
    </item>
    
    <item>
      <title>Saving Machine Learning Models</title>
      <link>https://ericpena.github.io/machine_learning/basics/saving_machine_learning_models.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/saving_machine_learning_models.html</guid>
      <description>In scikit there are two main ways to save a model for future use: a pickle string and a pickled model as a file.
Preliminaries from sklearn.linear_model import LogisticRegression from sklearn import datasets import pickle from sklearn.externals import joblib Load Data # Load the iris data iris = datasets.load_iris() # Create a matrix, X, of features and a vector, y. X, y = iris.data, iris.target Train Model # Train a naive logistic regression model clf = LogisticRegression(random_state=0) clf.</description>
    </item>
    
  </channel>
</rss>