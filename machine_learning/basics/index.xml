<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/basics/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 May 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/basics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Least Squares Regression</title>
      <link>https://ericpena.github.io/machine_learning/basics/least-squares.html</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/basics/least-squares.html</guid>
      <description>Mathematical Foundations The linear model is often the first model we learn modeling modeling data. Given a vector of inputs $X^T = (X_1, X_2, \ldots, X_p)$, we can predict the output Y with the following model:
$$\hat Y = \hat \beta_0 + \sum_{j=1}^p X_j \hat \beta_j$$
According to this notation, $X^T$ is a row vector and $X$ is a column vector. What we want to do is choose the coefficients $\beta$ to minimize what is called the residual sum of squares:</description>
    </item>
    
  </channel>
</rss>