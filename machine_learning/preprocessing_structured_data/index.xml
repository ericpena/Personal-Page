<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 11:53:49 -0700</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/preprocessing_structured_data/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Convert Pandas Categorical Data For Scikit-Learn</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/convert_pandas_categorical_column_into_integers_for_scikit-learn.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/convert_pandas_categorical_column_into_integers_for_scikit-learn.html</guid>
      <description>Preliminaries # Import required packages from sklearn import preprocessing import pandas as pd Create DataFrame raw_data = {&amp;#39;patient&amp;#39;: [1, 1, 1, 2, 2], &amp;#39;obs&amp;#39;: [1, 2, 3, 1, 2], &amp;#39;treatment&amp;#39;: [0, 1, 0, 1, 0], &amp;#39;score&amp;#39;: [&amp;#39;strong&amp;#39;, &amp;#39;weak&amp;#39;, &amp;#39;normal&amp;#39;, &amp;#39;weak&amp;#39;, &amp;#39;strong&amp;#39;]} df = pd.DataFrame(raw_data, columns = [&amp;#39;patient&amp;#39;, &amp;#39;obs&amp;#39;, &amp;#39;treatment&amp;#39;, &amp;#39;score&amp;#39;]) Fit The Label Encoder # Create a label (category) encoder object le = preprocessing.LabelEncoder() # Fit the encoder to the pandas column le.</description>
    </item>
    
    <item>
      <title>Delete Observations With Missing Values</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/delete_observations_with_missing_values.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/delete_observations_with_missing_values.html</guid>
      <description>Preliminaries # Load libraries import numpy as np import pandas as pd Create Feature Matrix # Create feature matrix X = np.array([[1.1, 11.1], [2.2, 22.2], [3.3, 33.3], [4.4, 44.4], [np.nan, 55]]) Delete Observations With Missing Values # Remove observations with missing values X[~np.isnan(X).any(axis=1)] array([[ 1.1, 11.1], [ 2.2, 22.2], [ 3.3, 33.3], [ 4.4, 44.4]])  </description>
    </item>
    
    <item>
      <title>Deleting Missing Values</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/deleting_missing_values.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/deleting_missing_values.html</guid>
      <description>Preliminaries # Load library import numpy as np import pandas as pd Create Data Frame # Create feature matrix X = np.array([[1, 2], [6, 3], [8, 4], [9, 5], [np.nan, 4]]) Drop Missing Values Using NumPy # Remove observations with missing values X[~np.isnan(X).any(axis=1)] array([[ 1., 2.], [ 6., 3.], [ 8., 4.], [ 9., 5.]])  Drop Missing Values Using pandas # Load data as a data frame df = pd.</description>
    </item>
    
    <item>
      <title>Detecting Outliers</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/detecting_outliers.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/detecting_outliers.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.datasets import make_blobs Create Data # Create simulated data X, _ = make_blobs(n_samples = 10, n_features = 2, centers = 1, random_state = 1) # Replace the first observation&amp;#39;s values with extreme values X[0,0] = 10000 X[0,1] = 10000 Detect Outliers EllipticEnvelope assumes the data is normally distributed and based on that assumption &amp;ldquo;draws&amp;rdquo; an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1).</description>
    </item>
    
    <item>
      <title>Discretize Features</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/discretize_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/discretize_features.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import Binarizer import numpy as np Create Data # Create feature age = np.array([[6], [12], [20], [36], [65]]) Option 1: Binarize Feature # Create binarizer binarizer = Binarizer(18) # Transform feature binarizer.fit_transform(age) array([[0], [0], [1], [1], [1]])  Option 2: Break Up Feature Into Bins # Bin feature np.digitize(age, bins=[20,30,64]) array([[0], [0], [1], [2], [3]])  </description>
    </item>
    
    <item>
      <title>Encoding Ordinal Categorical Features</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/encoding_ordinal_categorical_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/encoding_ordinal_categorical_features.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Feature Matrix # Create features df = pd.DataFrame({&amp;#39;Score&amp;#39;: [&amp;#39;Low&amp;#39;, &amp;#39;Low&amp;#39;, &amp;#39;Medium&amp;#39;, &amp;#39;Medium&amp;#39;, &amp;#39;High&amp;#39;]}) # View data frame df .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  Create Scale Map # Create mapper scale_mapper = {&amp;#39;Low&amp;#39;:1, &amp;#39;Medium&amp;#39;:2, &amp;#39;High&amp;#39;:3} Map Scale To Features # Map feature values to scale df[&amp;#39;Scale&amp;#39;] = df[&amp;#39;Score&amp;#39;].replace(scale_mapper) # View data frame df .</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes With Downsampling</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling.html</guid>
      <description>In downsampling, we randomly sample without replacement from the majority class (i.e. the class with more observations) to create a new subset of observation equal in size to the minority class.
Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target Make Iris Dataset Imbalanced # Remove first 40 observations X = X[40:,:] y = y[40:] # Create binary target vector indicating if class 0 y = np.</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes With Upsampling</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_upsampling.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_upsampling.html</guid>
      <description>In upsampling, for every observation in the majority class, we randomly select an observation from the minority class with replacement. The end result is the same number of observations from the minority and majority classes.
Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.</description>
    </item>
    
    <item>
      <title>Handling Outliers</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_outliers.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/handling_outliers.html</guid>
      <description>Preliminaries # Load library import pandas as pd Create Data # Create DataFrame houses = pd.DataFrame() houses[&amp;#39;Price&amp;#39;] = [534433, 392333, 293222, 4322032] houses[&amp;#39;Bathrooms&amp;#39;] = [2, 3.5, 2, 116] houses[&amp;#39;Square_Feet&amp;#39;] = [1500, 2500, 1500, 48000] houses .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }  Option 1: Drop # Drop observations greater than some value houses[houses[&amp;#39;Bathrooms&amp;#39;] &amp;lt; 20] .dataframe thead th { text-align: left; } .</description>
    </item>
    
    <item>
      <title>Impute Missing Values With Means</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/impute_missing_values_with_means.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/impute_missing_values_with_means.html</guid>
      <description>Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most &amp;lsquo;naive&amp;rsquo; imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it.
Preliminaries import pandas as pd import numpy as np from sklearn.preprocessing import Imputer Create Data # Create an empty dataset df = pd.</description>
    </item>
    
    <item>
      <title>Imputing Missing Class Labels</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/imputing_missing_class_labels.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/imputing_missing_class_labels.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.preprocessing import Imputer Create Feature Matrix With Missing Values # Create feature matrix with categorical feature X = np.array([[0, 2.10, 1.45], [1, 1.18, 1.33], [0, 1.22, 1.27], [0, -0.21, -1.19], [np.nan, 0.87, 1.31], [np.nan, -0.67, -0.22]]) Fill Missing Values&amp;rsquo; Class With Most Frequent Class # Create Imputer object imputer = Imputer(strategy=&amp;#39;most_frequent&amp;#39;, axis=0) # Fill missing values with most frequent class imputer.fit_transform(X) array([[ 0.</description>
    </item>
    
    <item>
      <title>Imputing Missing Class Labels Using k-Nearest Neighbors</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/imputing_missing_class_labels_using_k-nearest_neighbors.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/imputing_missing_class_labels_using_k-nearest_neighbors.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.neighbors import KNeighborsClassifier Create Feature Matrix # Create feature matrix with categorical feature X = np.array([[0, 2.10, 1.45], [1, 1.18, 1.33], [0, 1.22, 1.27], [1, -0.21, -1.19]]) Create Feature Matrix With Missing Values # Create feature matrix with missing values in the categorical feature X_with_nan = np.array([[np.nan, 0.87, 1.31], [np.nan, -0.67, -0.22]]) Train k-Nearest Neighbor Classifier # Train KNN learner clf = KNeighborsClassifier(3, weights=&amp;#39;distance&amp;#39;) trained_model = clf.</description>
    </item>
    
    <item>
      <title>Normalizing Observations</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/normalizing_observations.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/normalizing_observations.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import Normalizer import numpy as np Create Feature Matrix # Create feature matrix X = np.array([[0.5, 0.5], [1.1, 3.4], [1.5, 20.2], [1.63, 34.4], [10.9, 3.3]]) Normalize Observations Normalizer rescales the values on individual observations to have unit norm (the sum of their lengths is one).
# Create normalizer normalizer = Normalizer(norm=&amp;#39;l2&amp;#39;) # Transform feature matrix normalizer.transform(X) array([[ 0.70710678, 0.70710678], [ 0.30782029, 0.95144452], [ 0.07405353, 0.</description>
    </item>
    
    <item>
      <title>One-Hot Encode Features With Multiple Labels</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/one-hot_encode_features_with_multiple_labels.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/one-hot_encode_features_with_multiple_labels.html</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import MultiLabelBinarizer import numpy as np Create Data # Create NumPy array y = [(&amp;#39;Texas&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;California&amp;#39;, &amp;#39;Alabama&amp;#39;), (&amp;#39;Texas&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;Delware&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;Texas&amp;#39;, &amp;#39;Alabama&amp;#39;)] One-hot Encode Data # Create MultiLabelBinarizer object one_hot = MultiLabelBinarizer() # One-hot encode data one_hot.fit_transform(y) array([[0, 0, 0, 1, 1], [1, 1, 0, 0, 0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 0], [1, 0, 0, 0, 1]])  View Column Headers # View classes one_hot.</description>
    </item>
    
    <item>
      <title>One-Hot Encode Nominal Categorical Features</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/one-hot_encode_nominal_categorical_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/one-hot_encode_nominal_categorical_features.html</guid>
      <description>Preliminaries # Load libraries import numpy as np import pandas as pd from sklearn.preprocessing import OneHotEncoder Create Data With One Class Label # Create NumPy array x = np.array([[&amp;#39;Texas&amp;#39;], [&amp;#39;California&amp;#39;], [&amp;#39;Texas&amp;#39;], [&amp;#39;Delaware&amp;#39;], [&amp;#39;Texas&amp;#39;]]) One-hot Encode Data (Method 1) # Create LabelBinzarizer object one_hot = OneHotEncoder() # One-hot encode data one_hot.fit_transform(x) &amp;lt;5x3 sparse matrix of type &#39;&amp;lt;class &#39;numpy.float64&#39;&amp;gt;&#39; with 5 stored elements in Compressed Sparse Row format&amp;gt;  View Column Headers # View classes one_hot.</description>
    </item>
    
    <item>
      <title>Preprocessing Categorical Features</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/preprocessing_categorical_features.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/preprocessing_categorical_features.html</guid>
      <description>Often, machine learning methods (e.g. logistic regression, SVM with a linear kernel, etc) will require that categorical variables be converted into dummy variables (also called OneHot encoding). For example, a single feature Fruit would be converted into three features, Apples, Oranges, and Bananas, one for each category in the categorical feature.
There are common ways to preprocess categorical features: using pandas or scikit-learn.
Preliminaries from sklearn import preprocessing from sklearn.pipeline import Pipeline import pandas as pd Create Data raw_data = {&amp;#39;first_name&amp;#39;: [&amp;#39;Jason&amp;#39;, &amp;#39;Molly&amp;#39;, &amp;#39;Tina&amp;#39;, &amp;#39;Jake&amp;#39;, &amp;#39;Amy&amp;#39;], &amp;#39;last_name&amp;#39;: [&amp;#39;Miller&amp;#39;, &amp;#39;Jacobson&amp;#39;, &amp;#39;Ali&amp;#39;, &amp;#39;Milner&amp;#39;, &amp;#39;Cooze&amp;#39;], &amp;#39;age&amp;#39;: [42, 52, 36, 24, 73], &amp;#39;city&amp;#39;: [&amp;#39;San Francisco&amp;#39;, &amp;#39;Baltimore&amp;#39;, &amp;#39;Miami&amp;#39;, &amp;#39;Douglas&amp;#39;, &amp;#39;Boston&amp;#39;]} df = pd.</description>
    </item>
    
    <item>
      <title>Preprocessing Iris Data</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/preprocessing_iris_data.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/preprocessing_iris_data.html</guid>
      <description>Preliminaries from sklearn import datasets import numpy as np from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler Load Data # Load the iris data iris = datasets.load_iris() # Create a variable for the feature data X = iris.data # Create a variable for the target data y = iris.target Split Data For Cross Validation # Random split the data into four new datasets, training features, training outcome, test features,  # and test outcome.</description>
    </item>
    
    <item>
      <title>Rescale A Feature</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/rescale_a_feature.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/rescale_a_feature.html</guid>
      <description>Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]]) Rescale Feature Using Min-Max # Create scaler minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1)) # Scale feature x_scale = minmax_scale.fit_transform(x) # Show feature x_scale array([[ 0. ], [ 0.28571429], [ 0.35714286], [ 0.42857143], [ 1. ]])  </description>
    </item>
    
    <item>
      <title>Standardize A Feature</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_structured_data/standardize_a_feature.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_structured_data/standardize_a_feature.html</guid>
      <description>Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]]) Standardize Feature # Create scaler scaler = preprocessing.StandardScaler() # Transform the feature standardized = scaler.fit_transform(x) # Show feature standardized array([[-1.26687088], [-0.39316683], [-0.17474081], [ 0.0436852 ], [ 1.79109332]])  </description>
    </item>
    
  </channel>
</rss>