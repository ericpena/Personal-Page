<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>https://ericpena.github.io/machine_learning/preprocessing_text/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 11:53:49 -0700</lastBuildDate>
    
        <atom:link href="https://ericpena.github.io/machine_learning/preprocessing_text/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bag Of Words</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/bag_of_words.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/bag_of_words.html</guid>
      <description>Preliminaries # Load library import numpy as np from sklearn.feature_extraction.text import CountVectorizer import pandas as pd Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Sweden is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Bag Of Words # Create the bag of words feature matrix count = CountVectorizer() bag_of_words = count.fit_transform(text_data) # Show feature matrix bag_of_words.toarray() array([[0, 0, 0, 2, 0, 0, 1, 0], [0, 1, 0, 0, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)  View Bag Of Words Matrix Column Headers # Get feature names feature_names = count.</description>
    </item>
    
    <item>
      <title>Parse HTML</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/parse_html.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/parse_html.html</guid>
      <description>Preliminaries # Load library from bs4 import BeautifulSoup Create HTML # Create some HTML code html = &amp;#34;&amp;lt;div class=&amp;#39;full_name&amp;#39;&amp;gt;&amp;lt;span style=&amp;#39;font-weight:bold&amp;#39;&amp;gt;Masego&amp;lt;/span&amp;gt; Azra&amp;lt;/div&amp;gt;&amp;#34; Parse HTML # Parse html soup = BeautifulSoup(html, &amp;#34;lxml&amp;#34;) # Find the div with the class &amp;#34;full_name&amp;#34;, show text soup.find(&amp;#34;div&amp;#34;, { &amp;#34;class&amp;#34; : &amp;#34;full_name&amp;#34; }).text &#39;Masego Azra&#39;  </description>
    </item>
    
    <item>
      <title>Remove Punctuation</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/remove_punctuation.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/remove_punctuation.html</guid>
      <description>Preliminaries # Load libraries import string import numpy as np Create Text Data # Create text text_data = [&amp;#39;Hi!!!! I. Love. This. Song....&amp;#39;, &amp;#39;10000% Agree!!!! #LoveIT&amp;#39;, &amp;#39;Right?!?!&amp;#39;] Remove Punctuation # Create function using string.punctuation to remove all punctuation def remove_punctuation(sentence: str) -&amp;gt; str: return sentence.translate(str.maketrans(&amp;#39;&amp;#39;, &amp;#39;&amp;#39;, string.punctuation)) # Apply function [remove_punctuation(sentence) for sentence in text_data] [&#39;Hi I Love This Song&#39;, &#39;10000 Agree LoveIT&#39;, &#39;Right&#39;]  </description>
    </item>
    
    <item>
      <title>Remove Stop Words</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/remove_stop_words.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/remove_stop_words.html</guid>
      <description>Preliminaries # Load library from nltk.corpus import stopwords # You will have to download the set of stop words the first time import nltk nltk.download(&amp;#39;stopwords&amp;#39;) [nltk_data] Downloading package stopwords to [nltk_data] /Users/chrisalbon/nltk_data... [nltk_data] Package stopwords is already up-to-date! True  Create Word Tokens # Create word tokens tokenized_words = [&amp;#39;i&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;going&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;go&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;store&amp;#39;, &amp;#39;and&amp;#39;, &amp;#39;park&amp;#39;] Load Stop Words # Load stop words stop_words = stopwords.words(&amp;#39;english&amp;#39;) # Show stop words stop_words[:5] [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;]  Remove Stop Words # Remove stop words [word for word in tokenized_words if word not in stop_words] [&#39;going&#39;, &#39;go&#39;, &#39;store&#39;, &#39;park&#39;]  </description>
    </item>
    
    <item>
      <title>Replace Characters</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/replace_characters.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/replace_characters.html</guid>
      <description>Preliminaries # Import library import re Create Text # Create text text_data = [&amp;#39;Interrobang. By Aishwarya Henriette&amp;#39;, &amp;#39;Parking And Going. By Karl Gautier&amp;#39;, &amp;#39;Today Is The night. By Jarek Prakash&amp;#39;] Replace Character (Method 1) # Remove periods remove_periods = [string.replace(&amp;#39;.&amp;#39;, &amp;#39;&amp;#39;) for string in text_data] # Show text remove_periods [&#39;Interrobang By Aishwarya Henriette&#39;, &#39;Parking And Going By Karl Gautier&#39;, &#39;Today Is The night By Jarek Prakash&#39;]  Replace Character (Method 2) # Create function def replace_letters_with_X(string: str) -&amp;gt; str: return re.</description>
    </item>
    
    <item>
      <title>Stemming Words</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/stemming_words.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/stemming_words.html</guid>
      <description>Preliminaries # Load library from nltk.stem.porter import PorterStemmer Create Text Data # Create word tokens tokenized_words = [&amp;#39;i&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;humbled&amp;#39;, &amp;#39;by&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;traditional&amp;#39;, &amp;#39;meeting&amp;#39;] Stem Words Stemming reduces a word to its stem by identifying and removing affixes (e.g. gerunds) while keeping the root meaning of the word. NLTK&amp;rsquo;s PorterStemmer implements the widely used Porter stemming algorithm.
# Create stemmer porter = PorterStemmer() # Apply stemmer [porter.stem(word) for word in tokenized_words] [&#39;i&#39;, &#39;am&#39;, &#39;humbl&#39;, &#39;by&#39;, &#39;thi&#39;, &#39;tradit&#39;, &#39;meet&#39;]  </description>
    </item>
    
    <item>
      <title>Strip Whitespace</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/strip_whitespace.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/strip_whitespace.html</guid>
      <description>Create Text # Create text text_data = [&amp;#39; Interrobang. By Aishwarya Henriette &amp;#39;, &amp;#39;Parking And Going. By Karl Gautier&amp;#39;, &amp;#39; Today Is The night. By Jarek Prakash &amp;#39;] Remove Whitespace # Strip whitespaces strip_whitespace = [string.strip() for string in text_data] # Show text strip_whitespace [&#39;Interrobang. By Aishwarya Henriette&#39;, &#39;Parking And Going. By Karl Gautier&#39;, &#39;Today Is The night. By Jarek Prakash&#39;]  </description>
    </item>
    
    <item>
      <title>Tag Parts Of Speech</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/tag_parts_of_speech.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/tag_parts_of_speech.html</guid>
      <description>Preliminaries # Load libraries from nltk import pos_tag from nltk import word_tokenize Create Text Data # Create text text_data = &amp;#34;Chris loved outdoor running&amp;#34; Tag Parts Of Speech # Use pre-trained part of speech tagger text_tagged = pos_tag(word_tokenize(text_data)) # Show parts of speech text_tagged [(&#39;Chris&#39;, &#39;NNP&#39;), (&#39;loved&#39;, &#39;VBD&#39;), (&#39;outdoor&#39;, &#39;RP&#39;), (&#39;running&#39;, &#39;VBG&#39;)]  Common Penn Treebank Parts Of Speech Tags The output is a list of tuples with the word and the tag of the part of speech.</description>
    </item>
    
    <item>
      <title>Term Frequency Inverse Document Frequency</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/tf-idf.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/tf-idf.html</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer import pandas as pd Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Sweden is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Feature Matrix # Create the tf-idf feature matrix tfidf = TfidfVectorizer() feature_matrix = tfidf.fit_transform(text_data) # Show tf-idf feature matrix feature_matrix.toarray() array([[ 0. , 0. , 0. , 0.89442719, 0. , 0. , 0.4472136 , 0. ], [ 0.</description>
    </item>
    
    <item>
      <title>Tokenize Text</title>
      <link>https://ericpena.github.io/machine_learning/preprocessing_text/tokenize_text.html</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>https://ericpena.github.io/machine_learning/preprocessing_text/tokenize_text.html</guid>
      <description>Preliminaries # Load library from nltk.tokenize import word_tokenize, sent_tokenize Create Text Data # Create text string = &amp;#34;The science of today is the technology of tomorrow. Tomorrow is today.&amp;#34; Tokenize Words # Tokenize words word_tokenize(string) [&#39;The&#39;, &#39;science&#39;, &#39;of&#39;, &#39;today&#39;, &#39;is&#39;, &#39;the&#39;, &#39;technology&#39;, &#39;of&#39;, &#39;tomorrow&#39;, &#39;.&#39;, &#39;Tomorrow&#39;, &#39;is&#39;, &#39;today&#39;, &#39;.&#39;]  Tokenize Sentences # Tokenize sentences sent_tokenize(string) [&#39;The science of today is the technology of tomorrow.&#39;, &#39;Tomorrow is today.&#39;]  </description>
    </item>
    
  </channel>
</rss>