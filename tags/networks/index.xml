<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Networks | ERIC PEÑA</title>
    <link>/tags/networks/</link>
      <atom:link href="/tags/networks/index.xml" rel="self" type="application/rss+xml" />
    <description>Networks</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2019 Eric Peña</copyright><lastBuildDate>Wed, 11 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Networks</title>
      <link>/tags/networks/</link>
    </image>
    
    <item>
      <title>Diversity in Competitive Threshold Linear Networks</title>
      <link>/project/ctln/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/ctln/</guid>
      <description>&lt;figure&gt;
  &lt;img src=&#34;img/network.png&#34; alt=&#34;Network&#34; width=&#34;600&#34;/&gt;
  &lt;figcaption&gt;Figure 1 — Directed Network Which Represents Threshold Linear Network&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;div&gt;$$\frac{d x_i}{dt} = -x_i + \left[ \sum_{j=1}^{n} W_{ij} x_j + \theta \right]_+ i = 1, \ldots, n$$&lt;/div&gt;
&lt;p&gt;&lt;iframe src=&#34;https://docs.google.com/presentation/d/1fx30MNJ0vK8NCKlWVHHwYNCAWU1UkN1zVmRUMJ6gjVQ/edit?usp=sharing&#34; frameborder=&#34;0&#34; width=&#34;800&#34; height=&#34;600&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Complexity — A Guided Tour</title>
      <link>/post/complexity-reflection/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/complexity-reflection/</guid>
      <description>&lt;h1 id=&#34;part-i&#34;&gt;Part I:&lt;/h1&gt;
&lt;p&gt;Providing the multifaceted story of how complex systems has evolved into what it is today is no easy feat. Dr. Melanie Mitchell not only gives a comprehensive historical tour of science and mathematics, she also provides foundational knowledge that any reader can carry to appreciate the essence of what it means for a system to be complex in the technical sense. Early on, she gives a clear definition of complex systems as an interdisciplinary field that studies how large networks of entities with no central controller can organize themselves and exhibit collective behavior given simple rules. However, she also humbly confesses that given how nebulous the field is, there has yet to be one clear definition of complexity that everyone can agree on. I appreciate how candid she is about what is known and what is not known in this field. Mitchell shares that even a panel she organized of great system science thinkers at the Santa Fe Institute could not agree on an answer to the question: &amp;ldquo;how do you define complexity?&amp;quot;.&lt;/p&gt;
&lt;p&gt;She acknowledges the potential skepticism that one might feel because of the nascence of this field. However, she encourages the reader by sharing fascinating real-world examples&amp;mdash;such as ant colonies, brain activity, and internet networks&amp;mdash;that show the ubiquity of complex systems and its necessity in understanding how systems evolve in our lives. She eloquently describes the emergent collective behavior that arises from these systems and the wide scale at which they occur. From thoughts and consciousness in the brain to global economic market movements, these can all be thought of as emergent phenomena attributed to complexity. She mentions that some have even call these systems superorganisms that exhibit a collective intelligence even in non-living, non-conscious systems. She borrows examples from economics stating that the self-interest of smaller economic entities such as companies and individuals create macroscopic effects that tend to not resemble the smaller systems at all. I appreciate that complex systems are not only accessible to all of us everywhere we are, but also require them to survive such as brain neuroactivity and heartbeat rhythms.&lt;/p&gt;
&lt;p&gt;Given my background, the section on dynamics, chaos, and prediction resonated with me most. Mitchell beautifully tells the story of how the natural world was analyzed by great scientists in history and how our understanding of nature evolved. This historical account builds from Aristotle, Copernicus, Galileo, Kepler, and finally Isaac Newton and his revolutionary laws of motion. Newton established a concept of universality extending the natural laws from the terrestrial realm to celestial bodies&amp;mdash;one of the most enlightening realizations in science.&lt;/p&gt;
&lt;p&gt;When I consider physics paradigms in the context of complexity, I think about the dismantling of reductionism&amp;mdash;where the notion that a system is the sum of its parts, as is the case in classical mechanics&amp;mdash;is no longer valid. Mitchell does not talk much about reductionism directly but instead touches on a topic much richer on how concepts in physics directly led to the understanding of complexity and its causes. Much of physics is used primary to make predictions but Mitchell mentions two important scientific discoveries that make prediction very difficult: the uncertainty principle from quantum mechanics and the sensitivity of initial conditions from chaos theory. In 1927, Heisenberg taught us that the momentum and position of a particle cannot be known simultaneously but rather act as a trade-off of information. This is not an experimental limitation but an inherent limitation of information that is written into the law of physics. This must have been devastating during the time of Newton&#39;s &amp;ldquo;clockwork universe&amp;rdquo; when mathematicians such as Pierre Simon Laplace had ideas of being able to, in theory, predict everything at all times given we have the information to do so. The development of chaos theory by Poincar&#39;e and Lorenz had a similar effect stating that even the tiniest variation in initial conditions can lead to drastically different outcomes&amp;mdash;which served as another blow to predictability.&lt;/p&gt;
&lt;p&gt;Mitchell sheds hope and motivates the reader by mentioning that in the same way there are universal laws in physics, there are similar invariants in the study of complex systems as well. Period-doubling as a system evolves to a chaotic state and the Feigenbaum&#39;s constant for the rate at which bifurcations converge are examples of remarkable invariant properties in complex systems. This provides an indication that the essence of physics can likely be applied to the field of complex systems and even possibly toward its version of a unified theory.&lt;/p&gt;
&lt;p&gt;Dr. Melanie Mitchell seamlessly weaves together the concepts of information, entropy, energy, and thermodynamics. These are integral to the field so much so that she even refers to &amp;ldquo;entropy-defying self-organization&amp;rdquo; as the &amp;ldquo;holy grail of complex systems&amp;rdquo;. She resolves the misconception that entropy can at times decrease without consequence even though the law of thermodynamics forbids it. She talks about this in terms of Maxwell&#39;s Demon stating that work is required for the demon to do its job. I had learned about Maxwell&#39;s Demon in the past but had never considered this in terms of information. Szilard thought that work and energy was expended when obtaining the measurements of the particle&#39;s velocities&amp;mdash;in other words, it requires work to obtain the relevant information needed. I find this to be a novel idea and a clever way to connect all of these concepts in one easy to understand example.&lt;/p&gt;
&lt;p&gt;Another pivotal moment for me in reading Part I was in chapter 4&amp;mdash;computation. In the same way that physics endured a disillusionment of endless predictability, mathematics and computability had a relatable event in history through the work of Kurt Gödel and Alan Turing.
David Hilbert&#39;s provocative questions for the mathematics community challenged the stability of mathematics itself. Gödel figured out how to convert statements into mathematical language and learned that there exists self-contradictory mathematical statements&amp;mdash;showing that not everything in mathematics could be proven. Mathematics was viewed as being exacting and able to prove anything at that time. Gödel&#39;s Incompleteness Theorem shattered the apparent grandeur of mathematics and worried many practitioners. Alan Turing also came up with a similar answer using what was called a Turing Machine. The result had extended to machine language as well.&lt;/p&gt;
&lt;p&gt;Dr. Mitchell is not only able to clearly explain a topic as abstruse as complex theory, she does so to a general audience without inundating the reader with technical jargon. Since there is not a definition of complexity that the scientific community can agree on, Mitchell offers some thoughts about possible ways that complexity could be defined. She is again very candid about the limitations of these definitions and explains why they would not hold up in isolation. All throughout Part I, she keeps the theme of being informative, honest, and motivating. She is honest about scientists not yet having a clear understanding of the field but also offers other examples in history where this was also true such as the concept of energy before it was well understood. Even today, research on genes and dark matter have yet to be understood fully but are making tremendous progress. I learned a great deal so far from this book and I look forward to the insight it has to offer in the remaining chapters.&lt;/p&gt;
&lt;h1 id=&#34;parts-ii-and-iii&#34;&gt;Parts II and III&lt;/h1&gt;
&lt;p&gt;Melanie Mitchell states that it is hypothesized that the balance between exploration and exploitation of information may serve as a general property of what are now called complex systems. Having read parts II and III of the book, it is evident that this idea could not have been developed without the use of computation, particularly self-replicating systems. Mitchell takes us deeper into the journey of complex systems and focuses on the connection between computation and living systems. From the fine details of a genetic algorithm to large-scope philosophical ideas, she provides different levels of abstraction allowing the reader to appreciate these topics from all angles. The historical account she gives of evolutionary computation reads as if she is alluding to the past and the future almost simultaneously &amp;mdash; she tells where the inspiration has come from and where it could lead.&lt;/p&gt;
&lt;h2 id=&#34;self-replication&#34;&gt;Self Replication&lt;/h2&gt;
&lt;p&gt;Part II begins with the idea of &lt;em&gt;&lt;strong&gt;what life is&lt;/strong&gt;&lt;/em&gt; and what are its properties. Mitchell explains how the idea of self-replicating artificial life is an old idea but continues to live on, even in our movies and artwork today. I thought this was a great introduction to the idea of evolutionary programming. The discussion leads to the idea of reproduction of computer systems that can, for example, print their own code. This is a novel idea on a fundamental level and a great zeroeth-order example to begin understanding its significance; but the fact that DNA, for example, can actually transcribe its own interpreter may provoke one to admit: some systems actually are &lt;em&gt;living&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;genetic-algorithms&#34;&gt;Genetic Algorithms&lt;/h2&gt;
&lt;p&gt;I was inspired by the chapter on genetic algorithms to create my own program&amp;mdash;a simple program that evolves and learns to print the characters:
&lt;strong&gt;ericpena&lt;/strong&gt;. Although a simple task for a computer to perform, it made apparent the power of this methodology to solve more complicated tasks. John von Neumann is essentially the person who established the idea of self-replicating machines in a tangible way. Although, he thought that replication was not enough as these systems also need to evolve, learn, and compete with one another to survive. It was John Holland that proposed the idea for genetic algorithms that was inspired by how nature evolves to find near-optimal solutions via natural selection. Mitchell gives a great example on how a genetic algorithm can help improve the performance of Robby, the Soda-Can-Collecting Robot. This chapter revealed the vast potential for genetic algorithms and how in many ways they are better than human-developed algorithms for various tasks. One particular discussion that stood out to me regarding Robby is the following: Robby&#39;s success isn&#39;t necessarily dependent on the individual, step-by-step decisions that Robby makes to collect cans but rather the aggregate of these steps that define a hollistic strategy, a rather successful strategy at that. Mitchell explains this by stating that it isn&#39;t always individual genes at work but rather their interactions that produce results.&lt;/p&gt;
&lt;h2 id=&#34;cellular-automata-and-the-nature-of-computability&#34;&gt;Cellular Automata and The Nature of Computability&lt;/h2&gt;
&lt;p&gt;I have been enthralled by cellular automata ever since I first learned about them. In this chapter, Mitchell talks about how computation occurs in nature and what it even means for nature to &amp;ldquo;compute&amp;rdquo;. Similar to the way in which physicists simplify problems to gain understanding&amp;mdash;through frictionless slopes and spherical cows&amp;mdash;scientists have performed a simplification for computation in the form of cellular automata. Although the rules of a cell can be relatively simple, the interconnectedness of a grid of cells can produce complex behavior that can, at times, simulate natural processes such as forest fires, fire flies, and even neurons. The field of cellular automata seemed to have taken a life of its own. John von Neumann, who played a key role in creating cellular automata, proved that it is actually Turing Complete. John Conway&#39;s Game of Life programs popularize the discipline even further especially when proving that his game is capable of simulating a universal computer. With the large number of configurations and possibilities, it may have been difficult to wrap one&#39;s head around the innate behavior of this mechanism. It was Stephan Wolfram, who nothing short of genius, worked diligently and cleverly to classify the ways in which cellular automata can behave and what patterns it can exhibit. He created libraries of patterns for these things, particularly four classes, and became rather popular for his book on them, {\it A New Kind of Science}. I learned about Wolfram during my undergraduate program and used Mathematica for many of my homework problems&amp;mdash;although I had not read about his beliefs that Mitchell mentions in the reading. I found it interesting that Wolfram believes natural processes are intrinsically computable and can be explained in this way. While in physics, I conceded to the idea that nature is infinitely complex and our best hope in grasping it is to make very accurate approximations with efforts such as quantum mechanics or Einsteinian gravity. However, Mitchell explains that Wolfram believes that nothing can be more complex than a universal computer which forces an upper limit on the complexity that the universe can exhibit&amp;mdash;this is very interesting although I am not sure I agree completely given the unpredictability we read about earlier in the book, particularly in quantum mechanical phenomena. I was nevertheless fascinated by these ideas and will revisit them in the future.&lt;/p&gt;
&lt;p&gt;A connection became apparent to me while reflecting on the chapters on genetic algorithms and cellular automata.  On one hand, many processes in nature consists of simple rules but large in number that coalesce into complex behavior. Cellular automata has this idea built into its very structure. In a way, we can say that the mechanism by which nature evolves and produces complexity is comparable to the way in which cellular automata does so. On the other hand, genetic algorithms were inspired by what occurs in nature, namely natural selection and evolution. I found it fascinating that nature utilizes general principles found in cellular automata and that genetic algorithms mimic what is done in nature. This further reinforces the link between computation and living systems.&lt;/p&gt;
&lt;h2 id=&#34;information-processing&#34;&gt;Information Processing&lt;/h2&gt;
&lt;p&gt;Mitchell guides the reader in understanding information processing particularly in living systems. I enjoyed how she uses different levels of programming, high level and machine language, to home in on the concept of abstraction. She mentions that it&#39;s relatively easy to imagine how changes in high level code are translated into low level machine language since the tether between the two is tangible and somewhat predictable. She compares this to cellular automata where the ability to create a productive level of abstraction is not present. This is particular to cellular automata but I imagine that it is a limitation that can be applied to much of complex systems as a whole&amp;mdash;this is, creating a high level framework from which many applications can benefit from especially biological applications.
The book wonderfully explains information processing both in terms of what information is and how it is processed. It accomplishes this with the help of three real-world examples: the immune system, ant colonies, and biological metabolism. One of my favorite parts&amp;mdash;which Mitchell believes is a quite profound idea&amp;mdash;is that on the {\it meaning} of information. We can think in terms of inputs and outputs but what does the information {\it mean} and what part of a system is doing this type of analysis? As Mitchell points out, this is particularly mysterious for systems with no central controller.&lt;/p&gt;
&lt;h1 id=&#34;parts-iv-and-v&#34;&gt;Parts IV and V&lt;/h1&gt;
&lt;p&gt;Part IV of Melanie Mitchell&#39;s book discusses the science of networks&amp;mdash;a topic in which tremendous progress and development has been made relatively recently. She begins the discussion with Harvard University psychologist, Stanley Milgram. He was one of the first to have devised an experiment designed to understand the degrees of separation in a network&amp;mdash;his experiment is the quintessential example of the small world property in a network.&lt;/p&gt;
&lt;p&gt;Professor Mitchell introduces networks and gives examples that demonstrate that networks are ubiquitous in our everyday lives. From social networks to epidemiological studies, these fields can and should be explored in the context of network science. She mentions popular names in the network science world: Duncan Watts, Steven Strogratz, Albert-László Barabási, and Réka Albert who all published papers that formed the foundation of network science. She mentions that physicists made contributions to the field of network science, which is right in the physics wheelhouse given its mathematical nature and real-world application. Mitchell explains that physicists have been trained to simplify complex problems without losing their essential features. I did enjoy and will attempt to commit to memory the eloquent quote from Duncan Watts, &amp;ldquo;No one descends with such fury and in so great a number as a pack of hungry physicists, adrenalized by the scent of a new problem.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;It is helpful to the reader to know generally what a network is. Mitchell explains the basics of in-degree, out-degree, and their distributions in a network. She goes into what clustering means and how we think about clustering in the real-world. She gives popular examples such as an airplane&#39;s use of hubs to optimize flight itineraries. If one were to create a network whose degree distribution is random, all nodes would have somewhat similar degree and the degree distribution would be considered uniform&amp;mdash;unlike what we see in real-world biological and social networks. Naturally occurring networks exhibit hub structures and skewed degree distributions which is quite interesting. What invisible hand is fine-tuning these networks to acquire these specific properties? Mitchell gives us great insight on this. She breaks down the general attributes of networks into two pillars: small-world and scale-free. She explains that when a network has small-world-ness, it exhibits attractive features for a network to have such as robustness and faster communication. A skewed degree distribution makes a network robust to node deletion since hubs are few in number and high in degree. The Milgram experiment is a clear example of how these networks can benefit from efficient information spread whereas disease outbreaks may be exacerbated.&lt;/p&gt;
&lt;p&gt;Network science was especially propelled by the use of computers: their increase in computing power and availability of data. Mitchell explains that although real-world networks resembled small-world structures, it was not exact. They are better explained by a property called scale-free that pervades mostly all natural networks. She gives an explanation of using Google&#39;s page-rank to show that the world wide web itself is a scale-free network. She goes on the talk about this in terms of scaling invariance and self-similarity and says that this is what is meant by scale-free&amp;mdash;terms that are all synonymous. Another concept she equates to the scale-free property of networks is power law degree distribution. An important part of this discussion is Barab&#39;asi&#39;s work on preferential attachment and how this mechanism facilitates the scale-free structure in naturally forming networks.&lt;/p&gt;
&lt;p&gt;It certainly does aide the reader in having stimulating real-world examples of network structures from which to explore these properties. The examples she delves into are the brain of the worm C. elegans, genetic regulatory networks in our bodies, epidemiological studies, and even ecological insights that the reader can learn from. Mitchell not only explains that the brain of a worm is found to have small-world structure, she also encourages the reader to consider a rather provocative question: &amp;ldquo;Why would evolution favor brain networks with the small-world property?&amp;quot;. Two main ideas stem from this question&amp;mdash;the resilience of scale-free network structures and energy efficiency for global information processing. A fully connected network would use entirely too much energy to complete similar tasks. Mitchell briefly mentions synchronization at the end of the brain example. I understand this topic can get much more involved with how nature utilizes synchronization. The synchronization of fire flies is especially interesting&amp;mdash;more of this can be found in an article written by Renato Mirollo and Steven Strogatz, &amp;ldquo;Synchronization of Pulse-Coupled Biological Oscillators&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Mitchell naturally segues into the topic of scaling. The start of this chapter reminded me of Galileo&#39;s Square-Cube Law where he essentially argues that structures cannot simply be scaled up ad infinitum. The core of the argument involves recognizing that volume grows as the cube of the multiplier and the strength of support grows only as the cross-sectional area or square of the multiplier. One of the key messages with regards to scaling is that metabolic rate does not scale linearly with $bodymass$ but rather with $bodymass^{\frac{3}{4}}$. This occurs in a myriad of creatures and shows that they become more efficient as the scaling grows. This power law distribution of metabolic rate versus body-mass is referred to as Kleiber&#39;s law. James Brown, Brian Enquist, and Geoffrey West played pivotal roles in forming the metabolic scaling theory. Geoffrey West wrote a popular book on this subject as well that the general audience can appreciate: &amp;ldquo;Scale: The Universal Laws of Growth, Innovation, Sustainability, and the Pace of Life in Organisms, Cities, Economies, and Companies&amp;rdquo;. He elaborates on how cities and economies tend to exhibit similar power law distributions when they are scaled up as well. Mitchell clearly explains Zipf&#39;s law&amp;mdash;she emphasizes it&#39;s significance and ubiquity and also provides some insight into why this power-law distribution tends to naturally occur. Mitchell shares that this is an important open problem in complexity science but I was particularly interested in Mandelbrot&#39;s argument that the simultaneous optimization of information content and transmission leads directly to Zipf&#39;s law.&lt;/p&gt;
&lt;p&gt;The conclusion of the book serves the reader in several ways. It ties various concepts together mentioned throughout the book and gives a historical account of the key players that contributed to the field of complex systems. It also explains the goal of complexity as a science while sharing the honest opinions of complexity enthusiasts and critics, alike. Mitchell candidly addresses the major challenges of complexity science such as the lack of an all-encompassing mathematical framework from which complexity can be studied. Most importantly, it leaves us with the refreshing hope for future scientists to work diligently to explore the possibility of a grand unified theory of complexity and to discover what Strogatz refers to as the &amp;ldquo;conceptual equivalent of calculus&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mathematics of Network Theory</title>
      <link>/post/network-theory/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/network-theory/</guid>
      <description>&lt;p&gt;Graphs may be represented in the form of a matrix. Main types of graphs that may be represented are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Graph&lt;/li&gt;
&lt;li&gt;Multigraph&lt;/li&gt;
&lt;li&gt;Directed Graph&lt;/li&gt;
&lt;li&gt;Weighted Graph&lt;/li&gt;
&lt;li&gt;Bipartite Graph&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;directed-graph&#34;&gt;Directed Graph&lt;/h3&gt;
&lt;p&gt;Directed graphs are graphs that contain edges with direction. Vertices may have inward and outward edges.&lt;/p&gt;
&lt;p&gt;Unlike adjacency matricies for simped graphs, adjacency matricies for directed graphs are non-symmetric. Elements of an adjacency matrix for a directed graph may be denoted as:
$$A_{ij}$$
which represents an edge from vertex $j$ to $i$.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/DirectedGraph1.png&#34; alt=&#34;Directed Graph&#34; width=&#34;500&#34;/&gt;
  &lt;figcaption&gt;Figure 1 — Directed graph with four verticies&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The corresponding adjacency matrix for the graph above is:
$$A = \begin{pmatrix}0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0\\ 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \end{pmatrix}$$&lt;/p&gt;
&lt;h3 id=&#34;cocitation&#34;&gt;Cocitation&lt;/h3&gt;
&lt;p&gt;The cocitation of two vertices $i$ and $j$ in a directed network is the number of vertices that have outgoing edges pointing to both $i$ and $j$. We can see that:&lt;/p&gt;
&lt;div&gt;$$A_{ik}A_{jk} = 1$$&lt;/div&gt;
&lt;p&gt;if $i$ and $j$ are both cited by $k$. If we sum over all these elements we get the following relation:&lt;/p&gt;
&lt;div&gt;$$C_{ij} = \sum\limits_{k=1}^n A_{ik}A_{jk} = \sum\limits_{k=1}^n A_{ik}A_{kj}^T = AA^T $$&lt;/div&gt;
&lt;p&gt;This is a cocitation network for which there is an edge between $i$ and $j$ if $C_{ij} &amp;gt; 0$, for $i \neq j$.&lt;/p&gt;
&lt;p&gt;The diagonal elements of the cocitation matrix are given by:&lt;/p&gt;
&lt;div&gt;$$C_{ii} = \sum\limits_{k=1}^n A_{ik}^2 = \sum\limits_{k=1}^n A_{ik}$$&lt;/div&gt;
&lt;p&gt;In constructing the cocitation network we ignore these diagonal elements, meaning that the network&#39;s adjacency matrix is equal to the cocitation matrix but with all the diagonal elements set to zero.&lt;/p&gt;
&lt;h3 id=&#34;bibliographic-coupling&#34;&gt;Bibliographic Coupling&lt;/h3&gt;
&lt;p&gt;Cocitation and Bibliographic coupling are similar mathematically but give different results. They&#39;re both affected by the number of in and out edges. Bibliographic Coupling of two vertices are the number of other vertices to which both $i$ and $j$ point to. Bibliographic Coupling is general more stable since the number of citations can vary with time. Bibliographic Coupling is known at time of publishing and doesn&#39;t change at all. This may or may not be a good thing depending on the situation. Mathematically, it can be described by the following:&lt;/p&gt;
&lt;div&gt;$$B_{ij} = \sum\limits_{k=1}^n A_{ki}A_{kj} = \sum\limits_{k=1}^n A_{ik}^TA_{kj} = A^TA $$&lt;/div&gt;
&lt;p&gt;The diagonal elements of $\textbf{B}$ are:&lt;/p&gt;
&lt;div&gt;$$B_{ii} = \sum\limits_{k=1}^n A_{ki}^2 = \sum\limits_{k=1}^n A_{ki}$$&lt;/div&gt;
&lt;p&gt;$B_{ii}$ is equal to the number of other vertices that vertex $i$ points to - the number of papers $i$ cites.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/DirectedGraph2.png&#34; alt=&#34;Directed Graph&#34; width=&#34;400&#34;/&gt;
  &lt;figcaption&gt;Figure 2 — Shows cocitation and bibliographic coupling network comparison&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;hypergraphs&#34;&gt;Hypergraphs&lt;/h3&gt;
&lt;p&gt;Networks with link that join more than two vertices are called hypergraphs. These types of graphs are useful when representing family relations for example. Edges that relate more than two vertices are called hyperedges. In sociology, these networks may be called &lt;em&gt;affiliation networks&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;bipartite-networks&#34;&gt;Bipartite Networks&lt;/h3&gt;
&lt;p&gt;Hypergraphs may be difficult to deal with and represent mathematically but a tool that can help are bipartite graphs - a way of conveniently representing the hypergraph structure. In sociology, this may be called: &lt;em&gt;two-mode networks&lt;/em&gt;. Edges only exist between two vertices of unlike-types.&lt;/p&gt;
&lt;p&gt;The adjacency matrix for a bipartite graph is a rectangular matrix called an &lt;em&gt;incidence matrix&lt;/em&gt; which is a $g$ by $n$ matrix where $g$ is the number of groups and $n$ are the number of members in the groups.&lt;/p&gt;
&lt;div&gt;
$$B_{ij} = \begin{cases} 
      1, &amp; \textit{if vertex j belongs to group i} \\
      0, &amp; \textit{otherwise}
\end{cases}$$
&lt;/div&gt;
&lt;figure&gt;
  &lt;img src=&#34;img/BipartiteGraph1.png&#34; alt=&#34;Directed Graph&#34; width=&#34;400&#34;/&gt;
  &lt;figcaption&gt;Figure 3 — Bipartite Graph&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The adjancency matrix for the bipartite graph above can be written as a $4$ by $5$ matrix:&lt;/p&gt;
&lt;p&gt;$$B = \begin{pmatrix}1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0\\1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0\\0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 1\\0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\end{pmatrix}$$&lt;/p&gt;
&lt;p&gt;This is a much easier way of representing the hypergraph of actors to movies for example. For much info, read section 6.6 (p.125) of Networks - An Introduction (Newman).&lt;/p&gt;
&lt;p&gt;The bipartite graph can be broken down even further by making two one-mode projections. One projection can be made with the &lt;em&gt;groups&lt;/em&gt; side and another can be made with &lt;em&gt;members&lt;/em&gt; side. These projections have the benefit of being simpler to study but are less powerful because information is lost through these projections.&lt;/p&gt;
&lt;p&gt;The two one-mode projections in words are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of groups for which members $i$ and $j$ are both a part of. This is an $n$ x $n$ matrix: $$P = B^TB$$&lt;/li&gt;
&lt;li&gt;The number of common members of groups $i$ and $j$. This is a $g$ x $g$ matrix: $$P&#39;=BB^T$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;quick-thought&#34;&gt;Quick Thought&lt;/h4&gt;
&lt;p&gt;Naturally you want to relate this to cociation and bibliographic coupling networks but it may be confusing to do so. The main difference between cocitation and bibliographic coupling is the direction of the arrows. This bipartite network consists of two different types of nodes and un-directed edges. Therefore, you may have some cyclic thinking if you try to relate them too much. Although The &lt;em&gt;first&lt;/em&gt; projection (the one on the members) is similar to the cocitation network in that the diagonals should be ignored and forced to be zero.&lt;/p&gt;
&lt;h4 id=&#34;information-loss&#34;&gt;Information Loss&lt;/h4&gt;
&lt;p&gt;Although these projections make life a little easier, it does come at a cost: loss of information. Some of the things we loose are the number of groups in the network and the exact membership of each group. If we make the projection weighted graphs, we can at least get information as to how many commons groups a pair of vertices share for example.&lt;/p&gt;
&lt;h3 id=&#34;trees&#34;&gt;Trees&lt;/h3&gt;
&lt;p&gt;A &lt;em&gt;tree&lt;/em&gt; is a connected, undirected network that contains no closed loops. Connected means that every vertex in the network is reachable from every other via some path through the network. A network can also consists of two or more parts. If the individual parts of the network are trees, the then network as a whole is considered a forest. There are leaves on a tree - vertices with one edge on them but topologically, there isn&#39;t really a root.&lt;/p&gt;
&lt;p&gt;The most important property of a tree is that, since there are no closed loops, there is only one path between any pair of vertices. In a forest, there is at most one path but there may be none.&lt;/p&gt;
&lt;p&gt;Another very useful property of trees is that a tree of $n$ vertices always has $n-1$ edges. The reverse is also true: any connected network with $n$ vertices and $n-1$ edges is a tree. If such a network were not a tree then there must be a loop in the network somewhere, implying that we could remove an edge without disconnecting any part of the network.&lt;/p&gt;
&lt;h3 id=&#34;planar-network&#34;&gt;Planar Network&lt;/h3&gt;
&lt;p&gt;Simply put, a planar network is a network that can be drawn on a plane without having any edges cross. All trees are planar but most of the time, network are not planar (e.g., citation networks, metabolic networks, internet, etc.). Some networks are forced to be planar because of physics space constraints such as rivers or road networks.&lt;/p&gt;
&lt;p&gt;These types of networks play an important role in the &lt;em&gt;four-color theorem&lt;/em&gt; which state that the number of colors required to color a graph in this way is called the chromatic number of the graph and many mathematical results are known about chromatic numbers.&lt;/p&gt;
&lt;p&gt;An important to point out is that there is a method of determining if a network is planar. It&#39;s fairly easy to tell by observation if the network is small but when the network is very large, a general method is required.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kuratowski&#39;s Theorem&lt;/em&gt;: Every non-planar network contains a least one subgraph that is an expansion of $K_5$ and $UG$. (Read more about this on p. 132 of Networks - an Introduction (Newman)).&lt;/p&gt;
&lt;h3 id=&#34;degree&#34;&gt;Degree&lt;/h3&gt;
&lt;h4 id=&#34;mean-degree&#34;&gt;Mean Degree&lt;/h4&gt;
&lt;p&gt;We will denote the degree of vertex $i$ by $k_i$. For an undirected graph of n vertices the degree can be written in terms of the adjacency matrix as:&lt;/p&gt;
&lt;div&gt;$$k_i = \sum\limits_{j=1}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;Every edge in an undirected graph has two ends and if there are m edges in total then there are $2m$ ends of edges. But the number of ends of edges is also equal to the sum of the degrees of all the vertices, so&lt;/p&gt;
&lt;div&gt;$$2m = \sum\limits_{i=1}^n k_i$$&lt;/div&gt;
&lt;p&gt;Another way of writing this that is more intuitive is:&lt;/p&gt;
&lt;div&gt;$$m = \frac{1}{2}\sum\limits_{i=1}^n k_i = \frac{1}{2}\sum\limits_{ij}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;The mean degree $c$ of an undirected graph is:&lt;/p&gt;
&lt;div&gt;$$c = \frac{1}{n} \sum\limits_{i=1}^n k_i$$&lt;/div&gt;
&lt;p&gt;And combining this with the earlier equation:&lt;/p&gt;
&lt;p&gt;$$c = \frac{2m}{n}$$&lt;/p&gt;
&lt;h4 id=&#34;density&#34;&gt;Density&lt;/h4&gt;
&lt;p&gt;The maximum possible number of edges in a simple graph is $\binom{n}{2} = \frac{1}{2}n(n-1)$. The connectance or density $\rho$ of a graph is the fraction of these edges that are actually present:&lt;/p&gt;
&lt;p&gt;$$\rho = \frac{m}{\binom{n}{2}}=\frac{2m}{n(n-1)}=\frac{c}{n-1}$$&lt;/p&gt;
&lt;p&gt;When the network is sufficiently large, $\rho$ may be approximated with just $\frac{c}{n}$.&lt;/p&gt;
&lt;p&gt;A network where $\rho$ tends to a constant as $n \rightarrow \infty$ is said to be &lt;em&gt;dense&lt;/em&gt;. A network in which $\rho \rightarrow 0$ as $n \rightarrow \infty$ is said to be &lt;em&gt;sparse&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;directed-network-degree&#34;&gt;Directed Network Degree&lt;/h4&gt;
&lt;p&gt;Vertex degrees in a directed network are more complicated. They are broken up into &lt;em&gt;in-degree&lt;/em&gt; and &lt;em&gt;out-degree&lt;/em&gt;. If $A_{ij}$ is the adjacency matrix of a directed network, the *in* and *out* degree can be written as:&lt;/p&gt;
&lt;div&gt;$$k_i^{in} = \sum\limits_{j=1}^n A_{ij},\ \ \  k_j^{out} = \sum\limits_{i=1}^n A_{ij}$$&lt;/div&gt;
&lt;p&gt;We also know the number of edges are:&lt;/p&gt;
&lt;div&gt;$$m = \sum	\limits_{i=1}^n k_i^{in} = \sum\limits_{j=1}^n k_j^{out} = \sum	\limits_{ij} A_{ij}$$&lt;/div&gt;
&lt;p&gt;As far as the mean degree of directed networks:&lt;/p&gt;
&lt;div&gt;$$c_{in} = \frac{1}{n} \sum\limits_{i=1}^n k_i^{in} = \frac{1}{n} \sum\limits_{j=1}^n k_j^{out} = c_{out}$$&lt;/div&gt;
&lt;p&gt;Combining these two relations, the mean degree can concisely be written as:&lt;/p&gt;
&lt;p&gt;$$c = \frac{m}{n}$$&lt;/p&gt;
&lt;h3 id=&#34;paths&#34;&gt;Paths&lt;/h3&gt;
&lt;p&gt;A path along a network is a route across the network moving from vertex to vertex along the edges. In a directed network, the path can on go in the direction of the edge but can go either way for an undirected network. A path may reach a vertex or go along an edge it has seen before. A path that does not intersect itself is considered a &lt;em&gt;self-avoiding path&lt;/em&gt;. Geodesic paths and Hamiltonian paths are two special cases of self-avoiding paths.&lt;/p&gt;
&lt;p&gt;The number of paths of length $r$ may be important to study and can be calculated for directed and undirected networks. We will use the fact that for directed and undirected networks, $A_{ij}$ is 1 if there is an edge from vertex $j$ to vertex $i$, and 0 otherwise. We can start by asking how many paths of length 2 are there in a network. Imagine we want to study all paths of length 2 from $j$ to $i$ via $k$. The product $A_{ik}A_{kj}$ is 1 where there is a path of length 2 from $j$ to $i$ via $k$, and 0 otherwise.&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{(2)} = \sum\limits_{k=1}^n A_{ik}A_{kj}=\left[A^2\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;We can study the path of length 3 as well. The product $A_{ik}A_{kl}A_{lj}$ is 1 where there exists a path of length 3, and 0 otherwise.&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{(3)} = \sum\limits_{k,l=1}^n A_{ik}A_{kl}A_{lj}=\left[A^3\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;Generalizing to any length $r$ gives:&lt;/p&gt;
&lt;div&gt;$$N_{ij}^{r}=\left[A^r\right]_{ij}$$&lt;/div&gt;
&lt;p&gt;There is a proof of induction on page 137 of Network - An Introduction (Newman).&lt;/p&gt;
&lt;p&gt;Another important thing to consider are loops in a network. The number of loops may be calculated as well.&lt;/p&gt;
&lt;div&gt;$$L_r = \sum\limits_{i=1}^n\left[A^r\right]_{ii}=Tr A^r$$&lt;/div&gt;
&lt;p&gt;There &amp;lsquo;Tr&amp;rsquo; is the trace of a matrix. The number of loops can be written in terms of the eigenvalues of the adjacency matrix as well. The adjacency matrix can be written as $A=UKU^T$ where $U$ is the orthogonal matrix of eigenvectors and $K$ is the orthogonal matrix of eigenvalues:&lt;/p&gt;
&lt;p&gt;$$A^r = (UKU^T)^r = UK^rU^T$$&lt;/p&gt;
&lt;div&gt;$$L_r = Tr(UK^rU^T)=Tr(U^TUK^r)=Tr(k^r)=\sum\limits_i k_i^r$$&lt;/div&gt;
&lt;p&gt;Where $k_i$ is the $i^{th}$ eigenvalue of the adjacency matrix. This applies to directed and undirected graphs. There is one important thing to note when learning about counting the number of loops on length r. For each consideration below, the calculation for determining the number of loops uses the following criteria for counting distinct loops.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although there are loop paths that have the same vertices and same order, if there are different starting points, then they are considered separate loops.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$1\rightarrow 2\rightarrow 3 \rightarrow 1 \ \ \ and \ \ \ 2\rightarrow 3\rightarrow 2 \rightarrow 1$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If loops are in the opposite direction, they are counted as distinct loops.
$$1 \rightarrow 2 \rightarrow 3 \rightarrow 1 \ \ \ and \ \ \ 1 \rightarrow 3 \rightarrow 2 \rightarrow 1$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;geodesic-paths&#34;&gt;Geodesic Paths&lt;/h3&gt;
&lt;p&gt;A geodesic path is shortest network distance between vertices in question. This is also called &lt;em&gt;geodesic distance&lt;/em&gt; or &lt;em&gt;shortest distance&lt;/em&gt;. Mathematically, a geodesic distance is the smallest value of r such that $\left[ A^r \right]_{ij} &amp;gt; 0$ between vertices $i$ and $j$.&lt;/p&gt;
&lt;p&gt;It may be the case that no shortest distance exists (for example: for separate components of the network where the distance may be said to be infinity). Another interesting fact - If a path intesects itself, it has a loop and therefore cannot be a geodesic path since it can be shortened by removing this loop.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;diameter&lt;/em&gt; of a graph is the length of the longest geodesic path between any pair of vertices in the network for which a path actually exists.&lt;/p&gt;
&lt;h3 id=&#34;eulerian-and-hamiltonian-paths&#34;&gt;Eulerian and Hamiltonian Paths&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Eulerian Path&lt;/strong&gt;: a path that traverses each edge in the network exactly once&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hamiltonian Path&lt;/strong&gt;: a path that visits each vertex exactly once&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If there are any vertex degree greater than 2, then the Eulerian path is not self-avoiding since it has to visit vertices more than once in order to traverse tall their edges.&lt;/p&gt;
&lt;h4 id=&#34;kronigsberg-bridges&#34;&gt;Kronigsberg Bridges&lt;/h4&gt;
&lt;p&gt;This problem becomes finding an Eulerian path on this network of bridges and the name is in honor of Euler who presented this problem. Euler observed that since any Eulerian path must both enter and leave every vertex it passes (except for the first and last), there can at most be two vertices with odd degree. All four of the vertices in the Kronigsberg Problem has odd degree. More precisely, there can only be 2 or 0 vertices of odd degree for an Eulerian condition to be possible. With this logic, Euler proved the Kronigsberg problem has no solution.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
